{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling monosemanticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./venvllm/lib/python3.10/site-packages (4.45.2)\n",
      "Requirement already satisfied: datasets in ./venvllm/lib/python3.10/site-packages (2.21.0)\n",
      "Requirement already satisfied: requests in ./venvllm/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./venvllm/lib/python3.10/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venvllm/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venvllm/lib/python3.10/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venvllm/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./venvllm/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./venvllm/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venvllm/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: filelock in ./venvllm/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./venvllm/lib/python3.10/site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: aiohttp in ./venvllm/lib/python3.10/site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: xxhash in ./venvllm/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: fsspec[http]<=2024.6.1,>=2023.1.0 in ./venvllm/lib/python3.10/site-packages (from datasets) (2024.2.0)\n",
      "Requirement already satisfied: pandas in ./venvllm/lib/python3.10/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./venvllm/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./venvllm/lib/python3.10/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: multiprocess in ./venvllm/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./venvllm/lib/python3.10/site-packages (from aiohttp->datasets) (1.17.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venvllm/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venvllm/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./venvllm/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./venvllm/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venvllm/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venvllm/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venvllm/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venvllm/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venvllm/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venvllm/lib/python3.10/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venvllm/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venvllm/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./venvllm/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./venvllm/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./venvllm/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: keras in ./venvllm/lib/python3.10/site-packages (3.6.0)\n",
      "Requirement already satisfied: huggingface_hub in ./venvllm/lib/python3.10/site-packages (0.25.2)\n",
      "Requirement already satisfied: optree in ./venvllm/lib/python3.10/site-packages (from keras) (0.13.0)\n",
      "Requirement already satisfied: ml-dtypes in ./venvllm/lib/python3.10/site-packages (from keras) (0.4.1)\n",
      "Requirement already satisfied: namex in ./venvllm/lib/python3.10/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: packaging in ./venvllm/lib/python3.10/site-packages (from keras) (24.1)\n",
      "Requirement already satisfied: rich in ./venvllm/lib/python3.10/site-packages (from keras) (12.6.0)\n",
      "Requirement already satisfied: numpy in ./venvllm/lib/python3.10/site-packages (from keras) (1.26.3)\n",
      "Requirement already satisfied: absl-py in ./venvllm/lib/python3.10/site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: h5py in ./venvllm/lib/python3.10/site-packages (from keras) (3.12.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venvllm/lib/python3.10/site-packages (from huggingface_hub) (2024.2.0)\n",
      "Requirement already satisfied: filelock in ./venvllm/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./venvllm/lib/python3.10/site-packages (from huggingface_hub) (4.66.5)\n",
      "Requirement already satisfied: requests in ./venvllm/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venvllm/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venvllm/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venvllm/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.8.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venvllm/lib/python3.10/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venvllm/lib/python3.10/site-packages (from requests->huggingface_hub) (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venvllm/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in ./venvllm/lib/python3.10/site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in ./venvllm/lib/python3.10/site-packages (from rich->keras) (0.9.1)\n",
      "Requirement already satisfied: tensorflow in ./venvllm/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: six>=1.12.0 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (1.26.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: setuptools in ./venvllm/lib/python3.10/site-packages (from tensorflow) (59.6.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (4.25.5)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (1.67.1)\n",
      "Requirement already satisfied: packaging in ./venvllm/lib/python3.10/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./venvllm/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: namex in ./venvllm/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in ./venvllm/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: rich in ./venvllm/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (12.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venvllm/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venvllm/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venvllm/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venvllm/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./venvllm/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./venvllm/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./venvllm/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./venvllm/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in ./venvllm/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (0.9.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in ./venvllm/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: python-dotenv in ./venvllm/lib/python3.10/site-packages (1.0.1)\n",
      "Requirement already satisfied: zstandard in ./venvllm/lib/python3.10/site-packages (0.23.0)\n"
     ]
    }
   ],
   "source": [
    "# Required installations for transformers and datasets\n",
    "!pip install transformers datasets\n",
    "!pip install keras huggingface_hub\n",
    "!pip install tensorflow\n",
    "!pip install python-dotenv\n",
    "!pip install zstandard\n",
    "#!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 09:44:40.027376: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732265080.041495 1778897 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732265080.045567 1778897 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-22 09:44:40.060991: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer activations - LLaMA 3.2 3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/drew99/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Hugging Face Login (get token from os environment)\n",
    "# login(os.getenv(\"HF_ACCESS_TOKEN\"))\n",
    "\n",
    "# notebook_login()\n",
    "\n",
    "\n",
    "# Could also later use \n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ff2dd5b42c437a8f93a6c97296b84c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the LLaMA 3.2B model without quantization (for now)\n",
    "model_name = \"meta-llama/Llama-3.2-3B\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device) # 16-bit precision\n",
    "\n",
    "# Some other LLaMA models to try:\n",
    "# https://huggingface.co/meta-llama/Llama-3.2-3B\"\n",
    "# https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct-QLORA_INT4_EO8\n",
    "# https://huggingface.co/mradermacher/Fireball-Meta-Llama-3.2-8B-Instruct-agent-003-128k-code-DPO-i1-GGUF  i1-Q4_K_M\n",
    "# https://huggingface.co/TheBloke/Llama-2-7B-Chat-GPTQ\n",
    "\n",
    "# Prepare 4-bit quantization configuration (optional)\n",
    "# Uncomment the following lines if you wish to use quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (1): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (2): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (3): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (4): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (5): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (6): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (7): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (8): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (9): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (10): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (11): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (12): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (13): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (14): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (15): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (16): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (17): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (18): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (19): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (20): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (21): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (22): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (23): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (24): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (25): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (26): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (27): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.model.layers) # Specific to LLaMA model # https://huggingface.co/docs/transformers/main/model_doc/llama#transformers.LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_cache = []\n",
    "def save_activations_hook(module, input, output):\n",
    "    # input is a tuple; input[0] is the tensor we need\n",
    "    activation_cache.append(input[0].cpu().detach().numpy())\n",
    "\n",
    "# Register hook on the 16th layer\n",
    "layer_index = 15  # Zero-based index; 15 corresponds to the 16th layer\n",
    "hook_handle = model.model.layers[layer_index].register_forward_hook(save_activations_hook) \n",
    "# for other models, you may need to use model.transformer.h[15] instead of model.model.layers[15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual stream shape at layer 16: (1, 6, 3072)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example input\n",
    "input_text = \"Your input text here.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Access the residual stream activation\n",
    "residual_stream = activation_cache[0]\n",
    "print(f\"Residual stream shape at layer {layer_index+1}: {residual_stream.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e8ecbc60a644bfa9d2897eba75a35e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset and saving activations in batches...\n",
      "all_data shape: (2910, 3075)\n",
      "all_data shape: (5254, 3075)\n",
      "all_data shape: (7698, 3075)\n",
      "all_data shape: (10530, 3075)\n",
      "all_data shape: (13452, 3075)\n",
      "all_data shape: (16771, 3075)\n",
      "all_data shape: (19101, 3075)\n",
      "all_data shape: (21961, 3075)\n",
      "all_data shape: (24559, 3075)\n",
      "all_data shape: (27488, 3075)\n",
      "all_data shape: (30681, 3075)\n",
      "all_data shape: (32968, 3075)\n",
      "all_data shape: (35208, 3075)\n",
      "all_data shape: (37700, 3075)\n",
      "all_data shape: (40361, 3075)\n",
      "all_data shape: (43586, 3075)\n",
      "all_data shape: (46561, 3075)\n",
      "all_data shape: (49329, 3075)\n",
      "all_data shape: (52517, 3075)\n",
      "all_data shape: (55162, 3075)\n",
      "all_data shape: (58179, 3075)\n",
      "all_data shape: (61216, 3075)\n",
      "all_data shape: (64546, 3075)\n",
      "all_data shape: (67435, 3075)\n",
      "all_data shape: (70601, 3075)\n",
      "all_data shape: (73867, 3075)\n",
      "all_data shape: (76587, 3075)\n",
      "all_data shape: (79081, 3075)\n",
      "all_data shape: (81939, 3075)\n",
      "Saved file 1 == 81920 examples\n",
      "all_data shape: (2555, 3075)\n",
      "all_data shape: (5696, 3075)\n",
      "all_data shape: (8355, 3075)\n",
      "all_data shape: (10896, 3075)\n",
      "all_data shape: (14261, 3075)\n",
      "all_data shape: (17119, 3075)\n",
      "all_data shape: (19211, 3075)\n",
      "all_data shape: (22187, 3075)\n",
      "all_data shape: (24274, 3075)\n",
      "all_data shape: (27584, 3075)\n",
      "all_data shape: (30584, 3075)\n",
      "all_data shape: (33554, 3075)\n",
      "all_data shape: (35760, 3075)\n",
      "all_data shape: (38183, 3075)\n",
      "all_data shape: (41165, 3075)\n",
      "all_data shape: (43734, 3075)\n",
      "all_data shape: (46274, 3075)\n",
      "all_data shape: (49317, 3075)\n",
      "all_data shape: (52241, 3075)\n",
      "all_data shape: (54818, 3075)\n",
      "all_data shape: (57043, 3075)\n",
      "all_data shape: (59749, 3075)\n",
      "all_data shape: (62916, 3075)\n",
      "all_data shape: (65123, 3075)\n",
      "all_data shape: (67729, 3075)\n",
      "all_data shape: (70168, 3075)\n",
      "all_data shape: (72606, 3075)\n",
      "all_data shape: (75496, 3075)\n",
      "all_data shape: (78573, 3075)\n",
      "all_data shape: (81117, 3075)\n",
      "all_data shape: (84459, 3075)\n",
      "Saved file 2 == 163840 examples\n",
      "all_data shape: (4582, 3075)\n",
      "all_data shape: (8021, 3075)\n",
      "all_data shape: (10730, 3075)\n",
      "all_data shape: (13031, 3075)\n",
      "all_data shape: (16145, 3075)\n",
      "all_data shape: (18590, 3075)\n",
      "all_data shape: (21637, 3075)\n",
      "all_data shape: (24140, 3075)\n",
      "all_data shape: (26609, 3075)\n",
      "all_data shape: (29472, 3075)\n",
      "all_data shape: (32280, 3075)\n",
      "all_data shape: (35248, 3075)\n",
      "all_data shape: (38613, 3075)\n",
      "all_data shape: (41151, 3075)\n",
      "all_data shape: (44323, 3075)\n",
      "all_data shape: (47990, 3075)\n",
      "all_data shape: (49925, 3075)\n",
      "all_data shape: (52386, 3075)\n",
      "all_data shape: (54911, 3075)\n",
      "all_data shape: (57994, 3075)\n",
      "all_data shape: (60925, 3075)\n",
      "all_data shape: (63831, 3075)\n",
      "all_data shape: (67003, 3075)\n",
      "all_data shape: (70006, 3075)\n",
      "all_data shape: (72849, 3075)\n",
      "all_data shape: (75952, 3075)\n",
      "all_data shape: (78606, 3075)\n",
      "all_data shape: (81146, 3075)\n",
      "all_data shape: (84311, 3075)\n",
      "Saved file 3 == 245760 examples\n",
      "all_data shape: (5300, 3075)\n",
      "all_data shape: (7777, 3075)\n",
      "all_data shape: (10464, 3075)\n",
      "all_data shape: (13107, 3075)\n",
      "all_data shape: (16235, 3075)\n",
      "all_data shape: (19037, 3075)\n",
      "all_data shape: (21421, 3075)\n",
      "all_data shape: (24470, 3075)\n",
      "all_data shape: (27406, 3075)\n",
      "all_data shape: (29741, 3075)\n",
      "all_data shape: (32304, 3075)\n",
      "all_data shape: (34348, 3075)\n",
      "all_data shape: (37092, 3075)\n",
      "all_data shape: (39384, 3075)\n",
      "all_data shape: (42400, 3075)\n",
      "all_data shape: (45857, 3075)\n",
      "all_data shape: (48333, 3075)\n",
      "all_data shape: (50990, 3075)\n",
      "all_data shape: (52882, 3075)\n",
      "all_data shape: (55541, 3075)\n",
      "all_data shape: (58274, 3075)\n",
      "all_data shape: (61272, 3075)\n",
      "all_data shape: (63695, 3075)\n",
      "all_data shape: (66199, 3075)\n",
      "all_data shape: (68213, 3075)\n",
      "all_data shape: (71616, 3075)\n",
      "all_data shape: (73758, 3075)\n",
      "all_data shape: (76332, 3075)\n",
      "all_data shape: (79315, 3075)\n",
      "all_data shape: (81502, 3075)\n",
      "all_data shape: (84158, 3075)\n",
      "Saved file 4 == 327680 examples\n",
      "all_data shape: (3856, 3075)\n",
      "all_data shape: (6284, 3075)\n",
      "all_data shape: (8761, 3075)\n",
      "all_data shape: (11554, 3075)\n",
      "all_data shape: (13855, 3075)\n",
      "all_data shape: (16974, 3075)\n",
      "all_data shape: (19971, 3075)\n",
      "all_data shape: (23683, 3075)\n",
      "all_data shape: (27018, 3075)\n",
      "all_data shape: (29989, 3075)\n",
      "all_data shape: (32257, 3075)\n",
      "all_data shape: (34340, 3075)\n",
      "all_data shape: (37511, 3075)\n",
      "all_data shape: (40592, 3075)\n",
      "all_data shape: (44225, 3075)\n",
      "all_data shape: (47185, 3075)\n",
      "all_data shape: (49666, 3075)\n",
      "all_data shape: (52419, 3075)\n",
      "all_data shape: (55652, 3075)\n",
      "all_data shape: (58304, 3075)\n",
      "all_data shape: (60566, 3075)\n",
      "all_data shape: (63608, 3075)\n",
      "all_data shape: (66476, 3075)\n",
      "all_data shape: (69363, 3075)\n",
      "all_data shape: (72624, 3075)\n",
      "all_data shape: (75772, 3075)\n",
      "all_data shape: (78233, 3075)\n",
      "all_data shape: (82063, 3075)\n",
      "Saved file 5 == 409600 examples\n",
      "all_data shape: (3239, 3075)\n",
      "all_data shape: (5833, 3075)\n",
      "all_data shape: (8346, 3075)\n",
      "all_data shape: (10895, 3075)\n",
      "all_data shape: (13597, 3075)\n",
      "all_data shape: (16655, 3075)\n",
      "all_data shape: (19175, 3075)\n",
      "all_data shape: (21990, 3075)\n",
      "all_data shape: (25023, 3075)\n",
      "all_data shape: (28446, 3075)\n",
      "all_data shape: (31336, 3075)\n",
      "all_data shape: (34046, 3075)\n",
      "all_data shape: (37092, 3075)\n",
      "all_data shape: (39488, 3075)\n",
      "all_data shape: (42899, 3075)\n",
      "all_data shape: (46124, 3075)\n",
      "all_data shape: (48360, 3075)\n",
      "all_data shape: (51732, 3075)\n",
      "all_data shape: (55137, 3075)\n",
      "all_data shape: (57418, 3075)\n",
      "all_data shape: (59491, 3075)\n",
      "all_data shape: (62880, 3075)\n",
      "all_data shape: (66343, 3075)\n",
      "all_data shape: (69679, 3075)\n",
      "all_data shape: (72203, 3075)\n",
      "all_data shape: (74774, 3075)\n",
      "all_data shape: (76880, 3075)\n",
      "all_data shape: (80179, 3075)\n",
      "all_data shape: (83234, 3075)\n",
      "Saved file 6 == 491520 examples\n",
      "all_data shape: (4278, 3075)\n",
      "all_data shape: (7376, 3075)\n",
      "all_data shape: (9955, 3075)\n",
      "all_data shape: (13648, 3075)\n",
      "all_data shape: (16870, 3075)\n",
      "all_data shape: (19569, 3075)\n",
      "all_data shape: (22521, 3075)\n",
      "all_data shape: (25567, 3075)\n",
      "all_data shape: (28118, 3075)\n",
      "all_data shape: (30423, 3075)\n",
      "all_data shape: (32770, 3075)\n",
      "all_data shape: (34812, 3075)\n",
      "all_data shape: (37711, 3075)\n",
      "all_data shape: (40429, 3075)\n",
      "all_data shape: (43985, 3075)\n",
      "all_data shape: (47434, 3075)\n",
      "all_data shape: (50499, 3075)\n",
      "all_data shape: (52961, 3075)\n",
      "all_data shape: (56038, 3075)\n",
      "all_data shape: (58919, 3075)\n",
      "all_data shape: (61354, 3075)\n",
      "all_data shape: (64083, 3075)\n",
      "all_data shape: (67354, 3075)\n",
      "all_data shape: (70855, 3075)\n",
      "all_data shape: (73961, 3075)\n",
      "all_data shape: (76721, 3075)\n",
      "all_data shape: (79709, 3075)\n",
      "all_data shape: (82403, 3075)\n",
      "Saved file 7 == 573440 examples\n",
      "all_data shape: (3559, 3075)\n",
      "all_data shape: (5941, 3075)\n",
      "all_data shape: (8794, 3075)\n",
      "all_data shape: (11276, 3075)\n",
      "all_data shape: (13894, 3075)\n",
      "all_data shape: (16970, 3075)\n",
      "all_data shape: (20376, 3075)\n",
      "all_data shape: (23299, 3075)\n",
      "all_data shape: (26400, 3075)\n",
      "all_data shape: (29633, 3075)\n",
      "all_data shape: (31949, 3075)\n",
      "all_data shape: (34367, 3075)\n",
      "all_data shape: (37639, 3075)\n",
      "all_data shape: (40030, 3075)\n",
      "all_data shape: (43353, 3075)\n",
      "all_data shape: (46002, 3075)\n",
      "all_data shape: (48820, 3075)\n",
      "all_data shape: (50465, 3075)\n",
      "all_data shape: (52874, 3075)\n",
      "all_data shape: (56143, 3075)\n",
      "all_data shape: (59715, 3075)\n",
      "all_data shape: (61691, 3075)\n",
      "all_data shape: (64424, 3075)\n",
      "all_data shape: (66717, 3075)\n",
      "all_data shape: (69514, 3075)\n",
      "all_data shape: (72921, 3075)\n",
      "all_data shape: (75934, 3075)\n",
      "all_data shape: (78786, 3075)\n",
      "all_data shape: (80839, 3075)\n",
      "all_data shape: (84346, 3075)\n",
      "Saved file 8 == 655360 examples\n",
      "all_data shape: (4995, 3075)\n",
      "all_data shape: (8565, 3075)\n",
      "all_data shape: (11409, 3075)\n",
      "all_data shape: (14610, 3075)\n",
      "all_data shape: (17145, 3075)\n",
      "all_data shape: (19923, 3075)\n",
      "all_data shape: (21822, 3075)\n",
      "all_data shape: (24111, 3075)\n",
      "all_data shape: (26809, 3075)\n",
      "all_data shape: (29551, 3075)\n",
      "all_data shape: (32074, 3075)\n",
      "all_data shape: (35241, 3075)\n",
      "all_data shape: (38299, 3075)\n",
      "all_data shape: (40535, 3075)\n",
      "all_data shape: (43971, 3075)\n",
      "all_data shape: (45662, 3075)\n",
      "all_data shape: (48868, 3075)\n",
      "all_data shape: (51564, 3075)\n",
      "all_data shape: (54924, 3075)\n",
      "all_data shape: (58550, 3075)\n",
      "all_data shape: (61707, 3075)\n",
      "all_data shape: (64749, 3075)\n",
      "all_data shape: (67536, 3075)\n",
      "all_data shape: (70411, 3075)\n",
      "all_data shape: (73198, 3075)\n",
      "all_data shape: (75101, 3075)\n",
      "all_data shape: (77916, 3075)\n",
      "all_data shape: (80417, 3075)\n",
      "all_data shape: (82955, 3075)\n",
      "Saved file 9 == 737280 examples\n",
      "all_data shape: (4128, 3075)\n",
      "all_data shape: (7636, 3075)\n",
      "all_data shape: (10614, 3075)\n",
      "all_data shape: (13443, 3075)\n",
      "all_data shape: (16447, 3075)\n",
      "all_data shape: (20073, 3075)\n",
      "all_data shape: (22751, 3075)\n",
      "all_data shape: (25222, 3075)\n",
      "all_data shape: (27765, 3075)\n",
      "all_data shape: (30859, 3075)\n",
      "all_data shape: (33424, 3075)\n",
      "all_data shape: (36253, 3075)\n",
      "all_data shape: (38900, 3075)\n",
      "all_data shape: (42124, 3075)\n",
      "all_data shape: (45723, 3075)\n",
      "all_data shape: (48108, 3075)\n",
      "all_data shape: (50667, 3075)\n",
      "all_data shape: (53237, 3075)\n",
      "all_data shape: (56594, 3075)\n",
      "all_data shape: (59002, 3075)\n",
      "all_data shape: (61477, 3075)\n",
      "all_data shape: (64622, 3075)\n",
      "all_data shape: (67393, 3075)\n",
      "all_data shape: (70037, 3075)\n",
      "all_data shape: (73204, 3075)\n",
      "all_data shape: (76235, 3075)\n",
      "all_data shape: (78449, 3075)\n",
      "all_data shape: (81337, 3075)\n",
      "all_data shape: (83959, 3075)\n",
      "Saved file 10 == 819200 examples\n",
      "all_data shape: (4682, 3075)\n",
      "all_data shape: (7559, 3075)\n",
      "all_data shape: (10394, 3075)\n",
      "all_data shape: (13379, 3075)\n",
      "all_data shape: (15999, 3075)\n",
      "all_data shape: (18032, 3075)\n",
      "all_data shape: (21283, 3075)\n",
      "all_data shape: (24291, 3075)\n",
      "all_data shape: (27235, 3075)\n",
      "all_data shape: (29974, 3075)\n",
      "all_data shape: (32822, 3075)\n",
      "all_data shape: (36565, 3075)\n",
      "all_data shape: (39666, 3075)\n",
      "all_data shape: (42189, 3075)\n",
      "all_data shape: (45082, 3075)\n",
      "all_data shape: (48431, 3075)\n",
      "all_data shape: (50839, 3075)\n",
      "all_data shape: (54138, 3075)\n",
      "all_data shape: (57696, 3075)\n",
      "all_data shape: (60471, 3075)\n",
      "all_data shape: (62923, 3075)\n",
      "all_data shape: (65891, 3075)\n",
      "all_data shape: (68739, 3075)\n",
      "all_data shape: (72137, 3075)\n",
      "all_data shape: (75699, 3075)\n",
      "all_data shape: (78647, 3075)\n",
      "all_data shape: (81633, 3075)\n",
      "all_data shape: (84058, 3075)\n",
      "Saved file 11 == 901120 examples\n",
      "all_data shape: (4358, 3075)\n",
      "all_data shape: (7785, 3075)\n",
      "all_data shape: (10812, 3075)\n",
      "all_data shape: (13296, 3075)\n",
      "all_data shape: (16290, 3075)\n",
      "all_data shape: (18893, 3075)\n",
      "all_data shape: (21017, 3075)\n",
      "all_data shape: (23397, 3075)\n",
      "all_data shape: (26261, 3075)\n",
      "all_data shape: (28239, 3075)\n",
      "all_data shape: (30482, 3075)\n",
      "all_data shape: (33376, 3075)\n",
      "all_data shape: (36790, 3075)\n",
      "all_data shape: (38497, 3075)\n",
      "all_data shape: (40877, 3075)\n",
      "all_data shape: (43906, 3075)\n",
      "all_data shape: (46625, 3075)\n",
      "all_data shape: (48366, 3075)\n",
      "all_data shape: (51239, 3075)\n",
      "all_data shape: (54266, 3075)\n",
      "all_data shape: (57201, 3075)\n",
      "all_data shape: (60066, 3075)\n",
      "all_data shape: (62062, 3075)\n",
      "all_data shape: (65202, 3075)\n",
      "all_data shape: (67802, 3075)\n",
      "all_data shape: (70308, 3075)\n",
      "all_data shape: (73871, 3075)\n",
      "all_data shape: (76777, 3075)\n",
      "all_data shape: (79532, 3075)\n",
      "all_data shape: (81929, 3075)\n",
      "Saved file 12 == 983040 examples\n",
      "all_data shape: (3142, 3075)\n",
      "all_data shape: (6320, 3075)\n",
      "all_data shape: (9686, 3075)\n",
      "all_data shape: (12885, 3075)\n",
      "all_data shape: (15930, 3075)\n",
      "all_data shape: (18394, 3075)\n",
      "all_data shape: (21465, 3075)\n",
      "all_data shape: (24493, 3075)\n",
      "all_data shape: (27663, 3075)\n",
      "all_data shape: (30553, 3075)\n",
      "all_data shape: (33467, 3075)\n",
      "all_data shape: (36096, 3075)\n",
      "all_data shape: (38555, 3075)\n",
      "all_data shape: (41196, 3075)\n",
      "all_data shape: (44458, 3075)\n",
      "all_data shape: (46309, 3075)\n",
      "all_data shape: (49607, 3075)\n",
      "all_data shape: (51875, 3075)\n",
      "all_data shape: (55219, 3075)\n",
      "all_data shape: (57892, 3075)\n",
      "all_data shape: (60109, 3075)\n",
      "all_data shape: (63910, 3075)\n",
      "all_data shape: (67757, 3075)\n",
      "all_data shape: (70650, 3075)\n",
      "all_data shape: (73236, 3075)\n",
      "all_data shape: (76271, 3075)\n",
      "all_data shape: (79897, 3075)\n",
      "all_data shape: (82743, 3075)\n",
      "Saved file 13 == 1064960 examples\n",
      "all_data shape: (3723, 3075)\n",
      "all_data shape: (5962, 3075)\n",
      "all_data shape: (9485, 3075)\n",
      "all_data shape: (12085, 3075)\n",
      "all_data shape: (14425, 3075)\n",
      "all_data shape: (17620, 3075)\n",
      "all_data shape: (20751, 3075)\n",
      "all_data shape: (23619, 3075)\n",
      "all_data shape: (25335, 3075)\n",
      "all_data shape: (27447, 3075)\n",
      "all_data shape: (30684, 3075)\n",
      "all_data shape: (33534, 3075)\n",
      "all_data shape: (36835, 3075)\n",
      "all_data shape: (40454, 3075)\n",
      "all_data shape: (43094, 3075)\n",
      "all_data shape: (46378, 3075)\n",
      "all_data shape: (48819, 3075)\n",
      "all_data shape: (52108, 3075)\n",
      "all_data shape: (54767, 3075)\n",
      "all_data shape: (57943, 3075)\n",
      "all_data shape: (60761, 3075)\n",
      "all_data shape: (64305, 3075)\n",
      "all_data shape: (66661, 3075)\n",
      "all_data shape: (69905, 3075)\n",
      "all_data shape: (72685, 3075)\n",
      "all_data shape: (75339, 3075)\n",
      "all_data shape: (78032, 3075)\n",
      "all_data shape: (81388, 3075)\n",
      "all_data shape: (84413, 3075)\n",
      "Saved file 14 == 1146880 examples\n",
      "all_data shape: (5343, 3075)\n",
      "all_data shape: (8437, 3075)\n",
      "all_data shape: (11249, 3075)\n",
      "all_data shape: (13364, 3075)\n",
      "all_data shape: (16583, 3075)\n",
      "all_data shape: (19144, 3075)\n",
      "all_data shape: (22106, 3075)\n",
      "all_data shape: (24776, 3075)\n",
      "all_data shape: (27315, 3075)\n",
      "all_data shape: (29602, 3075)\n",
      "all_data shape: (33111, 3075)\n",
      "all_data shape: (36533, 3075)\n",
      "all_data shape: (39243, 3075)\n",
      "all_data shape: (42612, 3075)\n",
      "all_data shape: (45155, 3075)\n",
      "all_data shape: (47176, 3075)\n",
      "all_data shape: (50240, 3075)\n",
      "all_data shape: (53197, 3075)\n",
      "all_data shape: (56742, 3075)\n",
      "all_data shape: (59636, 3075)\n",
      "all_data shape: (62616, 3075)\n",
      "all_data shape: (65334, 3075)\n",
      "all_data shape: (68666, 3075)\n",
      "all_data shape: (72012, 3075)\n",
      "all_data shape: (75387, 3075)\n",
      "all_data shape: (78242, 3075)\n",
      "all_data shape: (81013, 3075)\n",
      "all_data shape: (84285, 3075)\n",
      "Saved file 15 == 1228800 examples\n",
      "all_data shape: (5488, 3075)\n",
      "all_data shape: (8694, 3075)\n",
      "all_data shape: (11057, 3075)\n",
      "all_data shape: (14264, 3075)\n",
      "all_data shape: (17082, 3075)\n",
      "all_data shape: (19490, 3075)\n",
      "all_data shape: (23125, 3075)\n",
      "all_data shape: (25609, 3075)\n",
      "all_data shape: (28431, 3075)\n",
      "all_data shape: (31788, 3075)\n",
      "all_data shape: (34849, 3075)\n",
      "all_data shape: (36999, 3075)\n",
      "all_data shape: (39989, 3075)\n",
      "all_data shape: (42808, 3075)\n",
      "all_data shape: (45495, 3075)\n",
      "all_data shape: (48038, 3075)\n",
      "all_data shape: (50066, 3075)\n",
      "all_data shape: (53112, 3075)\n",
      "all_data shape: (55980, 3075)\n",
      "all_data shape: (57736, 3075)\n",
      "all_data shape: (59979, 3075)\n",
      "all_data shape: (62907, 3075)\n",
      "all_data shape: (65925, 3075)\n",
      "all_data shape: (68816, 3075)\n",
      "all_data shape: (71653, 3075)\n",
      "all_data shape: (74636, 3075)\n",
      "all_data shape: (77396, 3075)\n",
      "all_data shape: (79530, 3075)\n",
      "all_data shape: (82289, 3075)\n",
      "Saved file 16 == 1310720 examples\n",
      "all_data shape: (3283, 3075)\n",
      "all_data shape: (6291, 3075)\n",
      "all_data shape: (9008, 3075)\n",
      "all_data shape: (12033, 3075)\n",
      "all_data shape: (14736, 3075)\n",
      "all_data shape: (18035, 3075)\n",
      "all_data shape: (20770, 3075)\n",
      "all_data shape: (23603, 3075)\n",
      "all_data shape: (26312, 3075)\n",
      "all_data shape: (29397, 3075)\n",
      "all_data shape: (32767, 3075)\n",
      "all_data shape: (36002, 3075)\n",
      "all_data shape: (38781, 3075)\n",
      "all_data shape: (41730, 3075)\n",
      "all_data shape: (44612, 3075)\n",
      "all_data shape: (48209, 3075)\n",
      "all_data shape: (50605, 3075)\n",
      "all_data shape: (53429, 3075)\n",
      "all_data shape: (57254, 3075)\n",
      "all_data shape: (59500, 3075)\n",
      "all_data shape: (62432, 3075)\n",
      "all_data shape: (65226, 3075)\n",
      "all_data shape: (68063, 3075)\n",
      "all_data shape: (70869, 3075)\n",
      "all_data shape: (73562, 3075)\n",
      "all_data shape: (76314, 3075)\n",
      "all_data shape: (78924, 3075)\n",
      "all_data shape: (81952, 3075)\n",
      "Saved file 17 == 1392640 examples\n",
      "all_data shape: (2858, 3075)\n",
      "all_data shape: (5672, 3075)\n",
      "all_data shape: (8617, 3075)\n",
      "all_data shape: (10354, 3075)\n",
      "all_data shape: (13088, 3075)\n",
      "all_data shape: (16552, 3075)\n",
      "all_data shape: (19511, 3075)\n",
      "all_data shape: (22348, 3075)\n",
      "all_data shape: (25160, 3075)\n",
      "all_data shape: (27802, 3075)\n",
      "all_data shape: (31149, 3075)\n",
      "all_data shape: (34305, 3075)\n",
      "all_data shape: (36813, 3075)\n",
      "all_data shape: (39533, 3075)\n",
      "all_data shape: (42694, 3075)\n",
      "all_data shape: (45354, 3075)\n",
      "all_data shape: (48230, 3075)\n",
      "all_data shape: (50757, 3075)\n",
      "all_data shape: (53601, 3075)\n",
      "all_data shape: (56499, 3075)\n",
      "all_data shape: (59186, 3075)\n",
      "all_data shape: (61963, 3075)\n",
      "all_data shape: (64439, 3075)\n",
      "all_data shape: (66954, 3075)\n",
      "all_data shape: (69721, 3075)\n",
      "all_data shape: (72625, 3075)\n",
      "all_data shape: (75666, 3075)\n",
      "all_data shape: (78596, 3075)\n",
      "all_data shape: (81501, 3075)\n",
      "all_data shape: (84106, 3075)\n",
      "Saved file 18 == 1474560 examples\n",
      "all_data shape: (5734, 3075)\n",
      "all_data shape: (9450, 3075)\n",
      "all_data shape: (12939, 3075)\n",
      "all_data shape: (15598, 3075)\n",
      "all_data shape: (18393, 3075)\n",
      "all_data shape: (21123, 3075)\n",
      "all_data shape: (23856, 3075)\n",
      "all_data shape: (27357, 3075)\n",
      "all_data shape: (30841, 3075)\n",
      "all_data shape: (33648, 3075)\n",
      "all_data shape: (36544, 3075)\n",
      "all_data shape: (38549, 3075)\n",
      "all_data shape: (41499, 3075)\n",
      "all_data shape: (44280, 3075)\n",
      "all_data shape: (47035, 3075)\n",
      "all_data shape: (50552, 3075)\n",
      "all_data shape: (53824, 3075)\n",
      "all_data shape: (56632, 3075)\n",
      "all_data shape: (59670, 3075)\n",
      "all_data shape: (62641, 3075)\n",
      "all_data shape: (65885, 3075)\n",
      "all_data shape: (69410, 3075)\n",
      "all_data shape: (72468, 3075)\n",
      "all_data shape: (75106, 3075)\n",
      "all_data shape: (77420, 3075)\n",
      "all_data shape: (80434, 3075)\n",
      "all_data shape: (83099, 3075)\n",
      "Saved file 19 == 1556480 examples\n",
      "all_data shape: (3383, 3075)\n",
      "all_data shape: (5879, 3075)\n",
      "all_data shape: (7823, 3075)\n",
      "all_data shape: (10011, 3075)\n",
      "all_data shape: (13083, 3075)\n",
      "all_data shape: (16176, 3075)\n",
      "all_data shape: (18484, 3075)\n",
      "all_data shape: (21987, 3075)\n",
      "all_data shape: (24550, 3075)\n",
      "all_data shape: (27711, 3075)\n",
      "all_data shape: (30296, 3075)\n",
      "all_data shape: (33093, 3075)\n",
      "all_data shape: (36825, 3075)\n",
      "all_data shape: (39338, 3075)\n",
      "all_data shape: (42146, 3075)\n",
      "all_data shape: (44953, 3075)\n",
      "all_data shape: (48169, 3075)\n",
      "all_data shape: (51309, 3075)\n",
      "all_data shape: (54321, 3075)\n",
      "all_data shape: (57228, 3075)\n",
      "all_data shape: (60035, 3075)\n",
      "all_data shape: (62311, 3075)\n",
      "all_data shape: (65305, 3075)\n",
      "all_data shape: (68506, 3075)\n",
      "all_data shape: (70724, 3075)\n",
      "all_data shape: (73994, 3075)\n",
      "all_data shape: (77076, 3075)\n",
      "all_data shape: (80218, 3075)\n",
      "all_data shape: (83758, 3075)\n",
      "Saved file 20 == 1638400 examples\n",
      "all_data shape: (4635, 3075)\n",
      "all_data shape: (7764, 3075)\n",
      "all_data shape: (11311, 3075)\n",
      "all_data shape: (14356, 3075)\n",
      "all_data shape: (16827, 3075)\n",
      "all_data shape: (19480, 3075)\n",
      "all_data shape: (21914, 3075)\n",
      "all_data shape: (24739, 3075)\n",
      "all_data shape: (27564, 3075)\n",
      "all_data shape: (30106, 3075)\n",
      "all_data shape: (32176, 3075)\n",
      "all_data shape: (35913, 3075)\n",
      "all_data shape: (38474, 3075)\n",
      "all_data shape: (41716, 3075)\n",
      "all_data shape: (44932, 3075)\n",
      "all_data shape: (47981, 3075)\n",
      "all_data shape: (50931, 3075)\n",
      "all_data shape: (53399, 3075)\n",
      "all_data shape: (55898, 3075)\n",
      "all_data shape: (58441, 3075)\n",
      "all_data shape: (61472, 3075)\n",
      "all_data shape: (64679, 3075)\n",
      "all_data shape: (67058, 3075)\n",
      "all_data shape: (70367, 3075)\n",
      "all_data shape: (72969, 3075)\n",
      "all_data shape: (76146, 3075)\n",
      "all_data shape: (79227, 3075)\n",
      "all_data shape: (81475, 3075)\n",
      "all_data shape: (84112, 3075)\n",
      "Saved file 21 == 1720320 examples\n",
      "all_data shape: (5002, 3075)\n",
      "all_data shape: (7529, 3075)\n",
      "all_data shape: (9861, 3075)\n",
      "all_data shape: (12088, 3075)\n",
      "all_data shape: (14942, 3075)\n",
      "all_data shape: (17329, 3075)\n",
      "all_data shape: (20373, 3075)\n",
      "all_data shape: (23794, 3075)\n",
      "all_data shape: (26744, 3075)\n",
      "all_data shape: (30429, 3075)\n",
      "all_data shape: (33450, 3075)\n",
      "all_data shape: (36666, 3075)\n",
      "all_data shape: (40157, 3075)\n",
      "all_data shape: (42862, 3075)\n",
      "all_data shape: (45861, 3075)\n",
      "all_data shape: (48728, 3075)\n",
      "all_data shape: (51272, 3075)\n",
      "all_data shape: (54208, 3075)\n",
      "all_data shape: (56686, 3075)\n",
      "all_data shape: (59475, 3075)\n",
      "all_data shape: (61794, 3075)\n",
      "all_data shape: (64725, 3075)\n",
      "all_data shape: (68120, 3075)\n",
      "all_data shape: (71277, 3075)\n",
      "all_data shape: (74853, 3075)\n",
      "all_data shape: (77617, 3075)\n",
      "all_data shape: (80188, 3075)\n",
      "all_data shape: (82484, 3075)\n",
      "Saved file 22 == 1802240 examples\n",
      "all_data shape: (2809, 3075)\n",
      "all_data shape: (5635, 3075)\n",
      "all_data shape: (8741, 3075)\n",
      "all_data shape: (11899, 3075)\n",
      "all_data shape: (14302, 3075)\n",
      "all_data shape: (18009, 3075)\n",
      "all_data shape: (21246, 3075)\n",
      "all_data shape: (24506, 3075)\n",
      "all_data shape: (26465, 3075)\n",
      "all_data shape: (29312, 3075)\n",
      "all_data shape: (32125, 3075)\n",
      "all_data shape: (35533, 3075)\n",
      "all_data shape: (37391, 3075)\n",
      "all_data shape: (39893, 3075)\n",
      "all_data shape: (42601, 3075)\n",
      "all_data shape: (45270, 3075)\n",
      "all_data shape: (48213, 3075)\n",
      "all_data shape: (51206, 3075)\n",
      "all_data shape: (53458, 3075)\n",
      "all_data shape: (55871, 3075)\n",
      "all_data shape: (58051, 3075)\n",
      "all_data shape: (61248, 3075)\n",
      "all_data shape: (63938, 3075)\n",
      "all_data shape: (67170, 3075)\n",
      "all_data shape: (70592, 3075)\n",
      "all_data shape: (74013, 3075)\n",
      "all_data shape: (77088, 3075)\n",
      "all_data shape: (79795, 3075)\n",
      "all_data shape: (83011, 3075)\n",
      "Saved file 23 == 1884160 examples\n",
      "all_data shape: (3651, 3075)\n",
      "all_data shape: (6583, 3075)\n",
      "all_data shape: (9710, 3075)\n",
      "all_data shape: (12763, 3075)\n",
      "all_data shape: (15278, 3075)\n",
      "all_data shape: (17801, 3075)\n",
      "all_data shape: (20497, 3075)\n",
      "all_data shape: (23204, 3075)\n",
      "all_data shape: (25299, 3075)\n",
      "all_data shape: (28802, 3075)\n",
      "all_data shape: (32190, 3075)\n",
      "all_data shape: (34621, 3075)\n",
      "all_data shape: (38202, 3075)\n",
      "all_data shape: (40609, 3075)\n",
      "all_data shape: (43534, 3075)\n",
      "all_data shape: (46200, 3075)\n",
      "all_data shape: (49556, 3075)\n",
      "all_data shape: (52381, 3075)\n",
      "all_data shape: (55272, 3075)\n",
      "all_data shape: (57931, 3075)\n",
      "all_data shape: (60421, 3075)\n",
      "all_data shape: (62947, 3075)\n",
      "all_data shape: (66122, 3075)\n",
      "all_data shape: (69093, 3075)\n",
      "all_data shape: (72104, 3075)\n",
      "all_data shape: (74925, 3075)\n",
      "all_data shape: (77869, 3075)\n",
      "all_data shape: (80405, 3075)\n",
      "all_data shape: (82683, 3075)\n",
      "Saved file 24 == 1966080 examples\n",
      "all_data shape: (3365, 3075)\n",
      "all_data shape: (6818, 3075)\n",
      "all_data shape: (10554, 3075)\n",
      "all_data shape: (12013, 3075)\n",
      "all_data shape: (15307, 3075)\n",
      "all_data shape: (17698, 3075)\n",
      "all_data shape: (21086, 3075)\n",
      "all_data shape: (23919, 3075)\n",
      "all_data shape: (27272, 3075)\n",
      "all_data shape: (29852, 3075)\n",
      "all_data shape: (33009, 3075)\n",
      "all_data shape: (35643, 3075)\n",
      "all_data shape: (38223, 3075)\n",
      "all_data shape: (41365, 3075)\n",
      "all_data shape: (44208, 3075)\n",
      "all_data shape: (46742, 3075)\n",
      "all_data shape: (49676, 3075)\n",
      "all_data shape: (52522, 3075)\n",
      "all_data shape: (54621, 3075)\n",
      "all_data shape: (57780, 3075)\n",
      "all_data shape: (60582, 3075)\n",
      "all_data shape: (63787, 3075)\n",
      "all_data shape: (67405, 3075)\n",
      "all_data shape: (70767, 3075)\n",
      "all_data shape: (74033, 3075)\n",
      "all_data shape: (77399, 3075)\n",
      "all_data shape: (80110, 3075)\n",
      "all_data shape: (82397, 3075)\n",
      "Saved file 25 == 2048000 examples\n",
      "all_data shape: (3430, 3075)\n",
      "all_data shape: (6120, 3075)\n",
      "all_data shape: (9248, 3075)\n",
      "all_data shape: (12548, 3075)\n",
      "all_data shape: (15385, 3075)\n",
      "all_data shape: (18089, 3075)\n",
      "all_data shape: (20769, 3075)\n",
      "all_data shape: (24395, 3075)\n",
      "all_data shape: (26456, 3075)\n",
      "all_data shape: (28741, 3075)\n",
      "all_data shape: (31996, 3075)\n",
      "all_data shape: (35216, 3075)\n",
      "all_data shape: (38843, 3075)\n",
      "all_data shape: (41111, 3075)\n",
      "all_data shape: (44654, 3075)\n",
      "all_data shape: (48280, 3075)\n",
      "all_data shape: (50738, 3075)\n",
      "all_data shape: (53596, 3075)\n",
      "all_data shape: (56604, 3075)\n",
      "all_data shape: (59874, 3075)\n",
      "all_data shape: (62915, 3075)\n",
      "all_data shape: (65737, 3075)\n",
      "all_data shape: (69027, 3075)\n",
      "all_data shape: (71899, 3075)\n",
      "all_data shape: (75234, 3075)\n",
      "all_data shape: (78128, 3075)\n",
      "all_data shape: (80925, 3075)\n",
      "all_data shape: (84455, 3075)\n",
      "Saved file 26 == 2129920 examples\n",
      "all_data shape: (5187, 3075)\n",
      "all_data shape: (8529, 3075)\n",
      "all_data shape: (10812, 3075)\n",
      "all_data shape: (13911, 3075)\n",
      "all_data shape: (16874, 3075)\n",
      "all_data shape: (20082, 3075)\n",
      "all_data shape: (21613, 3075)\n",
      "all_data shape: (25085, 3075)\n",
      "all_data shape: (28104, 3075)\n",
      "all_data shape: (30064, 3075)\n",
      "all_data shape: (33712, 3075)\n",
      "all_data shape: (36580, 3075)\n",
      "all_data shape: (39176, 3075)\n",
      "all_data shape: (42253, 3075)\n",
      "all_data shape: (46228, 3075)\n",
      "all_data shape: (49319, 3075)\n",
      "all_data shape: (51959, 3075)\n",
      "all_data shape: (55167, 3075)\n",
      "all_data shape: (58197, 3075)\n",
      "all_data shape: (61292, 3075)\n",
      "all_data shape: (64281, 3075)\n",
      "all_data shape: (67324, 3075)\n",
      "all_data shape: (70792, 3075)\n",
      "all_data shape: (73860, 3075)\n",
      "all_data shape: (76256, 3075)\n",
      "all_data shape: (78644, 3075)\n",
      "all_data shape: (81640, 3075)\n",
      "all_data shape: (84747, 3075)\n",
      "Saved file 27 == 2211840 examples\n",
      "all_data shape: (5177, 3075)\n",
      "all_data shape: (7302, 3075)\n",
      "all_data shape: (10575, 3075)\n",
      "all_data shape: (13156, 3075)\n",
      "all_data shape: (15272, 3075)\n",
      "all_data shape: (17736, 3075)\n",
      "all_data shape: (20963, 3075)\n",
      "all_data shape: (23522, 3075)\n",
      "all_data shape: (26680, 3075)\n",
      "all_data shape: (29469, 3075)\n",
      "all_data shape: (32017, 3075)\n",
      "all_data shape: (34407, 3075)\n",
      "all_data shape: (36497, 3075)\n",
      "all_data shape: (39155, 3075)\n",
      "all_data shape: (42295, 3075)\n",
      "all_data shape: (45105, 3075)\n",
      "all_data shape: (48640, 3075)\n",
      "all_data shape: (50681, 3075)\n",
      "all_data shape: (53428, 3075)\n",
      "all_data shape: (55331, 3075)\n",
      "all_data shape: (58404, 3075)\n",
      "all_data shape: (61714, 3075)\n",
      "all_data shape: (64460, 3075)\n",
      "all_data shape: (67484, 3075)\n",
      "all_data shape: (70442, 3075)\n",
      "all_data shape: (73368, 3075)\n",
      "all_data shape: (76268, 3075)\n",
      "all_data shape: (78329, 3075)\n",
      "all_data shape: (80346, 3075)\n",
      "all_data shape: (82514, 3075)\n",
      "Saved file 28 == 2293760 examples\n",
      "all_data shape: (3794, 3075)\n",
      "all_data shape: (6201, 3075)\n",
      "all_data shape: (8967, 3075)\n",
      "all_data shape: (11476, 3075)\n",
      "all_data shape: (14195, 3075)\n",
      "all_data shape: (17069, 3075)\n",
      "all_data shape: (20105, 3075)\n",
      "all_data shape: (22439, 3075)\n",
      "all_data shape: (26067, 3075)\n",
      "all_data shape: (28588, 3075)\n",
      "all_data shape: (30438, 3075)\n",
      "all_data shape: (33535, 3075)\n",
      "all_data shape: (36764, 3075)\n",
      "all_data shape: (39263, 3075)\n",
      "all_data shape: (41708, 3075)\n",
      "all_data shape: (44305, 3075)\n",
      "all_data shape: (46935, 3075)\n",
      "all_data shape: (49559, 3075)\n",
      "all_data shape: (52693, 3075)\n",
      "all_data shape: (55986, 3075)\n",
      "all_data shape: (58785, 3075)\n",
      "all_data shape: (61075, 3075)\n",
      "all_data shape: (64154, 3075)\n",
      "all_data shape: (66819, 3075)\n",
      "all_data shape: (69709, 3075)\n",
      "all_data shape: (72894, 3075)\n",
      "all_data shape: (76407, 3075)\n",
      "all_data shape: (79800, 3075)\n",
      "all_data shape: (82672, 3075)\n",
      "Saved file 29 == 2375680 examples\n",
      "all_data shape: (4321, 3075)\n",
      "all_data shape: (6935, 3075)\n",
      "all_data shape: (9716, 3075)\n",
      "all_data shape: (12457, 3075)\n",
      "all_data shape: (15540, 3075)\n",
      "all_data shape: (17899, 3075)\n",
      "all_data shape: (20784, 3075)\n",
      "all_data shape: (23254, 3075)\n",
      "all_data shape: (26708, 3075)\n",
      "all_data shape: (29430, 3075)\n",
      "all_data shape: (32207, 3075)\n",
      "all_data shape: (35699, 3075)\n",
      "all_data shape: (37990, 3075)\n",
      "all_data shape: (41345, 3075)\n",
      "all_data shape: (44510, 3075)\n",
      "all_data shape: (47931, 3075)\n",
      "all_data shape: (50087, 3075)\n",
      "all_data shape: (51881, 3075)\n",
      "all_data shape: (54888, 3075)\n",
      "all_data shape: (57889, 3075)\n",
      "all_data shape: (60894, 3075)\n",
      "all_data shape: (63482, 3075)\n",
      "all_data shape: (65876, 3075)\n",
      "all_data shape: (68593, 3075)\n",
      "all_data shape: (71348, 3075)\n",
      "all_data shape: (74374, 3075)\n",
      "all_data shape: (76522, 3075)\n",
      "all_data shape: (79468, 3075)\n",
      "all_data shape: (82390, 3075)\n",
      "Saved file 30 == 2457600 examples\n",
      "all_data shape: (3772, 3075)\n",
      "all_data shape: (6964, 3075)\n",
      "all_data shape: (9449, 3075)\n",
      "all_data shape: (12035, 3075)\n",
      "all_data shape: (14644, 3075)\n",
      "all_data shape: (16965, 3075)\n",
      "all_data shape: (19536, 3075)\n",
      "all_data shape: (22141, 3075)\n",
      "all_data shape: (25432, 3075)\n",
      "all_data shape: (27813, 3075)\n",
      "all_data shape: (30694, 3075)\n",
      "all_data shape: (34676, 3075)\n",
      "all_data shape: (36958, 3075)\n",
      "all_data shape: (39895, 3075)\n",
      "all_data shape: (43397, 3075)\n",
      "all_data shape: (46043, 3075)\n",
      "all_data shape: (48832, 3075)\n",
      "all_data shape: (51696, 3075)\n",
      "all_data shape: (54727, 3075)\n",
      "all_data shape: (57616, 3075)\n",
      "all_data shape: (60565, 3075)\n",
      "all_data shape: (63188, 3075)\n",
      "all_data shape: (65999, 3075)\n",
      "all_data shape: (68379, 3075)\n",
      "all_data shape: (70031, 3075)\n",
      "all_data shape: (73401, 3075)\n",
      "all_data shape: (75598, 3075)\n",
      "all_data shape: (78657, 3075)\n",
      "all_data shape: (82156, 3075)\n",
      "Saved file 31 == 2539520 examples\n",
      "all_data shape: (3328, 3075)\n",
      "all_data shape: (6222, 3075)\n",
      "all_data shape: (7949, 3075)\n",
      "all_data shape: (10866, 3075)\n",
      "all_data shape: (13636, 3075)\n",
      "all_data shape: (16113, 3075)\n",
      "all_data shape: (19512, 3075)\n",
      "all_data shape: (22771, 3075)\n",
      "all_data shape: (26107, 3075)\n",
      "all_data shape: (28389, 3075)\n",
      "all_data shape: (31190, 3075)\n",
      "all_data shape: (34290, 3075)\n",
      "all_data shape: (37972, 3075)\n",
      "all_data shape: (40804, 3075)\n",
      "all_data shape: (44182, 3075)\n",
      "all_data shape: (46832, 3075)\n",
      "all_data shape: (49482, 3075)\n",
      "all_data shape: (52424, 3075)\n",
      "all_data shape: (55311, 3075)\n",
      "all_data shape: (57940, 3075)\n",
      "all_data shape: (61518, 3075)\n",
      "all_data shape: (63538, 3075)\n",
      "all_data shape: (66232, 3075)\n",
      "all_data shape: (68675, 3075)\n",
      "all_data shape: (71541, 3075)\n",
      "all_data shape: (74100, 3075)\n",
      "all_data shape: (76605, 3075)\n",
      "all_data shape: (79374, 3075)\n",
      "all_data shape: (81992, 3075)\n",
      "Saved file 32 == 2621440 examples\n",
      "all_data shape: (3077, 3075)\n",
      "all_data shape: (5666, 3075)\n",
      "all_data shape: (8959, 3075)\n",
      "all_data shape: (12073, 3075)\n",
      "all_data shape: (15156, 3075)\n",
      "all_data shape: (18771, 3075)\n",
      "all_data shape: (21244, 3075)\n",
      "all_data shape: (23637, 3075)\n",
      "all_data shape: (26900, 3075)\n",
      "all_data shape: (29769, 3075)\n",
      "all_data shape: (32502, 3075)\n",
      "all_data shape: (34974, 3075)\n",
      "all_data shape: (37445, 3075)\n",
      "all_data shape: (40875, 3075)\n",
      "all_data shape: (42955, 3075)\n",
      "all_data shape: (45953, 3075)\n",
      "all_data shape: (49166, 3075)\n",
      "all_data shape: (52493, 3075)\n",
      "all_data shape: (55715, 3075)\n",
      "all_data shape: (58338, 3075)\n",
      "all_data shape: (61813, 3075)\n",
      "all_data shape: (63761, 3075)\n",
      "all_data shape: (66905, 3075)\n",
      "all_data shape: (69692, 3075)\n",
      "all_data shape: (73166, 3075)\n",
      "all_data shape: (75950, 3075)\n",
      "all_data shape: (78498, 3075)\n",
      "all_data shape: (81066, 3075)\n",
      "all_data shape: (84045, 3075)\n",
      "Saved file 33 == 2703360 examples\n",
      "all_data shape: (5172, 3075)\n",
      "all_data shape: (8441, 3075)\n",
      "all_data shape: (11993, 3075)\n",
      "all_data shape: (14514, 3075)\n",
      "all_data shape: (17014, 3075)\n",
      "all_data shape: (19418, 3075)\n",
      "all_data shape: (22367, 3075)\n",
      "all_data shape: (25996, 3075)\n",
      "all_data shape: (28833, 3075)\n",
      "all_data shape: (32094, 3075)\n",
      "all_data shape: (34937, 3075)\n",
      "all_data shape: (37510, 3075)\n",
      "all_data shape: (40538, 3075)\n",
      "all_data shape: (43198, 3075)\n",
      "all_data shape: (46589, 3075)\n",
      "all_data shape: (48890, 3075)\n",
      "all_data shape: (51472, 3075)\n",
      "all_data shape: (54576, 3075)\n",
      "all_data shape: (57578, 3075)\n",
      "all_data shape: (60767, 3075)\n",
      "all_data shape: (63963, 3075)\n",
      "all_data shape: (66406, 3075)\n",
      "all_data shape: (68950, 3075)\n",
      "all_data shape: (71814, 3075)\n",
      "all_data shape: (74880, 3075)\n",
      "all_data shape: (77801, 3075)\n",
      "all_data shape: (80483, 3075)\n",
      "all_data shape: (83556, 3075)\n",
      "Saved file 34 == 2785280 examples\n",
      "all_data shape: (4300, 3075)\n",
      "all_data shape: (7685, 3075)\n",
      "all_data shape: (9975, 3075)\n",
      "all_data shape: (13015, 3075)\n",
      "all_data shape: (16036, 3075)\n",
      "all_data shape: (17814, 3075)\n",
      "all_data shape: (21175, 3075)\n",
      "all_data shape: (24259, 3075)\n",
      "all_data shape: (27032, 3075)\n",
      "all_data shape: (29766, 3075)\n",
      "all_data shape: (33315, 3075)\n",
      "all_data shape: (36511, 3075)\n",
      "all_data shape: (39869, 3075)\n",
      "all_data shape: (42395, 3075)\n",
      "all_data shape: (45338, 3075)\n",
      "all_data shape: (47530, 3075)\n",
      "all_data shape: (50785, 3075)\n",
      "all_data shape: (53910, 3075)\n",
      "all_data shape: (56568, 3075)\n",
      "all_data shape: (59060, 3075)\n",
      "all_data shape: (61384, 3075)\n",
      "all_data shape: (64911, 3075)\n",
      "all_data shape: (67230, 3075)\n",
      "all_data shape: (70619, 3075)\n",
      "all_data shape: (74147, 3075)\n",
      "all_data shape: (76396, 3075)\n",
      "all_data shape: (79775, 3075)\n",
      "all_data shape: (82668, 3075)\n",
      "Saved file 35 == 2867200 examples\n",
      "all_data shape: (3174, 3075)\n",
      "all_data shape: (6345, 3075)\n",
      "all_data shape: (9545, 3075)\n",
      "all_data shape: (12825, 3075)\n",
      "all_data shape: (16416, 3075)\n",
      "all_data shape: (19688, 3075)\n",
      "all_data shape: (21937, 3075)\n",
      "all_data shape: (23931, 3075)\n",
      "all_data shape: (27104, 3075)\n",
      "all_data shape: (29014, 3075)\n",
      "all_data shape: (31915, 3075)\n",
      "all_data shape: (35145, 3075)\n",
      "all_data shape: (37213, 3075)\n",
      "all_data shape: (40376, 3075)\n",
      "all_data shape: (44013, 3075)\n",
      "all_data shape: (46677, 3075)\n",
      "all_data shape: (48884, 3075)\n",
      "all_data shape: (51913, 3075)\n",
      "all_data shape: (54428, 3075)\n",
      "all_data shape: (57738, 3075)\n",
      "all_data shape: (60351, 3075)\n",
      "all_data shape: (63027, 3075)\n",
      "all_data shape: (65913, 3075)\n",
      "all_data shape: (67814, 3075)\n",
      "all_data shape: (70986, 3075)\n",
      "all_data shape: (74508, 3075)\n",
      "all_data shape: (77023, 3075)\n",
      "all_data shape: (80530, 3075)\n",
      "all_data shape: (83334, 3075)\n",
      "Saved file 36 == 2949120 examples\n",
      "all_data shape: (4588, 3075)\n",
      "all_data shape: (7920, 3075)\n",
      "all_data shape: (9354, 3075)\n",
      "all_data shape: (12245, 3075)\n",
      "all_data shape: (14597, 3075)\n",
      "all_data shape: (17641, 3075)\n",
      "all_data shape: (20376, 3075)\n",
      "all_data shape: (22424, 3075)\n",
      "all_data shape: (25637, 3075)\n",
      "all_data shape: (28697, 3075)\n",
      "all_data shape: (31443, 3075)\n",
      "all_data shape: (34639, 3075)\n",
      "all_data shape: (37533, 3075)\n",
      "all_data shape: (39639, 3075)\n",
      "all_data shape: (42227, 3075)\n",
      "all_data shape: (45050, 3075)\n",
      "all_data shape: (47812, 3075)\n",
      "all_data shape: (50722, 3075)\n",
      "all_data shape: (53002, 3075)\n",
      "all_data shape: (55800, 3075)\n",
      "all_data shape: (58705, 3075)\n",
      "all_data shape: (60705, 3075)\n",
      "all_data shape: (63313, 3075)\n",
      "all_data shape: (65520, 3075)\n",
      "all_data shape: (68562, 3075)\n",
      "all_data shape: (71569, 3075)\n",
      "all_data shape: (73829, 3075)\n",
      "all_data shape: (76722, 3075)\n",
      "all_data shape: (79331, 3075)\n",
      "all_data shape: (82221, 3075)\n",
      "Saved file 37 == 3031040 examples\n",
      "all_data shape: (3171, 3075)\n",
      "all_data shape: (6143, 3075)\n",
      "all_data shape: (9332, 3075)\n",
      "all_data shape: (12238, 3075)\n",
      "all_data shape: (15720, 3075)\n",
      "all_data shape: (18832, 3075)\n",
      "all_data shape: (21468, 3075)\n",
      "all_data shape: (23300, 3075)\n",
      "all_data shape: (26253, 3075)\n",
      "all_data shape: (29950, 3075)\n",
      "all_data shape: (32682, 3075)\n",
      "all_data shape: (35706, 3075)\n",
      "all_data shape: (38173, 3075)\n",
      "all_data shape: (41442, 3075)\n",
      "all_data shape: (44554, 3075)\n",
      "all_data shape: (47927, 3075)\n",
      "all_data shape: (50427, 3075)\n",
      "all_data shape: (53623, 3075)\n",
      "all_data shape: (56649, 3075)\n",
      "all_data shape: (58969, 3075)\n",
      "all_data shape: (62251, 3075)\n",
      "all_data shape: (64891, 3075)\n",
      "all_data shape: (67306, 3075)\n",
      "all_data shape: (69601, 3075)\n",
      "all_data shape: (71923, 3075)\n",
      "all_data shape: (74578, 3075)\n",
      "all_data shape: (77130, 3075)\n",
      "all_data shape: (79984, 3075)\n",
      "all_data shape: (82130, 3075)\n",
      "Saved file 38 == 3112960 examples\n",
      "all_data shape: (2993, 3075)\n",
      "all_data shape: (6235, 3075)\n",
      "all_data shape: (9026, 3075)\n",
      "all_data shape: (11687, 3075)\n",
      "all_data shape: (15053, 3075)\n",
      "all_data shape: (18130, 3075)\n",
      "all_data shape: (20966, 3075)\n",
      "all_data shape: (23698, 3075)\n",
      "all_data shape: (26268, 3075)\n",
      "all_data shape: (29378, 3075)\n",
      "all_data shape: (32380, 3075)\n",
      "all_data shape: (34557, 3075)\n",
      "all_data shape: (37737, 3075)\n",
      "all_data shape: (39951, 3075)\n",
      "all_data shape: (43048, 3075)\n",
      "all_data shape: (46602, 3075)\n",
      "all_data shape: (48837, 3075)\n",
      "all_data shape: (51761, 3075)\n",
      "all_data shape: (54557, 3075)\n",
      "all_data shape: (56965, 3075)\n",
      "all_data shape: (59334, 3075)\n",
      "all_data shape: (61493, 3075)\n",
      "all_data shape: (64687, 3075)\n",
      "all_data shape: (67456, 3075)\n",
      "all_data shape: (69781, 3075)\n",
      "all_data shape: (72169, 3075)\n",
      "all_data shape: (75324, 3075)\n",
      "all_data shape: (78560, 3075)\n",
      "all_data shape: (80969, 3075)\n",
      "all_data shape: (84464, 3075)\n",
      "Saved file 39 == 3194880 examples\n",
      "all_data shape: (5714, 3075)\n",
      "all_data shape: (8246, 3075)\n",
      "all_data shape: (10779, 3075)\n",
      "all_data shape: (13753, 3075)\n",
      "all_data shape: (16350, 3075)\n",
      "all_data shape: (19164, 3075)\n",
      "all_data shape: (21482, 3075)\n",
      "all_data shape: (23493, 3075)\n",
      "all_data shape: (26953, 3075)\n",
      "all_data shape: (29675, 3075)\n",
      "all_data shape: (32255, 3075)\n",
      "all_data shape: (35761, 3075)\n",
      "all_data shape: (38086, 3075)\n",
      "all_data shape: (40742, 3075)\n",
      "all_data shape: (43812, 3075)\n",
      "all_data shape: (46610, 3075)\n",
      "all_data shape: (50256, 3075)\n",
      "all_data shape: (52547, 3075)\n",
      "all_data shape: (55829, 3075)\n",
      "all_data shape: (58515, 3075)\n",
      "all_data shape: (61780, 3075)\n",
      "all_data shape: (63911, 3075)\n",
      "all_data shape: (66949, 3075)\n",
      "all_data shape: (69876, 3075)\n",
      "all_data shape: (72280, 3075)\n",
      "all_data shape: (75282, 3075)\n",
      "all_data shape: (77731, 3075)\n",
      "all_data shape: (81108, 3075)\n",
      "all_data shape: (84439, 3075)\n",
      "Saved file 40 == 3276800 examples\n",
      "all_data shape: (5667, 3075)\n",
      "all_data shape: (8383, 3075)\n",
      "all_data shape: (10780, 3075)\n",
      "all_data shape: (14020, 3075)\n",
      "all_data shape: (17008, 3075)\n",
      "all_data shape: (19996, 3075)\n",
      "all_data shape: (22833, 3075)\n",
      "all_data shape: (25769, 3075)\n",
      "all_data shape: (29291, 3075)\n",
      "all_data shape: (32568, 3075)\n",
      "all_data shape: (35133, 3075)\n",
      "all_data shape: (37703, 3075)\n",
      "all_data shape: (40440, 3075)\n",
      "all_data shape: (43447, 3075)\n",
      "all_data shape: (46334, 3075)\n",
      "all_data shape: (48809, 3075)\n",
      "all_data shape: (52186, 3075)\n",
      "all_data shape: (54389, 3075)\n",
      "all_data shape: (57347, 3075)\n",
      "all_data shape: (60135, 3075)\n",
      "all_data shape: (63070, 3075)\n",
      "all_data shape: (65320, 3075)\n",
      "all_data shape: (67317, 3075)\n",
      "all_data shape: (69279, 3075)\n",
      "all_data shape: (71662, 3075)\n",
      "all_data shape: (74029, 3075)\n",
      "all_data shape: (76940, 3075)\n",
      "all_data shape: (80210, 3075)\n",
      "all_data shape: (83294, 3075)\n",
      "Saved file 41 == 3358720 examples\n",
      "all_data shape: (4369, 3075)\n",
      "all_data shape: (7130, 3075)\n",
      "all_data shape: (11107, 3075)\n",
      "all_data shape: (14544, 3075)\n",
      "all_data shape: (17096, 3075)\n",
      "all_data shape: (19685, 3075)\n",
      "all_data shape: (22438, 3075)\n",
      "all_data shape: (25372, 3075)\n",
      "all_data shape: (28567, 3075)\n",
      "all_data shape: (31482, 3075)\n",
      "all_data shape: (34158, 3075)\n",
      "all_data shape: (37495, 3075)\n",
      "all_data shape: (39854, 3075)\n",
      "all_data shape: (42305, 3075)\n",
      "all_data shape: (45469, 3075)\n",
      "all_data shape: (48049, 3075)\n",
      "all_data shape: (51080, 3075)\n",
      "all_data shape: (54989, 3075)\n",
      "all_data shape: (56997, 3075)\n",
      "all_data shape: (59203, 3075)\n",
      "all_data shape: (62367, 3075)\n",
      "all_data shape: (65536, 3075)\n",
      "all_data shape: (69023, 3075)\n",
      "all_data shape: (71868, 3075)\n",
      "all_data shape: (74907, 3075)\n",
      "all_data shape: (77559, 3075)\n",
      "all_data shape: (80326, 3075)\n",
      "all_data shape: (82813, 3075)\n",
      "Saved file 42 == 3440640 examples\n",
      "all_data shape: (3380, 3075)\n",
      "all_data shape: (6687, 3075)\n",
      "all_data shape: (9842, 3075)\n",
      "all_data shape: (12260, 3075)\n",
      "all_data shape: (14496, 3075)\n",
      "all_data shape: (17385, 3075)\n",
      "all_data shape: (20424, 3075)\n",
      "all_data shape: (23886, 3075)\n",
      "all_data shape: (26427, 3075)\n",
      "all_data shape: (28722, 3075)\n",
      "all_data shape: (31969, 3075)\n",
      "all_data shape: (33986, 3075)\n",
      "all_data shape: (36263, 3075)\n",
      "all_data shape: (39099, 3075)\n",
      "all_data shape: (42426, 3075)\n",
      "all_data shape: (45420, 3075)\n",
      "all_data shape: (48352, 3075)\n",
      "all_data shape: (51945, 3075)\n",
      "all_data shape: (53558, 3075)\n",
      "all_data shape: (56635, 3075)\n",
      "all_data shape: (60064, 3075)\n",
      "all_data shape: (63003, 3075)\n",
      "all_data shape: (65557, 3075)\n",
      "all_data shape: (68730, 3075)\n",
      "all_data shape: (71068, 3075)\n",
      "all_data shape: (73678, 3075)\n",
      "all_data shape: (76230, 3075)\n",
      "all_data shape: (79634, 3075)\n",
      "all_data shape: (82944, 3075)\n",
      "Saved file 43 == 3522560 examples\n",
      "all_data shape: (3426, 3075)\n",
      "all_data shape: (5681, 3075)\n",
      "all_data shape: (7696, 3075)\n",
      "all_data shape: (11039, 3075)\n",
      "all_data shape: (13675, 3075)\n",
      "all_data shape: (15830, 3075)\n",
      "all_data shape: (18773, 3075)\n",
      "all_data shape: (22411, 3075)\n",
      "all_data shape: (25058, 3075)\n",
      "all_data shape: (27576, 3075)\n",
      "all_data shape: (30327, 3075)\n",
      "all_data shape: (32632, 3075)\n",
      "all_data shape: (35866, 3075)\n",
      "all_data shape: (39071, 3075)\n",
      "all_data shape: (42022, 3075)\n",
      "all_data shape: (45018, 3075)\n",
      "all_data shape: (47809, 3075)\n",
      "all_data shape: (51300, 3075)\n",
      "all_data shape: (53374, 3075)\n",
      "all_data shape: (56714, 3075)\n",
      "all_data shape: (59759, 3075)\n",
      "all_data shape: (62679, 3075)\n",
      "all_data shape: (65627, 3075)\n",
      "all_data shape: (69053, 3075)\n",
      "all_data shape: (71774, 3075)\n",
      "all_data shape: (74747, 3075)\n",
      "all_data shape: (77272, 3075)\n",
      "all_data shape: (80312, 3075)\n",
      "all_data shape: (83427, 3075)\n",
      "Saved file 44 == 3604480 examples\n",
      "all_data shape: (4566, 3075)\n",
      "all_data shape: (6955, 3075)\n",
      "all_data shape: (8846, 3075)\n",
      "all_data shape: (11515, 3075)\n",
      "all_data shape: (14816, 3075)\n",
      "all_data shape: (17767, 3075)\n",
      "all_data shape: (20728, 3075)\n",
      "all_data shape: (23124, 3075)\n",
      "all_data shape: (25702, 3075)\n",
      "all_data shape: (28857, 3075)\n",
      "all_data shape: (31107, 3075)\n",
      "all_data shape: (34709, 3075)\n",
      "all_data shape: (37161, 3075)\n",
      "all_data shape: (40114, 3075)\n",
      "all_data shape: (43379, 3075)\n",
      "all_data shape: (45874, 3075)\n",
      "all_data shape: (48370, 3075)\n",
      "all_data shape: (51213, 3075)\n",
      "all_data shape: (54124, 3075)\n",
      "all_data shape: (56648, 3075)\n",
      "all_data shape: (59991, 3075)\n",
      "all_data shape: (62153, 3075)\n",
      "all_data shape: (66064, 3075)\n",
      "all_data shape: (68668, 3075)\n",
      "all_data shape: (71357, 3075)\n",
      "all_data shape: (74219, 3075)\n",
      "all_data shape: (77902, 3075)\n",
      "all_data shape: (81282, 3075)\n",
      "all_data shape: (83766, 3075)\n",
      "Saved file 45 == 3686400 examples\n",
      "all_data shape: (4566, 3075)\n",
      "all_data shape: (7612, 3075)\n",
      "all_data shape: (11066, 3075)\n",
      "all_data shape: (13970, 3075)\n",
      "all_data shape: (16665, 3075)\n",
      "all_data shape: (19047, 3075)\n",
      "all_data shape: (21419, 3075)\n",
      "all_data shape: (23589, 3075)\n",
      "all_data shape: (26300, 3075)\n",
      "all_data shape: (29303, 3075)\n",
      "all_data shape: (32672, 3075)\n",
      "all_data shape: (36017, 3075)\n",
      "all_data shape: (38539, 3075)\n",
      "all_data shape: (41500, 3075)\n",
      "all_data shape: (43745, 3075)\n",
      "all_data shape: (46451, 3075)\n",
      "all_data shape: (49082, 3075)\n",
      "all_data shape: (51994, 3075)\n",
      "all_data shape: (55214, 3075)\n",
      "all_data shape: (58224, 3075)\n",
      "all_data shape: (60985, 3075)\n",
      "all_data shape: (63789, 3075)\n",
      "all_data shape: (66529, 3075)\n",
      "all_data shape: (69928, 3075)\n",
      "all_data shape: (72841, 3075)\n",
      "all_data shape: (76225, 3075)\n",
      "all_data shape: (79044, 3075)\n",
      "all_data shape: (81962, 3075)\n",
      "Saved file 46 == 3768320 examples\n",
      "all_data shape: (2601, 3075)\n",
      "all_data shape: (5716, 3075)\n",
      "all_data shape: (8955, 3075)\n",
      "all_data shape: (11124, 3075)\n",
      "all_data shape: (14240, 3075)\n",
      "all_data shape: (15734, 3075)\n",
      "all_data shape: (18588, 3075)\n",
      "all_data shape: (21698, 3075)\n",
      "all_data shape: (24410, 3075)\n",
      "all_data shape: (27995, 3075)\n",
      "all_data shape: (30386, 3075)\n",
      "all_data shape: (33151, 3075)\n",
      "all_data shape: (35785, 3075)\n",
      "all_data shape: (38547, 3075)\n",
      "all_data shape: (40982, 3075)\n",
      "all_data shape: (44222, 3075)\n",
      "all_data shape: (45877, 3075)\n",
      "all_data shape: (49013, 3075)\n",
      "all_data shape: (52597, 3075)\n",
      "all_data shape: (56424, 3075)\n",
      "all_data shape: (59848, 3075)\n",
      "all_data shape: (62796, 3075)\n",
      "all_data shape: (65084, 3075)\n",
      "all_data shape: (68549, 3075)\n",
      "all_data shape: (72103, 3075)\n",
      "all_data shape: (75100, 3075)\n",
      "all_data shape: (78551, 3075)\n",
      "all_data shape: (81527, 3075)\n",
      "all_data shape: (84131, 3075)\n",
      "Saved file 47 == 3850240 examples\n",
      "all_data shape: (3992, 3075)\n",
      "all_data shape: (6537, 3075)\n",
      "all_data shape: (9101, 3075)\n",
      "all_data shape: (10813, 3075)\n",
      "all_data shape: (13640, 3075)\n",
      "all_data shape: (17069, 3075)\n",
      "all_data shape: (20224, 3075)\n",
      "all_data shape: (21746, 3075)\n",
      "all_data shape: (25194, 3075)\n",
      "all_data shape: (28618, 3075)\n",
      "all_data shape: (31768, 3075)\n",
      "all_data shape: (34380, 3075)\n",
      "all_data shape: (36361, 3075)\n",
      "all_data shape: (38443, 3075)\n",
      "all_data shape: (42229, 3075)\n",
      "all_data shape: (45270, 3075)\n",
      "all_data shape: (47506, 3075)\n",
      "all_data shape: (50688, 3075)\n",
      "all_data shape: (52576, 3075)\n",
      "all_data shape: (55040, 3075)\n",
      "all_data shape: (56580, 3075)\n",
      "all_data shape: (59207, 3075)\n",
      "all_data shape: (62043, 3075)\n",
      "all_data shape: (64579, 3075)\n",
      "all_data shape: (67445, 3075)\n",
      "all_data shape: (70244, 3075)\n",
      "all_data shape: (73429, 3075)\n",
      "all_data shape: (76135, 3075)\n",
      "all_data shape: (78232, 3075)\n",
      "all_data shape: (81271, 3075)\n",
      "all_data shape: (83688, 3075)\n",
      "Saved file 48 == 3932160 examples\n",
      "all_data shape: (5027, 3075)\n",
      "all_data shape: (8357, 3075)\n",
      "all_data shape: (11404, 3075)\n",
      "all_data shape: (14216, 3075)\n",
      "all_data shape: (17669, 3075)\n",
      "all_data shape: (19375, 3075)\n",
      "all_data shape: (22186, 3075)\n",
      "all_data shape: (24739, 3075)\n",
      "all_data shape: (27454, 3075)\n",
      "all_data shape: (30518, 3075)\n",
      "all_data shape: (33553, 3075)\n",
      "all_data shape: (35195, 3075)\n",
      "all_data shape: (37987, 3075)\n",
      "all_data shape: (40909, 3075)\n",
      "all_data shape: (44115, 3075)\n",
      "all_data shape: (46462, 3075)\n",
      "all_data shape: (48948, 3075)\n",
      "all_data shape: (51699, 3075)\n",
      "all_data shape: (54677, 3075)\n",
      "all_data shape: (57501, 3075)\n",
      "all_data shape: (60507, 3075)\n",
      "all_data shape: (63503, 3075)\n",
      "all_data shape: (66148, 3075)\n",
      "all_data shape: (69047, 3075)\n",
      "all_data shape: (71189, 3075)\n",
      "all_data shape: (74014, 3075)\n",
      "all_data shape: (77307, 3075)\n",
      "all_data shape: (79498, 3075)\n",
      "all_data shape: (82098, 3075)\n",
      "Saved file 49 == 4014080 examples\n",
      "all_data shape: (3854, 3075)\n",
      "all_data shape: (6405, 3075)\n",
      "all_data shape: (9630, 3075)\n",
      "all_data shape: (13041, 3075)\n",
      "all_data shape: (16505, 3075)\n",
      "all_data shape: (19271, 3075)\n",
      "all_data shape: (22352, 3075)\n",
      "all_data shape: (24726, 3075)\n",
      "all_data shape: (27601, 3075)\n",
      "all_data shape: (31083, 3075)\n",
      "all_data shape: (34650, 3075)\n",
      "all_data shape: (37587, 3075)\n",
      "all_data shape: (40025, 3075)\n",
      "all_data shape: (43399, 3075)\n",
      "all_data shape: (46028, 3075)\n",
      "all_data shape: (48699, 3075)\n",
      "all_data shape: (51890, 3075)\n",
      "all_data shape: (54618, 3075)\n",
      "all_data shape: (57208, 3075)\n",
      "all_data shape: (59883, 3075)\n",
      "all_data shape: (62318, 3075)\n",
      "all_data shape: (65386, 3075)\n",
      "all_data shape: (69287, 3075)\n",
      "all_data shape: (72625, 3075)\n",
      "all_data shape: (75569, 3075)\n",
      "all_data shape: (78639, 3075)\n",
      "all_data shape: (81418, 3075)\n",
      "all_data shape: (84654, 3075)\n",
      "Saved file 50 == 4096000 examples\n",
      "all_data shape: (5603, 3075)\n",
      "all_data shape: (8281, 3075)\n",
      "all_data shape: (10890, 3075)\n",
      "all_data shape: (13754, 3075)\n",
      "all_data shape: (15894, 3075)\n",
      "all_data shape: (18514, 3075)\n",
      "all_data shape: (21293, 3075)\n",
      "all_data shape: (24371, 3075)\n",
      "all_data shape: (27708, 3075)\n",
      "all_data shape: (31192, 3075)\n",
      "all_data shape: (33154, 3075)\n",
      "all_data shape: (35623, 3075)\n",
      "all_data shape: (38493, 3075)\n",
      "all_data shape: (40848, 3075)\n",
      "all_data shape: (43052, 3075)\n",
      "all_data shape: (46413, 3075)\n",
      "all_data shape: (49472, 3075)\n",
      "all_data shape: (51919, 3075)\n",
      "all_data shape: (54802, 3075)\n",
      "all_data shape: (57541, 3075)\n",
      "all_data shape: (60799, 3075)\n",
      "all_data shape: (63828, 3075)\n",
      "all_data shape: (67227, 3075)\n",
      "all_data shape: (69822, 3075)\n",
      "all_data shape: (72286, 3075)\n",
      "all_data shape: (73989, 3075)\n",
      "all_data shape: (76044, 3075)\n",
      "all_data shape: (78818, 3075)\n",
      "all_data shape: (82252, 3075)\n",
      "Saved file 51 == 4177920 examples\n",
      "all_data shape: (3225, 3075)\n",
      "all_data shape: (6029, 3075)\n",
      "all_data shape: (8864, 3075)\n",
      "all_data shape: (12205, 3075)\n",
      "all_data shape: (15367, 3075)\n",
      "all_data shape: (18791, 3075)\n",
      "all_data shape: (21985, 3075)\n",
      "all_data shape: (25026, 3075)\n",
      "all_data shape: (27991, 3075)\n",
      "all_data shape: (30477, 3075)\n",
      "all_data shape: (33547, 3075)\n",
      "all_data shape: (36001, 3075)\n",
      "all_data shape: (39058, 3075)\n",
      "all_data shape: (41483, 3075)\n",
      "all_data shape: (43728, 3075)\n",
      "all_data shape: (46117, 3075)\n",
      "all_data shape: (49544, 3075)\n",
      "all_data shape: (52478, 3075)\n",
      "all_data shape: (55540, 3075)\n",
      "all_data shape: (58671, 3075)\n",
      "all_data shape: (61434, 3075)\n",
      "all_data shape: (64569, 3075)\n",
      "all_data shape: (67416, 3075)\n",
      "all_data shape: (69162, 3075)\n",
      "all_data shape: (72289, 3075)\n",
      "all_data shape: (75164, 3075)\n",
      "all_data shape: (78050, 3075)\n",
      "all_data shape: (81322, 3075)\n",
      "all_data shape: (83840, 3075)\n",
      "Saved file 52 == 4259840 examples\n",
      "all_data shape: (4680, 3075)\n",
      "all_data shape: (7946, 3075)\n",
      "all_data shape: (10929, 3075)\n",
      "all_data shape: (14025, 3075)\n",
      "all_data shape: (17091, 3075)\n",
      "all_data shape: (20739, 3075)\n",
      "all_data shape: (23337, 3075)\n",
      "all_data shape: (25986, 3075)\n",
      "all_data shape: (28959, 3075)\n",
      "all_data shape: (31236, 3075)\n",
      "all_data shape: (34540, 3075)\n",
      "all_data shape: (37387, 3075)\n",
      "all_data shape: (40573, 3075)\n",
      "all_data shape: (43906, 3075)\n",
      "all_data shape: (47890, 3075)\n",
      "all_data shape: (51189, 3075)\n",
      "all_data shape: (54381, 3075)\n",
      "all_data shape: (56693, 3075)\n",
      "all_data shape: (59403, 3075)\n",
      "all_data shape: (61645, 3075)\n",
      "all_data shape: (65340, 3075)\n",
      "all_data shape: (68271, 3075)\n",
      "all_data shape: (71461, 3075)\n",
      "all_data shape: (74270, 3075)\n",
      "all_data shape: (76873, 3075)\n",
      "all_data shape: (79703, 3075)\n",
      "all_data shape: (82012, 3075)\n",
      "Saved file 53 == 4341760 examples\n",
      "all_data shape: (3297, 3075)\n",
      "all_data shape: (6133, 3075)\n",
      "all_data shape: (8671, 3075)\n",
      "all_data shape: (12054, 3075)\n",
      "all_data shape: (14813, 3075)\n",
      "all_data shape: (17657, 3075)\n",
      "all_data shape: (20526, 3075)\n",
      "all_data shape: (22714, 3075)\n",
      "all_data shape: (24658, 3075)\n",
      "all_data shape: (27316, 3075)\n",
      "all_data shape: (30425, 3075)\n",
      "all_data shape: (32378, 3075)\n",
      "all_data shape: (35537, 3075)\n",
      "all_data shape: (38478, 3075)\n",
      "all_data shape: (40947, 3075)\n",
      "all_data shape: (43013, 3075)\n",
      "all_data shape: (45305, 3075)\n",
      "all_data shape: (47562, 3075)\n",
      "all_data shape: (50646, 3075)\n",
      "all_data shape: (53227, 3075)\n",
      "all_data shape: (55650, 3075)\n",
      "all_data shape: (58557, 3075)\n",
      "all_data shape: (62179, 3075)\n",
      "all_data shape: (64797, 3075)\n",
      "all_data shape: (67935, 3075)\n",
      "all_data shape: (70191, 3075)\n",
      "all_data shape: (73413, 3075)\n",
      "all_data shape: (76208, 3075)\n",
      "all_data shape: (79054, 3075)\n",
      "all_data shape: (82106, 3075)\n",
      "Saved file 54 == 4423680 examples\n",
      "all_data shape: (2684, 3075)\n",
      "all_data shape: (4667, 3075)\n",
      "all_data shape: (7090, 3075)\n",
      "all_data shape: (10465, 3075)\n",
      "all_data shape: (13398, 3075)\n",
      "all_data shape: (16295, 3075)\n",
      "all_data shape: (19205, 3075)\n",
      "all_data shape: (21750, 3075)\n",
      "all_data shape: (24699, 3075)\n",
      "all_data shape: (27452, 3075)\n",
      "all_data shape: (30909, 3075)\n",
      "all_data shape: (34031, 3075)\n",
      "all_data shape: (37484, 3075)\n",
      "all_data shape: (40280, 3075)\n",
      "all_data shape: (42797, 3075)\n",
      "all_data shape: (45545, 3075)\n",
      "all_data shape: (48120, 3075)\n",
      "all_data shape: (50200, 3075)\n",
      "all_data shape: (52836, 3075)\n",
      "all_data shape: (56053, 3075)\n",
      "all_data shape: (59835, 3075)\n",
      "all_data shape: (63235, 3075)\n",
      "all_data shape: (65339, 3075)\n",
      "all_data shape: (68443, 3075)\n",
      "all_data shape: (71092, 3075)\n",
      "all_data shape: (73727, 3075)\n",
      "all_data shape: (76852, 3075)\n",
      "all_data shape: (79726, 3075)\n",
      "all_data shape: (81770, 3075)\n",
      "all_data shape: (85140, 3075)\n",
      "Saved file 55 == 4505600 examples\n",
      "all_data shape: (6037, 3075)\n",
      "all_data shape: (8486, 3075)\n",
      "all_data shape: (11515, 3075)\n",
      "all_data shape: (14207, 3075)\n",
      "all_data shape: (16730, 3075)\n",
      "all_data shape: (20299, 3075)\n",
      "all_data shape: (23425, 3075)\n",
      "all_data shape: (26183, 3075)\n",
      "all_data shape: (28934, 3075)\n",
      "all_data shape: (32121, 3075)\n",
      "all_data shape: (34094, 3075)\n",
      "all_data shape: (36890, 3075)\n",
      "all_data shape: (40031, 3075)\n",
      "all_data shape: (43141, 3075)\n",
      "all_data shape: (46270, 3075)\n",
      "all_data shape: (48829, 3075)\n",
      "all_data shape: (51592, 3075)\n",
      "all_data shape: (54748, 3075)\n",
      "all_data shape: (58142, 3075)\n",
      "all_data shape: (61411, 3075)\n",
      "all_data shape: (63987, 3075)\n",
      "all_data shape: (67329, 3075)\n",
      "all_data shape: (70039, 3075)\n",
      "all_data shape: (72398, 3075)\n",
      "all_data shape: (74725, 3075)\n",
      "all_data shape: (77916, 3075)\n",
      "all_data shape: (81595, 3075)\n",
      "all_data shape: (84546, 3075)\n",
      "Saved file 56 == 4587520 examples\n",
      "all_data shape: (5529, 3075)\n",
      "all_data shape: (8591, 3075)\n",
      "all_data shape: (10846, 3075)\n",
      "all_data shape: (13684, 3075)\n",
      "all_data shape: (15921, 3075)\n",
      "all_data shape: (18761, 3075)\n",
      "all_data shape: (21539, 3075)\n",
      "all_data shape: (24138, 3075)\n",
      "all_data shape: (27389, 3075)\n",
      "all_data shape: (30512, 3075)\n",
      "all_data shape: (33233, 3075)\n",
      "all_data shape: (35882, 3075)\n",
      "all_data shape: (38522, 3075)\n",
      "all_data shape: (41465, 3075)\n",
      "all_data shape: (44211, 3075)\n",
      "all_data shape: (46943, 3075)\n",
      "all_data shape: (49628, 3075)\n",
      "all_data shape: (52091, 3075)\n",
      "all_data shape: (54910, 3075)\n",
      "all_data shape: (58468, 3075)\n",
      "all_data shape: (61535, 3075)\n",
      "all_data shape: (64040, 3075)\n",
      "all_data shape: (66452, 3075)\n",
      "all_data shape: (69767, 3075)\n",
      "all_data shape: (72659, 3075)\n",
      "all_data shape: (76010, 3075)\n",
      "all_data shape: (78958, 3075)\n",
      "all_data shape: (82205, 3075)\n",
      "Saved file 57 == 4669440 examples\n",
      "all_data shape: (2714, 3075)\n",
      "all_data shape: (5731, 3075)\n",
      "all_data shape: (8025, 3075)\n",
      "all_data shape: (11245, 3075)\n",
      "all_data shape: (13317, 3075)\n",
      "all_data shape: (16289, 3075)\n",
      "all_data shape: (19122, 3075)\n",
      "all_data shape: (21897, 3075)\n",
      "all_data shape: (25253, 3075)\n",
      "all_data shape: (28609, 3075)\n",
      "all_data shape: (31424, 3075)\n",
      "all_data shape: (33958, 3075)\n",
      "all_data shape: (36541, 3075)\n",
      "all_data shape: (40637, 3075)\n",
      "all_data shape: (43539, 3075)\n",
      "all_data shape: (46044, 3075)\n",
      "all_data shape: (48698, 3075)\n",
      "all_data shape: (52421, 3075)\n",
      "all_data shape: (55927, 3075)\n",
      "all_data shape: (58744, 3075)\n",
      "all_data shape: (61950, 3075)\n",
      "all_data shape: (64907, 3075)\n",
      "all_data shape: (67342, 3075)\n",
      "all_data shape: (70534, 3075)\n",
      "all_data shape: (73722, 3075)\n",
      "all_data shape: (76650, 3075)\n",
      "all_data shape: (79202, 3075)\n",
      "all_data shape: (82096, 3075)\n",
      "Saved file 58 == 4751360 examples\n",
      "all_data shape: (2704, 3075)\n",
      "all_data shape: (5794, 3075)\n",
      "all_data shape: (8417, 3075)\n",
      "all_data shape: (10902, 3075)\n",
      "all_data shape: (13882, 3075)\n",
      "all_data shape: (17312, 3075)\n",
      "all_data shape: (21012, 3075)\n",
      "all_data shape: (23564, 3075)\n",
      "all_data shape: (25545, 3075)\n",
      "all_data shape: (27683, 3075)\n",
      "all_data shape: (30566, 3075)\n",
      "all_data shape: (33167, 3075)\n",
      "all_data shape: (36631, 3075)\n",
      "all_data shape: (39267, 3075)\n",
      "all_data shape: (42330, 3075)\n",
      "all_data shape: (45221, 3075)\n",
      "all_data shape: (48385, 3075)\n",
      "all_data shape: (51444, 3075)\n",
      "all_data shape: (54248, 3075)\n",
      "all_data shape: (56758, 3075)\n",
      "all_data shape: (60333, 3075)\n",
      "all_data shape: (63643, 3075)\n",
      "all_data shape: (65988, 3075)\n",
      "all_data shape: (68862, 3075)\n",
      "all_data shape: (71239, 3075)\n",
      "all_data shape: (74212, 3075)\n",
      "all_data shape: (78308, 3075)\n",
      "all_data shape: (81028, 3075)\n",
      "all_data shape: (83235, 3075)\n",
      "Saved file 59 == 4833280 examples\n",
      "all_data shape: (4051, 3075)\n",
      "all_data shape: (6654, 3075)\n",
      "all_data shape: (9130, 3075)\n",
      "all_data shape: (12607, 3075)\n",
      "all_data shape: (15750, 3075)\n",
      "all_data shape: (19402, 3075)\n",
      "all_data shape: (22291, 3075)\n",
      "all_data shape: (25219, 3075)\n",
      "all_data shape: (28425, 3075)\n",
      "all_data shape: (30674, 3075)\n",
      "all_data shape: (34243, 3075)\n",
      "all_data shape: (35784, 3075)\n",
      "all_data shape: (39362, 3075)\n",
      "all_data shape: (42008, 3075)\n",
      "all_data shape: (45077, 3075)\n",
      "all_data shape: (47317, 3075)\n",
      "all_data shape: (50731, 3075)\n",
      "all_data shape: (53162, 3075)\n",
      "all_data shape: (56093, 3075)\n",
      "all_data shape: (59915, 3075)\n",
      "all_data shape: (63026, 3075)\n",
      "all_data shape: (65709, 3075)\n",
      "all_data shape: (68995, 3075)\n",
      "all_data shape: (71672, 3075)\n",
      "all_data shape: (74262, 3075)\n",
      "all_data shape: (76930, 3075)\n",
      "all_data shape: (80035, 3075)\n",
      "all_data shape: (82887, 3075)\n",
      "Saved file 60 == 4915200 examples\n",
      "all_data shape: (3297, 3075)\n",
      "all_data shape: (6378, 3075)\n",
      "all_data shape: (9576, 3075)\n",
      "all_data shape: (12420, 3075)\n",
      "all_data shape: (15407, 3075)\n",
      "all_data shape: (19179, 3075)\n",
      "all_data shape: (21521, 3075)\n",
      "all_data shape: (23649, 3075)\n",
      "all_data shape: (26677, 3075)\n",
      "all_data shape: (28658, 3075)\n",
      "all_data shape: (30531, 3075)\n",
      "all_data shape: (33067, 3075)\n",
      "all_data shape: (35545, 3075)\n",
      "all_data shape: (38696, 3075)\n",
      "all_data shape: (41903, 3075)\n",
      "all_data shape: (44069, 3075)\n",
      "all_data shape: (46489, 3075)\n",
      "all_data shape: (48936, 3075)\n",
      "all_data shape: (51650, 3075)\n",
      "all_data shape: (55150, 3075)\n",
      "all_data shape: (57634, 3075)\n",
      "all_data shape: (60503, 3075)\n",
      "all_data shape: (63262, 3075)\n",
      "all_data shape: (66477, 3075)\n",
      "all_data shape: (68965, 3075)\n",
      "all_data shape: (72247, 3075)\n",
      "all_data shape: (74683, 3075)\n",
      "all_data shape: (78308, 3075)\n",
      "all_data shape: (80768, 3075)\n",
      "all_data shape: (82795, 3075)\n",
      "Saved file 61 == 4997120 examples\n",
      "all_data shape: (3721, 3075)\n",
      "all_data shape: (6483, 3075)\n",
      "all_data shape: (9231, 3075)\n",
      "all_data shape: (12189, 3075)\n",
      "all_data shape: (15401, 3075)\n",
      "all_data shape: (17817, 3075)\n",
      "all_data shape: (21420, 3075)\n",
      "all_data shape: (24298, 3075)\n",
      "all_data shape: (25994, 3075)\n",
      "all_data shape: (29268, 3075)\n",
      "all_data shape: (32318, 3075)\n",
      "all_data shape: (35334, 3075)\n",
      "all_data shape: (38100, 3075)\n",
      "all_data shape: (41379, 3075)\n",
      "all_data shape: (43723, 3075)\n",
      "all_data shape: (45453, 3075)\n",
      "all_data shape: (48515, 3075)\n",
      "all_data shape: (51691, 3075)\n",
      "all_data shape: (55250, 3075)\n",
      "all_data shape: (57903, 3075)\n",
      "all_data shape: (60291, 3075)\n",
      "all_data shape: (63127, 3075)\n",
      "all_data shape: (66062, 3075)\n",
      "all_data shape: (68581, 3075)\n",
      "all_data shape: (71838, 3075)\n",
      "all_data shape: (74838, 3075)\n",
      "all_data shape: (77108, 3075)\n",
      "all_data shape: (79500, 3075)\n",
      "all_data shape: (82567, 3075)\n",
      "Saved file 62 == 5079040 examples\n",
      "all_data shape: (3632, 3075)\n",
      "all_data shape: (7414, 3075)\n",
      "all_data shape: (10559, 3075)\n",
      "all_data shape: (13345, 3075)\n",
      "all_data shape: (16414, 3075)\n",
      "all_data shape: (19204, 3075)\n",
      "all_data shape: (21759, 3075)\n",
      "all_data shape: (24694, 3075)\n",
      "all_data shape: (27440, 3075)\n",
      "all_data shape: (30113, 3075)\n",
      "all_data shape: (33465, 3075)\n",
      "all_data shape: (35941, 3075)\n",
      "all_data shape: (38682, 3075)\n",
      "all_data shape: (41996, 3075)\n",
      "all_data shape: (44475, 3075)\n",
      "all_data shape: (47665, 3075)\n",
      "all_data shape: (50931, 3075)\n",
      "all_data shape: (53535, 3075)\n",
      "all_data shape: (56118, 3075)\n",
      "all_data shape: (58706, 3075)\n",
      "all_data shape: (62181, 3075)\n",
      "all_data shape: (65267, 3075)\n",
      "all_data shape: (68448, 3075)\n",
      "all_data shape: (71292, 3075)\n",
      "all_data shape: (74420, 3075)\n",
      "all_data shape: (76973, 3075)\n",
      "all_data shape: (80121, 3075)\n",
      "all_data shape: (83760, 3075)\n",
      "Saved file 63 == 5160960 examples\n",
      "all_data shape: (4243, 3075)\n",
      "all_data shape: (5964, 3075)\n",
      "all_data shape: (9130, 3075)\n",
      "all_data shape: (11232, 3075)\n",
      "all_data shape: (14221, 3075)\n",
      "all_data shape: (15956, 3075)\n",
      "all_data shape: (18340, 3075)\n",
      "all_data shape: (21350, 3075)\n",
      "all_data shape: (23872, 3075)\n",
      "all_data shape: (26576, 3075)\n",
      "all_data shape: (29360, 3075)\n",
      "all_data shape: (31627, 3075)\n",
      "all_data shape: (35092, 3075)\n",
      "all_data shape: (37866, 3075)\n",
      "all_data shape: (41214, 3075)\n",
      "all_data shape: (44225, 3075)\n",
      "all_data shape: (47303, 3075)\n",
      "all_data shape: (50588, 3075)\n",
      "all_data shape: (53192, 3075)\n",
      "all_data shape: (55539, 3075)\n",
      "all_data shape: (58651, 3075)\n",
      "all_data shape: (61791, 3075)\n",
      "all_data shape: (64263, 3075)\n",
      "all_data shape: (66698, 3075)\n",
      "all_data shape: (69166, 3075)\n",
      "all_data shape: (72319, 3075)\n",
      "all_data shape: (74970, 3075)\n",
      "all_data shape: (77824, 3075)\n",
      "all_data shape: (81146, 3075)\n",
      "all_data shape: (84344, 3075)\n",
      "Saved file 64 == 5242880 examples\n",
      "all_data shape: (4685, 3075)\n",
      "all_data shape: (7276, 3075)\n",
      "all_data shape: (9345, 3075)\n",
      "all_data shape: (12117, 3075)\n",
      "all_data shape: (14622, 3075)\n",
      "all_data shape: (18089, 3075)\n",
      "all_data shape: (21730, 3075)\n",
      "all_data shape: (24478, 3075)\n",
      "all_data shape: (26391, 3075)\n",
      "all_data shape: (29610, 3075)\n",
      "all_data shape: (31512, 3075)\n",
      "all_data shape: (34000, 3075)\n",
      "all_data shape: (36956, 3075)\n",
      "all_data shape: (38868, 3075)\n",
      "all_data shape: (42320, 3075)\n",
      "all_data shape: (45607, 3075)\n",
      "all_data shape: (48837, 3075)\n",
      "all_data shape: (51858, 3075)\n",
      "all_data shape: (53856, 3075)\n",
      "all_data shape: (57370, 3075)\n",
      "all_data shape: (59978, 3075)\n",
      "all_data shape: (62682, 3075)\n",
      "all_data shape: (66200, 3075)\n",
      "all_data shape: (68595, 3075)\n",
      "all_data shape: (71461, 3075)\n",
      "all_data shape: (74652, 3075)\n",
      "all_data shape: (77957, 3075)\n",
      "all_data shape: (80845, 3075)\n",
      "all_data shape: (83456, 3075)\n",
      "Saved file 65 == 5324800 examples\n",
      "all_data shape: (3364, 3075)\n",
      "all_data shape: (5812, 3075)\n",
      "all_data shape: (8397, 3075)\n",
      "all_data shape: (11227, 3075)\n",
      "all_data shape: (13884, 3075)\n",
      "all_data shape: (16911, 3075)\n",
      "all_data shape: (19506, 3075)\n",
      "all_data shape: (21470, 3075)\n",
      "all_data shape: (24196, 3075)\n",
      "all_data shape: (27205, 3075)\n",
      "all_data shape: (30940, 3075)\n",
      "all_data shape: (32930, 3075)\n",
      "all_data shape: (35469, 3075)\n",
      "all_data shape: (38520, 3075)\n",
      "all_data shape: (41928, 3075)\n",
      "all_data shape: (45265, 3075)\n",
      "all_data shape: (48766, 3075)\n",
      "all_data shape: (52386, 3075)\n",
      "all_data shape: (55015, 3075)\n",
      "all_data shape: (57684, 3075)\n",
      "all_data shape: (60481, 3075)\n",
      "all_data shape: (63690, 3075)\n",
      "all_data shape: (67447, 3075)\n",
      "all_data shape: (69636, 3075)\n",
      "all_data shape: (72656, 3075)\n",
      "all_data shape: (75567, 3075)\n",
      "all_data shape: (78999, 3075)\n",
      "all_data shape: (81234, 3075)\n",
      "all_data shape: (84486, 3075)\n",
      "Saved file 66 == 5406720 examples\n",
      "all_data shape: (5376, 3075)\n",
      "all_data shape: (9171, 3075)\n",
      "all_data shape: (12325, 3075)\n",
      "all_data shape: (14770, 3075)\n",
      "all_data shape: (17429, 3075)\n",
      "all_data shape: (20683, 3075)\n",
      "all_data shape: (22938, 3075)\n",
      "all_data shape: (25931, 3075)\n",
      "all_data shape: (28530, 3075)\n",
      "all_data shape: (31953, 3075)\n",
      "all_data shape: (35002, 3075)\n",
      "all_data shape: (38187, 3075)\n",
      "all_data shape: (41501, 3075)\n",
      "all_data shape: (44892, 3075)\n",
      "all_data shape: (47030, 3075)\n",
      "all_data shape: (49737, 3075)\n",
      "all_data shape: (51838, 3075)\n",
      "all_data shape: (55124, 3075)\n",
      "all_data shape: (58013, 3075)\n",
      "all_data shape: (61291, 3075)\n",
      "all_data shape: (63741, 3075)\n",
      "all_data shape: (65698, 3075)\n",
      "all_data shape: (67848, 3075)\n",
      "all_data shape: (70680, 3075)\n",
      "all_data shape: (73555, 3075)\n",
      "all_data shape: (75818, 3075)\n",
      "all_data shape: (78174, 3075)\n",
      "all_data shape: (79815, 3075)\n",
      "all_data shape: (82613, 3075)\n",
      "Saved file 67 == 5488640 examples\n",
      "all_data shape: (4001, 3075)\n",
      "all_data shape: (6620, 3075)\n",
      "all_data shape: (10443, 3075)\n",
      "all_data shape: (14315, 3075)\n",
      "all_data shape: (16853, 3075)\n",
      "all_data shape: (19403, 3075)\n",
      "all_data shape: (21643, 3075)\n",
      "all_data shape: (24066, 3075)\n",
      "all_data shape: (26451, 3075)\n",
      "all_data shape: (29366, 3075)\n",
      "all_data shape: (31847, 3075)\n",
      "all_data shape: (34271, 3075)\n",
      "all_data shape: (37278, 3075)\n",
      "all_data shape: (38886, 3075)\n",
      "all_data shape: (41845, 3075)\n",
      "all_data shape: (44561, 3075)\n",
      "all_data shape: (47314, 3075)\n",
      "all_data shape: (49888, 3075)\n",
      "all_data shape: (53350, 3075)\n",
      "all_data shape: (55818, 3075)\n",
      "all_data shape: (59086, 3075)\n",
      "all_data shape: (62018, 3075)\n",
      "all_data shape: (64368, 3075)\n",
      "all_data shape: (67355, 3075)\n",
      "all_data shape: (70965, 3075)\n",
      "all_data shape: (74619, 3075)\n",
      "all_data shape: (77298, 3075)\n",
      "all_data shape: (79718, 3075)\n",
      "all_data shape: (83210, 3075)\n",
      "Saved file 68 == 5570560 examples\n",
      "all_data shape: (4609, 3075)\n",
      "all_data shape: (7880, 3075)\n",
      "all_data shape: (10518, 3075)\n",
      "all_data shape: (13449, 3075)\n",
      "all_data shape: (16624, 3075)\n",
      "all_data shape: (19362, 3075)\n",
      "all_data shape: (21921, 3075)\n",
      "all_data shape: (25189, 3075)\n",
      "all_data shape: (28143, 3075)\n",
      "all_data shape: (31601, 3075)\n",
      "all_data shape: (34613, 3075)\n",
      "all_data shape: (37465, 3075)\n",
      "all_data shape: (41068, 3075)\n",
      "all_data shape: (43969, 3075)\n",
      "all_data shape: (47301, 3075)\n",
      "all_data shape: (50207, 3075)\n",
      "all_data shape: (52981, 3075)\n",
      "all_data shape: (56209, 3075)\n",
      "all_data shape: (58950, 3075)\n",
      "all_data shape: (61851, 3075)\n",
      "all_data shape: (64671, 3075)\n",
      "all_data shape: (67746, 3075)\n",
      "all_data shape: (69789, 3075)\n",
      "all_data shape: (72293, 3075)\n",
      "all_data shape: (75764, 3075)\n",
      "all_data shape: (78639, 3075)\n",
      "all_data shape: (81108, 3075)\n",
      "all_data shape: (83651, 3075)\n",
      "Saved file 69 == 5652480 examples\n",
      "all_data shape: (3586, 3075)\n",
      "all_data shape: (6575, 3075)\n",
      "all_data shape: (8308, 3075)\n",
      "all_data shape: (11045, 3075)\n",
      "all_data shape: (13896, 3075)\n",
      "all_data shape: (16865, 3075)\n",
      "all_data shape: (20182, 3075)\n",
      "all_data shape: (23265, 3075)\n",
      "all_data shape: (26190, 3075)\n",
      "all_data shape: (29255, 3075)\n",
      "all_data shape: (32465, 3075)\n",
      "all_data shape: (35753, 3075)\n",
      "all_data shape: (38468, 3075)\n",
      "all_data shape: (41280, 3075)\n",
      "all_data shape: (43958, 3075)\n",
      "all_data shape: (46228, 3075)\n",
      "all_data shape: (48995, 3075)\n",
      "all_data shape: (52133, 3075)\n",
      "all_data shape: (55150, 3075)\n",
      "all_data shape: (57618, 3075)\n",
      "all_data shape: (60574, 3075)\n",
      "all_data shape: (63584, 3075)\n",
      "all_data shape: (66837, 3075)\n",
      "all_data shape: (69929, 3075)\n",
      "all_data shape: (73089, 3075)\n",
      "all_data shape: (76054, 3075)\n",
      "all_data shape: (79684, 3075)\n",
      "all_data shape: (81580, 3075)\n",
      "all_data shape: (84127, 3075)\n",
      "Saved file 70 == 5734400 examples\n",
      "all_data shape: (5207, 3075)\n",
      "all_data shape: (7640, 3075)\n",
      "all_data shape: (9741, 3075)\n",
      "all_data shape: (12486, 3075)\n",
      "all_data shape: (15588, 3075)\n",
      "all_data shape: (18755, 3075)\n",
      "all_data shape: (20895, 3075)\n",
      "all_data shape: (24350, 3075)\n",
      "all_data shape: (26660, 3075)\n",
      "all_data shape: (29600, 3075)\n",
      "all_data shape: (32435, 3075)\n",
      "all_data shape: (34737, 3075)\n",
      "all_data shape: (36953, 3075)\n",
      "all_data shape: (39641, 3075)\n",
      "all_data shape: (42134, 3075)\n",
      "all_data shape: (45128, 3075)\n",
      "all_data shape: (47927, 3075)\n",
      "all_data shape: (50656, 3075)\n",
      "all_data shape: (53744, 3075)\n",
      "all_data shape: (55949, 3075)\n",
      "all_data shape: (59582, 3075)\n",
      "all_data shape: (62201, 3075)\n",
      "all_data shape: (64735, 3075)\n",
      "all_data shape: (68487, 3075)\n",
      "all_data shape: (72098, 3075)\n",
      "all_data shape: (74846, 3075)\n",
      "all_data shape: (76723, 3075)\n",
      "all_data shape: (79826, 3075)\n",
      "all_data shape: (82818, 3075)\n",
      "Saved file 71 == 5816320 examples\n",
      "all_data shape: (4399, 3075)\n",
      "all_data shape: (7749, 3075)\n",
      "all_data shape: (10749, 3075)\n",
      "all_data shape: (14057, 3075)\n",
      "all_data shape: (16731, 3075)\n",
      "all_data shape: (20129, 3075)\n",
      "all_data shape: (22682, 3075)\n",
      "all_data shape: (24804, 3075)\n",
      "all_data shape: (27854, 3075)\n",
      "all_data shape: (30916, 3075)\n",
      "all_data shape: (33368, 3075)\n",
      "all_data shape: (36605, 3075)\n",
      "all_data shape: (38594, 3075)\n",
      "all_data shape: (42065, 3075)\n",
      "all_data shape: (44781, 3075)\n",
      "all_data shape: (48142, 3075)\n",
      "all_data shape: (50322, 3075)\n",
      "all_data shape: (53277, 3075)\n",
      "all_data shape: (55723, 3075)\n",
      "all_data shape: (58473, 3075)\n",
      "all_data shape: (61723, 3075)\n",
      "all_data shape: (64028, 3075)\n",
      "all_data shape: (67240, 3075)\n",
      "all_data shape: (70829, 3075)\n",
      "all_data shape: (73184, 3075)\n",
      "all_data shape: (76421, 3075)\n",
      "all_data shape: (78966, 3075)\n",
      "all_data shape: (81862, 3075)\n",
      "all_data shape: (83992, 3075)\n",
      "Saved file 72 == 5898240 examples\n",
      "all_data shape: (5352, 3075)\n",
      "all_data shape: (8482, 3075)\n",
      "all_data shape: (10911, 3075)\n",
      "all_data shape: (13613, 3075)\n",
      "all_data shape: (16067, 3075)\n",
      "all_data shape: (18945, 3075)\n",
      "all_data shape: (21931, 3075)\n",
      "all_data shape: (24526, 3075)\n",
      "all_data shape: (27563, 3075)\n",
      "all_data shape: (29939, 3075)\n",
      "all_data shape: (33496, 3075)\n",
      "all_data shape: (35784, 3075)\n",
      "all_data shape: (38989, 3075)\n",
      "all_data shape: (41407, 3075)\n",
      "all_data shape: (44304, 3075)\n",
      "all_data shape: (47795, 3075)\n",
      "all_data shape: (50453, 3075)\n",
      "all_data shape: (52899, 3075)\n",
      "all_data shape: (55422, 3075)\n",
      "all_data shape: (57675, 3075)\n",
      "all_data shape: (60449, 3075)\n",
      "all_data shape: (62445, 3075)\n",
      "all_data shape: (65102, 3075)\n",
      "all_data shape: (67965, 3075)\n",
      "all_data shape: (71170, 3075)\n",
      "all_data shape: (74022, 3075)\n",
      "all_data shape: (77210, 3075)\n",
      "all_data shape: (80002, 3075)\n",
      "all_data shape: (82723, 3075)\n",
      "Saved file 73 == 5980160 examples\n",
      "all_data shape: (3031, 3075)\n",
      "all_data shape: (5791, 3075)\n",
      "all_data shape: (8872, 3075)\n",
      "all_data shape: (11930, 3075)\n",
      "all_data shape: (15054, 3075)\n",
      "all_data shape: (18186, 3075)\n",
      "all_data shape: (20647, 3075)\n",
      "all_data shape: (24621, 3075)\n",
      "all_data shape: (26799, 3075)\n",
      "all_data shape: (29871, 3075)\n",
      "all_data shape: (32833, 3075)\n",
      "all_data shape: (35431, 3075)\n",
      "all_data shape: (37538, 3075)\n",
      "all_data shape: (40586, 3075)\n",
      "all_data shape: (44062, 3075)\n",
      "all_data shape: (46554, 3075)\n",
      "all_data shape: (48830, 3075)\n",
      "all_data shape: (51628, 3075)\n",
      "all_data shape: (55483, 3075)\n",
      "all_data shape: (59534, 3075)\n",
      "all_data shape: (62889, 3075)\n",
      "all_data shape: (65195, 3075)\n",
      "all_data shape: (67409, 3075)\n",
      "all_data shape: (70356, 3075)\n",
      "all_data shape: (72826, 3075)\n",
      "all_data shape: (75324, 3075)\n",
      "all_data shape: (78151, 3075)\n",
      "all_data shape: (81578, 3075)\n",
      "all_data shape: (84084, 3075)\n",
      "Saved file 74 == 6062080 examples\n",
      "all_data shape: (4218, 3075)\n",
      "all_data shape: (6475, 3075)\n",
      "all_data shape: (9489, 3075)\n",
      "all_data shape: (12051, 3075)\n",
      "all_data shape: (13920, 3075)\n",
      "all_data shape: (17636, 3075)\n",
      "all_data shape: (20335, 3075)\n",
      "all_data shape: (23222, 3075)\n",
      "all_data shape: (25772, 3075)\n",
      "all_data shape: (28814, 3075)\n",
      "all_data shape: (31783, 3075)\n",
      "all_data shape: (34716, 3075)\n",
      "all_data shape: (37764, 3075)\n",
      "all_data shape: (41039, 3075)\n",
      "all_data shape: (42977, 3075)\n",
      "all_data shape: (45488, 3075)\n",
      "all_data shape: (48523, 3075)\n",
      "all_data shape: (50672, 3075)\n",
      "all_data shape: (53411, 3075)\n",
      "all_data shape: (56242, 3075)\n",
      "all_data shape: (58515, 3075)\n",
      "all_data shape: (60862, 3075)\n",
      "all_data shape: (63882, 3075)\n",
      "all_data shape: (66161, 3075)\n",
      "all_data shape: (69391, 3075)\n",
      "all_data shape: (72568, 3075)\n",
      "all_data shape: (76131, 3075)\n",
      "all_data shape: (78380, 3075)\n",
      "all_data shape: (80884, 3075)\n",
      "all_data shape: (84257, 3075)\n",
      "Saved file 75 == 6144000 examples\n",
      "all_data shape: (4186, 3075)\n",
      "all_data shape: (8032, 3075)\n",
      "all_data shape: (11265, 3075)\n",
      "all_data shape: (15154, 3075)\n",
      "all_data shape: (17884, 3075)\n",
      "all_data shape: (20811, 3075)\n",
      "all_data shape: (24166, 3075)\n",
      "all_data shape: (27017, 3075)\n",
      "all_data shape: (28824, 3075)\n",
      "all_data shape: (32609, 3075)\n",
      "all_data shape: (35983, 3075)\n",
      "all_data shape: (38775, 3075)\n",
      "all_data shape: (41449, 3075)\n",
      "all_data shape: (44383, 3075)\n",
      "all_data shape: (47098, 3075)\n",
      "all_data shape: (50070, 3075)\n",
      "all_data shape: (53823, 3075)\n",
      "all_data shape: (57014, 3075)\n",
      "all_data shape: (60147, 3075)\n",
      "all_data shape: (62135, 3075)\n",
      "all_data shape: (64438, 3075)\n",
      "all_data shape: (67175, 3075)\n",
      "all_data shape: (70407, 3075)\n",
      "all_data shape: (73730, 3075)\n",
      "all_data shape: (76146, 3075)\n",
      "all_data shape: (78912, 3075)\n",
      "all_data shape: (81236, 3075)\n",
      "all_data shape: (83561, 3075)\n",
      "Saved file 76 == 6225920 examples\n",
      "all_data shape: (4158, 3075)\n",
      "all_data shape: (7875, 3075)\n",
      "all_data shape: (10441, 3075)\n",
      "all_data shape: (13872, 3075)\n",
      "all_data shape: (16023, 3075)\n",
      "all_data shape: (19737, 3075)\n",
      "all_data shape: (22255, 3075)\n",
      "all_data shape: (26003, 3075)\n",
      "all_data shape: (29366, 3075)\n",
      "all_data shape: (32099, 3075)\n",
      "all_data shape: (33927, 3075)\n",
      "all_data shape: (36664, 3075)\n",
      "all_data shape: (39016, 3075)\n",
      "all_data shape: (41901, 3075)\n",
      "all_data shape: (45915, 3075)\n",
      "all_data shape: (48060, 3075)\n",
      "all_data shape: (50950, 3075)\n",
      "all_data shape: (53201, 3075)\n",
      "all_data shape: (55775, 3075)\n",
      "all_data shape: (58990, 3075)\n",
      "all_data shape: (62041, 3075)\n",
      "all_data shape: (64742, 3075)\n",
      "all_data shape: (67657, 3075)\n",
      "all_data shape: (70403, 3075)\n",
      "all_data shape: (72974, 3075)\n",
      "all_data shape: (76447, 3075)\n",
      "all_data shape: (79572, 3075)\n",
      "all_data shape: (82253, 3075)\n",
      "Saved file 77 == 6307840 examples\n",
      "all_data shape: (2751, 3075)\n",
      "all_data shape: (5574, 3075)\n",
      "all_data shape: (8021, 3075)\n",
      "all_data shape: (10402, 3075)\n",
      "all_data shape: (13435, 3075)\n",
      "all_data shape: (16341, 3075)\n",
      "all_data shape: (18685, 3075)\n",
      "all_data shape: (21698, 3075)\n",
      "all_data shape: (25290, 3075)\n",
      "all_data shape: (27307, 3075)\n",
      "all_data shape: (29214, 3075)\n",
      "all_data shape: (31945, 3075)\n",
      "all_data shape: (34220, 3075)\n",
      "all_data shape: (37074, 3075)\n",
      "all_data shape: (40069, 3075)\n",
      "all_data shape: (42566, 3075)\n",
      "all_data shape: (44859, 3075)\n",
      "all_data shape: (47104, 3075)\n",
      "all_data shape: (49482, 3075)\n",
      "all_data shape: (52792, 3075)\n",
      "all_data shape: (56088, 3075)\n",
      "all_data shape: (59046, 3075)\n",
      "all_data shape: (61053, 3075)\n",
      "all_data shape: (63528, 3075)\n",
      "all_data shape: (66463, 3075)\n",
      "all_data shape: (70084, 3075)\n",
      "all_data shape: (73135, 3075)\n",
      "all_data shape: (75659, 3075)\n",
      "all_data shape: (78111, 3075)\n",
      "all_data shape: (79779, 3075)\n",
      "all_data shape: (82853, 3075)\n",
      "Saved file 78 == 6389760 examples\n",
      "all_data shape: (3783, 3075)\n",
      "all_data shape: (6145, 3075)\n",
      "all_data shape: (8616, 3075)\n",
      "all_data shape: (11902, 3075)\n",
      "all_data shape: (14614, 3075)\n",
      "all_data shape: (17526, 3075)\n",
      "all_data shape: (20735, 3075)\n",
      "all_data shape: (23819, 3075)\n",
      "all_data shape: (27466, 3075)\n",
      "all_data shape: (30215, 3075)\n",
      "all_data shape: (33826, 3075)\n",
      "all_data shape: (36684, 3075)\n",
      "all_data shape: (38747, 3075)\n",
      "all_data shape: (42235, 3075)\n",
      "all_data shape: (45632, 3075)\n",
      "all_data shape: (47883, 3075)\n",
      "all_data shape: (50605, 3075)\n",
      "all_data shape: (53848, 3075)\n",
      "all_data shape: (57060, 3075)\n",
      "all_data shape: (59502, 3075)\n",
      "all_data shape: (61769, 3075)\n",
      "all_data shape: (63964, 3075)\n",
      "all_data shape: (66469, 3075)\n",
      "all_data shape: (68978, 3075)\n",
      "all_data shape: (71893, 3075)\n",
      "all_data shape: (75674, 3075)\n",
      "all_data shape: (78957, 3075)\n",
      "all_data shape: (81222, 3075)\n",
      "all_data shape: (84225, 3075)\n",
      "Saved file 79 == 6471680 examples\n",
      "all_data shape: (5495, 3075)\n",
      "all_data shape: (8215, 3075)\n",
      "all_data shape: (11506, 3075)\n",
      "all_data shape: (14335, 3075)\n",
      "all_data shape: (17514, 3075)\n",
      "all_data shape: (20731, 3075)\n",
      "all_data shape: (23630, 3075)\n",
      "all_data shape: (26492, 3075)\n",
      "all_data shape: (29508, 3075)\n",
      "all_data shape: (32136, 3075)\n",
      "all_data shape: (33810, 3075)\n",
      "all_data shape: (35855, 3075)\n",
      "all_data shape: (39407, 3075)\n",
      "all_data shape: (42288, 3075)\n",
      "all_data shape: (45002, 3075)\n",
      "all_data shape: (48028, 3075)\n",
      "all_data shape: (50045, 3075)\n",
      "all_data shape: (52606, 3075)\n",
      "all_data shape: (55625, 3075)\n",
      "all_data shape: (57922, 3075)\n",
      "all_data shape: (60161, 3075)\n",
      "all_data shape: (62406, 3075)\n",
      "all_data shape: (64391, 3075)\n",
      "all_data shape: (67168, 3075)\n",
      "all_data shape: (69520, 3075)\n",
      "all_data shape: (71262, 3075)\n",
      "all_data shape: (74269, 3075)\n",
      "all_data shape: (76747, 3075)\n",
      "all_data shape: (79051, 3075)\n",
      "all_data shape: (81770, 3075)\n",
      "all_data shape: (84773, 3075)\n",
      "Saved file 80 == 6553600 examples\n",
      "all_data shape: (5218, 3075)\n",
      "all_data shape: (8187, 3075)\n",
      "all_data shape: (10881, 3075)\n",
      "all_data shape: (12959, 3075)\n",
      "all_data shape: (16713, 3075)\n",
      "all_data shape: (19723, 3075)\n",
      "all_data shape: (22952, 3075)\n",
      "all_data shape: (26323, 3075)\n",
      "all_data shape: (28866, 3075)\n",
      "all_data shape: (31506, 3075)\n",
      "all_data shape: (33722, 3075)\n",
      "all_data shape: (36182, 3075)\n",
      "all_data shape: (39242, 3075)\n",
      "all_data shape: (42684, 3075)\n",
      "all_data shape: (46003, 3075)\n",
      "all_data shape: (48843, 3075)\n",
      "all_data shape: (51065, 3075)\n",
      "all_data shape: (53704, 3075)\n",
      "all_data shape: (56272, 3075)\n",
      "all_data shape: (58690, 3075)\n",
      "all_data shape: (62031, 3075)\n",
      "all_data shape: (64898, 3075)\n",
      "all_data shape: (67704, 3075)\n",
      "all_data shape: (70844, 3075)\n",
      "all_data shape: (74597, 3075)\n",
      "all_data shape: (77599, 3075)\n",
      "all_data shape: (80511, 3075)\n",
      "all_data shape: (83986, 3075)\n",
      "Saved file 81 == 6635520 examples\n",
      "all_data shape: (3762, 3075)\n",
      "all_data shape: (7168, 3075)\n",
      "all_data shape: (10741, 3075)\n",
      "all_data shape: (13145, 3075)\n",
      "all_data shape: (16373, 3075)\n",
      "all_data shape: (19275, 3075)\n",
      "all_data shape: (21826, 3075)\n",
      "all_data shape: (24186, 3075)\n",
      "all_data shape: (27380, 3075)\n",
      "all_data shape: (31039, 3075)\n",
      "all_data shape: (33902, 3075)\n",
      "all_data shape: (36727, 3075)\n",
      "all_data shape: (38958, 3075)\n",
      "all_data shape: (41700, 3075)\n",
      "all_data shape: (44889, 3075)\n",
      "all_data shape: (47584, 3075)\n",
      "all_data shape: (50404, 3075)\n",
      "all_data shape: (53042, 3075)\n",
      "all_data shape: (56400, 3075)\n",
      "all_data shape: (59199, 3075)\n",
      "all_data shape: (61771, 3075)\n",
      "all_data shape: (65141, 3075)\n",
      "all_data shape: (67906, 3075)\n",
      "all_data shape: (70455, 3075)\n",
      "all_data shape: (73371, 3075)\n",
      "all_data shape: (76330, 3075)\n",
      "all_data shape: (79331, 3075)\n",
      "all_data shape: (82207, 3075)\n",
      "Saved file 82 == 6717440 examples\n",
      "all_data shape: (3269, 3075)\n",
      "all_data shape: (5399, 3075)\n",
      "all_data shape: (7907, 3075)\n",
      "all_data shape: (10653, 3075)\n",
      "all_data shape: (13408, 3075)\n",
      "all_data shape: (16531, 3075)\n",
      "all_data shape: (18981, 3075)\n",
      "all_data shape: (22111, 3075)\n",
      "all_data shape: (24147, 3075)\n",
      "all_data shape: (26956, 3075)\n",
      "all_data shape: (30120, 3075)\n",
      "all_data shape: (33217, 3075)\n",
      "all_data shape: (35261, 3075)\n",
      "all_data shape: (38291, 3075)\n",
      "all_data shape: (41507, 3075)\n",
      "all_data shape: (44192, 3075)\n",
      "all_data shape: (46849, 3075)\n",
      "all_data shape: (50198, 3075)\n",
      "all_data shape: (53512, 3075)\n",
      "all_data shape: (55789, 3075)\n",
      "all_data shape: (57787, 3075)\n",
      "all_data shape: (60447, 3075)\n",
      "all_data shape: (63405, 3075)\n",
      "all_data shape: (66071, 3075)\n",
      "all_data shape: (68827, 3075)\n",
      "all_data shape: (72080, 3075)\n",
      "all_data shape: (74594, 3075)\n",
      "all_data shape: (77313, 3075)\n",
      "all_data shape: (80504, 3075)\n",
      "all_data shape: (82406, 3075)\n",
      "Saved file 83 == 6799360 examples\n",
      "all_data shape: (3818, 3075)\n",
      "all_data shape: (7271, 3075)\n",
      "all_data shape: (10679, 3075)\n",
      "all_data shape: (14378, 3075)\n",
      "all_data shape: (16866, 3075)\n",
      "all_data shape: (18984, 3075)\n",
      "all_data shape: (21254, 3075)\n",
      "all_data shape: (24044, 3075)\n",
      "all_data shape: (26715, 3075)\n",
      "all_data shape: (29748, 3075)\n",
      "all_data shape: (32646, 3075)\n",
      "all_data shape: (36283, 3075)\n",
      "all_data shape: (38462, 3075)\n",
      "all_data shape: (41485, 3075)\n",
      "all_data shape: (44721, 3075)\n",
      "all_data shape: (47619, 3075)\n",
      "all_data shape: (50979, 3075)\n",
      "all_data shape: (53097, 3075)\n",
      "all_data shape: (55229, 3075)\n",
      "all_data shape: (58024, 3075)\n",
      "all_data shape: (61057, 3075)\n",
      "all_data shape: (64256, 3075)\n",
      "all_data shape: (67166, 3075)\n",
      "all_data shape: (70607, 3075)\n",
      "all_data shape: (73402, 3075)\n",
      "all_data shape: (75780, 3075)\n",
      "all_data shape: (78884, 3075)\n",
      "all_data shape: (80900, 3075)\n",
      "all_data shape: (83441, 3075)\n",
      "Saved file 84 == 6881280 examples\n",
      "all_data shape: (3736, 3075)\n",
      "all_data shape: (6452, 3075)\n",
      "all_data shape: (9755, 3075)\n",
      "all_data shape: (13388, 3075)\n",
      "all_data shape: (16530, 3075)\n",
      "all_data shape: (19681, 3075)\n",
      "all_data shape: (23034, 3075)\n",
      "all_data shape: (25582, 3075)\n",
      "all_data shape: (27760, 3075)\n",
      "all_data shape: (31243, 3075)\n",
      "all_data shape: (34539, 3075)\n",
      "all_data shape: (37498, 3075)\n",
      "all_data shape: (40076, 3075)\n",
      "all_data shape: (41947, 3075)\n",
      "all_data shape: (45149, 3075)\n",
      "all_data shape: (47792, 3075)\n",
      "all_data shape: (50886, 3075)\n",
      "all_data shape: (53861, 3075)\n",
      "all_data shape: (56979, 3075)\n",
      "all_data shape: (59769, 3075)\n",
      "all_data shape: (62902, 3075)\n",
      "all_data shape: (65424, 3075)\n",
      "all_data shape: (68391, 3075)\n",
      "all_data shape: (71650, 3075)\n",
      "all_data shape: (74576, 3075)\n",
      "all_data shape: (78255, 3075)\n",
      "all_data shape: (80748, 3075)\n",
      "all_data shape: (82947, 3075)\n",
      "Saved file 85 == 6963200 examples\n",
      "all_data shape: (4073, 3075)\n",
      "all_data shape: (6945, 3075)\n",
      "all_data shape: (10027, 3075)\n",
      "all_data shape: (12662, 3075)\n",
      "all_data shape: (15605, 3075)\n",
      "all_data shape: (17902, 3075)\n",
      "all_data shape: (21074, 3075)\n",
      "all_data shape: (23886, 3075)\n",
      "all_data shape: (26633, 3075)\n",
      "all_data shape: (29900, 3075)\n",
      "all_data shape: (32506, 3075)\n",
      "all_data shape: (35233, 3075)\n",
      "all_data shape: (38424, 3075)\n",
      "all_data shape: (41186, 3075)\n",
      "all_data shape: (44322, 3075)\n",
      "all_data shape: (46936, 3075)\n",
      "all_data shape: (49529, 3075)\n",
      "all_data shape: (52598, 3075)\n",
      "all_data shape: (55158, 3075)\n",
      "all_data shape: (57348, 3075)\n",
      "all_data shape: (60297, 3075)\n",
      "all_data shape: (64004, 3075)\n",
      "all_data shape: (66739, 3075)\n",
      "all_data shape: (69202, 3075)\n",
      "all_data shape: (72042, 3075)\n",
      "all_data shape: (75334, 3075)\n",
      "all_data shape: (77799, 3075)\n",
      "all_data shape: (80963, 3075)\n",
      "all_data shape: (83354, 3075)\n",
      "Saved file 86 == 7045120 examples\n",
      "all_data shape: (3480, 3075)\n",
      "all_data shape: (6743, 3075)\n",
      "all_data shape: (9683, 3075)\n",
      "all_data shape: (12161, 3075)\n",
      "all_data shape: (15179, 3075)\n",
      "all_data shape: (18100, 3075)\n",
      "all_data shape: (21428, 3075)\n",
      "all_data shape: (24100, 3075)\n",
      "all_data shape: (25918, 3075)\n",
      "all_data shape: (29291, 3075)\n",
      "all_data shape: (32253, 3075)\n",
      "all_data shape: (35737, 3075)\n",
      "all_data shape: (39501, 3075)\n",
      "all_data shape: (42325, 3075)\n",
      "all_data shape: (45079, 3075)\n",
      "all_data shape: (48129, 3075)\n",
      "all_data shape: (50681, 3075)\n",
      "all_data shape: (53659, 3075)\n",
      "all_data shape: (56069, 3075)\n",
      "all_data shape: (59114, 3075)\n",
      "all_data shape: (61373, 3075)\n",
      "all_data shape: (64183, 3075)\n",
      "all_data shape: (66792, 3075)\n",
      "all_data shape: (69390, 3075)\n",
      "all_data shape: (71632, 3075)\n",
      "all_data shape: (74455, 3075)\n",
      "all_data shape: (77958, 3075)\n",
      "all_data shape: (80016, 3075)\n",
      "all_data shape: (82722, 3075)\n",
      "Saved file 87 == 7127040 examples\n",
      "all_data shape: (3613, 3075)\n",
      "all_data shape: (5851, 3075)\n",
      "all_data shape: (8830, 3075)\n",
      "all_data shape: (12013, 3075)\n",
      "all_data shape: (14172, 3075)\n",
      "all_data shape: (17119, 3075)\n",
      "all_data shape: (20474, 3075)\n",
      "all_data shape: (23331, 3075)\n",
      "all_data shape: (26027, 3075)\n",
      "all_data shape: (27754, 3075)\n",
      "all_data shape: (31354, 3075)\n",
      "all_data shape: (34556, 3075)\n",
      "all_data shape: (37327, 3075)\n",
      "all_data shape: (39664, 3075)\n",
      "all_data shape: (42556, 3075)\n",
      "all_data shape: (45187, 3075)\n",
      "all_data shape: (47385, 3075)\n",
      "all_data shape: (49797, 3075)\n",
      "all_data shape: (52440, 3075)\n",
      "all_data shape: (55070, 3075)\n",
      "all_data shape: (58178, 3075)\n",
      "all_data shape: (61548, 3075)\n",
      "all_data shape: (63740, 3075)\n",
      "all_data shape: (66546, 3075)\n",
      "all_data shape: (69433, 3075)\n",
      "all_data shape: (71703, 3075)\n",
      "all_data shape: (74726, 3075)\n",
      "all_data shape: (77599, 3075)\n",
      "all_data shape: (80886, 3075)\n",
      "all_data shape: (83178, 3075)\n",
      "Saved file 88 == 7208960 examples\n",
      "all_data shape: (4980, 3075)\n",
      "all_data shape: (7368, 3075)\n",
      "all_data shape: (10689, 3075)\n",
      "all_data shape: (13664, 3075)\n",
      "all_data shape: (16329, 3075)\n",
      "all_data shape: (18845, 3075)\n",
      "all_data shape: (21814, 3075)\n",
      "all_data shape: (24074, 3075)\n",
      "all_data shape: (27436, 3075)\n",
      "all_data shape: (30530, 3075)\n",
      "all_data shape: (33137, 3075)\n",
      "all_data shape: (35631, 3075)\n",
      "all_data shape: (38363, 3075)\n",
      "all_data shape: (41242, 3075)\n",
      "all_data shape: (44672, 3075)\n",
      "all_data shape: (47600, 3075)\n",
      "all_data shape: (50020, 3075)\n",
      "all_data shape: (52429, 3075)\n",
      "all_data shape: (54491, 3075)\n",
      "all_data shape: (56751, 3075)\n",
      "all_data shape: (59185, 3075)\n",
      "all_data shape: (63002, 3075)\n",
      "all_data shape: (66095, 3075)\n",
      "all_data shape: (69719, 3075)\n",
      "all_data shape: (73083, 3075)\n",
      "all_data shape: (75904, 3075)\n",
      "all_data shape: (78555, 3075)\n",
      "all_data shape: (80744, 3075)\n",
      "all_data shape: (83809, 3075)\n",
      "Saved file 89 == 7290880 examples\n",
      "all_data shape: (4571, 3075)\n",
      "all_data shape: (7434, 3075)\n",
      "all_data shape: (9622, 3075)\n",
      "all_data shape: (13535, 3075)\n",
      "all_data shape: (16232, 3075)\n",
      "all_data shape: (19200, 3075)\n",
      "all_data shape: (21667, 3075)\n",
      "all_data shape: (24141, 3075)\n",
      "all_data shape: (26654, 3075)\n",
      "all_data shape: (29999, 3075)\n",
      "all_data shape: (32443, 3075)\n",
      "all_data shape: (35711, 3075)\n",
      "all_data shape: (38557, 3075)\n",
      "all_data shape: (41520, 3075)\n",
      "all_data shape: (44363, 3075)\n",
      "all_data shape: (47178, 3075)\n",
      "all_data shape: (49549, 3075)\n",
      "all_data shape: (51585, 3075)\n",
      "all_data shape: (55183, 3075)\n",
      "all_data shape: (58137, 3075)\n",
      "all_data shape: (60947, 3075)\n",
      "all_data shape: (63728, 3075)\n",
      "all_data shape: (67075, 3075)\n",
      "all_data shape: (69924, 3075)\n",
      "all_data shape: (72766, 3075)\n",
      "all_data shape: (75355, 3075)\n",
      "all_data shape: (78434, 3075)\n",
      "all_data shape: (81622, 3075)\n",
      "all_data shape: (84144, 3075)\n",
      "Saved file 90 == 7372800 examples\n",
      "all_data shape: (5289, 3075)\n",
      "all_data shape: (7451, 3075)\n",
      "all_data shape: (9695, 3075)\n",
      "all_data shape: (12796, 3075)\n",
      "all_data shape: (16176, 3075)\n",
      "all_data shape: (19243, 3075)\n",
      "all_data shape: (22096, 3075)\n",
      "all_data shape: (24386, 3075)\n",
      "all_data shape: (27715, 3075)\n",
      "all_data shape: (30671, 3075)\n",
      "all_data shape: (33783, 3075)\n",
      "all_data shape: (35896, 3075)\n",
      "all_data shape: (39464, 3075)\n",
      "all_data shape: (41615, 3075)\n",
      "all_data shape: (44155, 3075)\n",
      "all_data shape: (47532, 3075)\n",
      "all_data shape: (50824, 3075)\n",
      "all_data shape: (53669, 3075)\n",
      "all_data shape: (56605, 3075)\n",
      "all_data shape: (59311, 3075)\n",
      "all_data shape: (63207, 3075)\n",
      "all_data shape: (65177, 3075)\n",
      "all_data shape: (67837, 3075)\n",
      "all_data shape: (70783, 3075)\n",
      "all_data shape: (73521, 3075)\n",
      "all_data shape: (76686, 3075)\n",
      "all_data shape: (79634, 3075)\n",
      "all_data shape: (83466, 3075)\n",
      "Saved file 91 == 7454720 examples\n",
      "all_data shape: (4253, 3075)\n",
      "all_data shape: (7451, 3075)\n",
      "all_data shape: (10364, 3075)\n",
      "all_data shape: (13247, 3075)\n",
      "all_data shape: (16286, 3075)\n",
      "all_data shape: (19977, 3075)\n",
      "all_data shape: (22938, 3075)\n",
      "all_data shape: (25612, 3075)\n",
      "all_data shape: (28869, 3075)\n",
      "all_data shape: (31555, 3075)\n",
      "all_data shape: (34022, 3075)\n",
      "all_data shape: (36972, 3075)\n",
      "all_data shape: (39867, 3075)\n",
      "all_data shape: (43035, 3075)\n",
      "all_data shape: (45018, 3075)\n",
      "all_data shape: (47711, 3075)\n",
      "all_data shape: (51434, 3075)\n",
      "all_data shape: (53875, 3075)\n",
      "all_data shape: (56660, 3075)\n",
      "all_data shape: (59203, 3075)\n",
      "all_data shape: (61933, 3075)\n",
      "all_data shape: (65119, 3075)\n",
      "all_data shape: (68320, 3075)\n",
      "all_data shape: (71078, 3075)\n",
      "all_data shape: (74151, 3075)\n",
      "all_data shape: (76329, 3075)\n",
      "all_data shape: (78646, 3075)\n",
      "all_data shape: (81619, 3075)\n",
      "all_data shape: (83824, 3075)\n",
      "Saved file 92 == 7536640 examples\n",
      "all_data shape: (4404, 3075)\n",
      "all_data shape: (6321, 3075)\n",
      "all_data shape: (9267, 3075)\n",
      "all_data shape: (12927, 3075)\n",
      "all_data shape: (15865, 3075)\n",
      "all_data shape: (18812, 3075)\n",
      "all_data shape: (20692, 3075)\n",
      "all_data shape: (23004, 3075)\n",
      "all_data shape: (25372, 3075)\n",
      "all_data shape: (28896, 3075)\n",
      "all_data shape: (31862, 3075)\n",
      "all_data shape: (34364, 3075)\n",
      "all_data shape: (36897, 3075)\n",
      "all_data shape: (39718, 3075)\n",
      "all_data shape: (42724, 3075)\n",
      "all_data shape: (45557, 3075)\n",
      "all_data shape: (48953, 3075)\n",
      "all_data shape: (52523, 3075)\n",
      "all_data shape: (54971, 3075)\n",
      "all_data shape: (57769, 3075)\n",
      "all_data shape: (59998, 3075)\n",
      "all_data shape: (62922, 3075)\n",
      "all_data shape: (65819, 3075)\n",
      "all_data shape: (68434, 3075)\n",
      "all_data shape: (71461, 3075)\n",
      "all_data shape: (74663, 3075)\n",
      "all_data shape: (77476, 3075)\n",
      "all_data shape: (80398, 3075)\n",
      "all_data shape: (83556, 3075)\n",
      "Saved file 93 == 7618560 examples\n",
      "all_data shape: (4165, 3075)\n",
      "all_data shape: (6412, 3075)\n",
      "all_data shape: (9884, 3075)\n",
      "all_data shape: (12662, 3075)\n",
      "all_data shape: (15562, 3075)\n",
      "all_data shape: (17561, 3075)\n",
      "all_data shape: (20494, 3075)\n",
      "all_data shape: (23364, 3075)\n",
      "all_data shape: (26064, 3075)\n",
      "all_data shape: (29181, 3075)\n",
      "all_data shape: (31609, 3075)\n",
      "all_data shape: (34034, 3075)\n",
      "all_data shape: (37537, 3075)\n",
      "all_data shape: (39998, 3075)\n",
      "all_data shape: (43467, 3075)\n",
      "all_data shape: (45586, 3075)\n",
      "all_data shape: (48924, 3075)\n",
      "all_data shape: (52584, 3075)\n",
      "all_data shape: (55751, 3075)\n",
      "all_data shape: (57865, 3075)\n",
      "all_data shape: (60973, 3075)\n",
      "all_data shape: (64276, 3075)\n",
      "all_data shape: (66829, 3075)\n",
      "all_data shape: (70364, 3075)\n",
      "all_data shape: (73892, 3075)\n",
      "all_data shape: (76709, 3075)\n",
      "all_data shape: (79756, 3075)\n",
      "all_data shape: (82152, 3075)\n",
      "Saved file 94 == 7700480 examples\n",
      "all_data shape: (3510, 3075)\n",
      "all_data shape: (5787, 3075)\n",
      "all_data shape: (8935, 3075)\n",
      "all_data shape: (12511, 3075)\n",
      "all_data shape: (15382, 3075)\n",
      "all_data shape: (18044, 3075)\n",
      "all_data shape: (21106, 3075)\n",
      "all_data shape: (23829, 3075)\n",
      "all_data shape: (26460, 3075)\n",
      "all_data shape: (29945, 3075)\n",
      "all_data shape: (32992, 3075)\n",
      "all_data shape: (35466, 3075)\n",
      "all_data shape: (38911, 3075)\n",
      "all_data shape: (42192, 3075)\n",
      "all_data shape: (45765, 3075)\n",
      "all_data shape: (48290, 3075)\n",
      "all_data shape: (51129, 3075)\n",
      "all_data shape: (54317, 3075)\n",
      "all_data shape: (56824, 3075)\n",
      "all_data shape: (60175, 3075)\n",
      "all_data shape: (63424, 3075)\n",
      "all_data shape: (65707, 3075)\n",
      "all_data shape: (68113, 3075)\n",
      "all_data shape: (71711, 3075)\n",
      "all_data shape: (74308, 3075)\n",
      "all_data shape: (77238, 3075)\n",
      "all_data shape: (80025, 3075)\n",
      "all_data shape: (82688, 3075)\n",
      "Saved file 95 == 7782400 examples\n",
      "all_data shape: (2850, 3075)\n",
      "all_data shape: (4830, 3075)\n",
      "all_data shape: (7008, 3075)\n",
      "all_data shape: (9969, 3075)\n",
      "all_data shape: (11730, 3075)\n",
      "all_data shape: (14681, 3075)\n",
      "all_data shape: (17750, 3075)\n",
      "all_data shape: (19867, 3075)\n",
      "all_data shape: (22935, 3075)\n",
      "all_data shape: (26046, 3075)\n",
      "all_data shape: (28582, 3075)\n",
      "all_data shape: (30864, 3075)\n",
      "all_data shape: (33798, 3075)\n",
      "all_data shape: (36165, 3075)\n",
      "all_data shape: (39462, 3075)\n",
      "all_data shape: (42738, 3075)\n",
      "all_data shape: (45321, 3075)\n",
      "all_data shape: (47673, 3075)\n",
      "all_data shape: (50763, 3075)\n",
      "all_data shape: (53425, 3075)\n",
      "all_data shape: (56298, 3075)\n",
      "all_data shape: (59277, 3075)\n",
      "all_data shape: (62360, 3075)\n",
      "all_data shape: (64987, 3075)\n",
      "all_data shape: (68082, 3075)\n",
      "all_data shape: (70975, 3075)\n",
      "all_data shape: (74006, 3075)\n",
      "all_data shape: (76605, 3075)\n",
      "all_data shape: (79824, 3075)\n",
      "all_data shape: (82714, 3075)\n",
      "Saved file 96 == 7864320 examples\n",
      "all_data shape: (3386, 3075)\n",
      "all_data shape: (6370, 3075)\n",
      "all_data shape: (8700, 3075)\n",
      "all_data shape: (11482, 3075)\n",
      "all_data shape: (14160, 3075)\n",
      "all_data shape: (16782, 3075)\n",
      "all_data shape: (19337, 3075)\n",
      "all_data shape: (22005, 3075)\n",
      "all_data shape: (25238, 3075)\n",
      "all_data shape: (28246, 3075)\n",
      "all_data shape: (31065, 3075)\n",
      "all_data shape: (33218, 3075)\n",
      "all_data shape: (35792, 3075)\n",
      "all_data shape: (39138, 3075)\n",
      "all_data shape: (41954, 3075)\n",
      "all_data shape: (44379, 3075)\n",
      "all_data shape: (47443, 3075)\n",
      "all_data shape: (50149, 3075)\n",
      "all_data shape: (53541, 3075)\n",
      "all_data shape: (56971, 3075)\n",
      "all_data shape: (59705, 3075)\n",
      "all_data shape: (62261, 3075)\n",
      "all_data shape: (64612, 3075)\n",
      "all_data shape: (67515, 3075)\n",
      "all_data shape: (70212, 3075)\n",
      "all_data shape: (73005, 3075)\n",
      "all_data shape: (75162, 3075)\n",
      "all_data shape: (77679, 3075)\n",
      "all_data shape: (80549, 3075)\n",
      "all_data shape: (82732, 3075)\n",
      "Saved file 97 == 7946240 examples\n",
      "all_data shape: (3781, 3075)\n",
      "all_data shape: (6815, 3075)\n",
      "all_data shape: (9618, 3075)\n",
      "all_data shape: (12475, 3075)\n",
      "all_data shape: (14669, 3075)\n",
      "all_data shape: (16416, 3075)\n",
      "all_data shape: (19507, 3075)\n",
      "all_data shape: (22221, 3075)\n",
      "all_data shape: (25363, 3075)\n",
      "all_data shape: (28911, 3075)\n",
      "all_data shape: (31360, 3075)\n",
      "all_data shape: (34852, 3075)\n",
      "all_data shape: (38188, 3075)\n",
      "all_data shape: (41173, 3075)\n",
      "all_data shape: (44261, 3075)\n",
      "all_data shape: (47638, 3075)\n",
      "all_data shape: (50050, 3075)\n",
      "all_data shape: (53039, 3075)\n",
      "all_data shape: (55242, 3075)\n",
      "all_data shape: (57559, 3075)\n",
      "all_data shape: (60458, 3075)\n",
      "all_data shape: (63612, 3075)\n",
      "all_data shape: (67171, 3075)\n",
      "all_data shape: (70250, 3075)\n",
      "all_data shape: (73512, 3075)\n",
      "all_data shape: (76366, 3075)\n",
      "all_data shape: (79982, 3075)\n",
      "all_data shape: (82194, 3075)\n",
      "Saved file 98 == 8028160 examples\n",
      "all_data shape: (2910, 3075)\n",
      "all_data shape: (5778, 3075)\n",
      "all_data shape: (8127, 3075)\n",
      "all_data shape: (10779, 3075)\n",
      "all_data shape: (13375, 3075)\n",
      "all_data shape: (16116, 3075)\n",
      "all_data shape: (19277, 3075)\n",
      "all_data shape: (22404, 3075)\n",
      "all_data shape: (25489, 3075)\n",
      "all_data shape: (28385, 3075)\n",
      "all_data shape: (31073, 3075)\n",
      "all_data shape: (34307, 3075)\n",
      "all_data shape: (37306, 3075)\n",
      "all_data shape: (40490, 3075)\n",
      "all_data shape: (43188, 3075)\n",
      "all_data shape: (45439, 3075)\n",
      "all_data shape: (49181, 3075)\n",
      "all_data shape: (51533, 3075)\n",
      "all_data shape: (54740, 3075)\n",
      "all_data shape: (57494, 3075)\n",
      "all_data shape: (60334, 3075)\n",
      "all_data shape: (63779, 3075)\n",
      "all_data shape: (67117, 3075)\n",
      "all_data shape: (69190, 3075)\n",
      "all_data shape: (72455, 3075)\n",
      "all_data shape: (75914, 3075)\n",
      "all_data shape: (77755, 3075)\n",
      "all_data shape: (80777, 3075)\n",
      "all_data shape: (83819, 3075)\n",
      "Saved file 99 == 8110080 examples\n",
      "all_data shape: (4786, 3075)\n",
      "all_data shape: (7443, 3075)\n",
      "all_data shape: (10401, 3075)\n",
      "all_data shape: (13835, 3075)\n",
      "all_data shape: (16839, 3075)\n",
      "all_data shape: (19595, 3075)\n",
      "all_data shape: (22918, 3075)\n",
      "all_data shape: (25421, 3075)\n",
      "all_data shape: (28178, 3075)\n",
      "all_data shape: (31503, 3075)\n",
      "all_data shape: (34092, 3075)\n",
      "all_data shape: (37288, 3075)\n",
      "all_data shape: (40525, 3075)\n",
      "all_data shape: (43016, 3075)\n",
      "all_data shape: (45925, 3075)\n",
      "all_data shape: (48664, 3075)\n",
      "all_data shape: (51585, 3075)\n",
      "all_data shape: (54968, 3075)\n",
      "all_data shape: (58466, 3075)\n",
      "all_data shape: (61706, 3075)\n",
      "all_data shape: (64816, 3075)\n",
      "all_data shape: (67029, 3075)\n",
      "all_data shape: (69755, 3075)\n",
      "all_data shape: (72970, 3075)\n",
      "all_data shape: (75594, 3075)\n",
      "all_data shape: (78364, 3075)\n",
      "all_data shape: (81606, 3075)\n",
      "all_data shape: (84953, 3075)\n",
      "Saved file 100 == 8192000 examples\n",
      "all_data shape: (5430, 3075)\n",
      "all_data shape: (7796, 3075)\n",
      "all_data shape: (11549, 3075)\n",
      "all_data shape: (13892, 3075)\n",
      "all_data shape: (15989, 3075)\n",
      "all_data shape: (19328, 3075)\n",
      "all_data shape: (22620, 3075)\n",
      "all_data shape: (25024, 3075)\n",
      "all_data shape: (28617, 3075)\n",
      "all_data shape: (30608, 3075)\n",
      "all_data shape: (34061, 3075)\n",
      "all_data shape: (36868, 3075)\n",
      "all_data shape: (40061, 3075)\n",
      "all_data shape: (43433, 3075)\n",
      "all_data shape: (45087, 3075)\n",
      "all_data shape: (47006, 3075)\n",
      "all_data shape: (49777, 3075)\n",
      "all_data shape: (51525, 3075)\n",
      "all_data shape: (53821, 3075)\n",
      "all_data shape: (56673, 3075)\n",
      "all_data shape: (58991, 3075)\n",
      "all_data shape: (61641, 3075)\n",
      "all_data shape: (64089, 3075)\n",
      "all_data shape: (66729, 3075)\n",
      "all_data shape: (70177, 3075)\n",
      "all_data shape: (72814, 3075)\n",
      "all_data shape: (74235, 3075)\n",
      "all_data shape: (76791, 3075)\n",
      "all_data shape: (80163, 3075)\n",
      "all_data shape: (82921, 3075)\n",
      "Saved file 101 == 8273920 examples\n",
      "all_data shape: (3773, 3075)\n",
      "all_data shape: (6735, 3075)\n",
      "all_data shape: (9560, 3075)\n",
      "all_data shape: (12858, 3075)\n",
      "all_data shape: (15154, 3075)\n",
      "all_data shape: (17161, 3075)\n",
      "all_data shape: (20156, 3075)\n",
      "all_data shape: (22716, 3075)\n",
      "all_data shape: (25570, 3075)\n",
      "all_data shape: (27956, 3075)\n",
      "all_data shape: (31334, 3075)\n",
      "all_data shape: (34093, 3075)\n",
      "all_data shape: (35976, 3075)\n",
      "all_data shape: (39196, 3075)\n",
      "all_data shape: (41912, 3075)\n",
      "all_data shape: (44465, 3075)\n",
      "all_data shape: (46505, 3075)\n",
      "all_data shape: (49336, 3075)\n",
      "all_data shape: (51906, 3075)\n",
      "all_data shape: (54849, 3075)\n",
      "all_data shape: (57643, 3075)\n",
      "all_data shape: (60219, 3075)\n",
      "all_data shape: (63629, 3075)\n",
      "all_data shape: (65919, 3075)\n",
      "all_data shape: (69132, 3075)\n",
      "all_data shape: (72584, 3075)\n",
      "all_data shape: (75037, 3075)\n",
      "all_data shape: (78291, 3075)\n",
      "all_data shape: (81531, 3075)\n",
      "all_data shape: (83467, 3075)\n",
      "Saved file 102 == 8355840 examples\n",
      "all_data shape: (4088, 3075)\n",
      "all_data shape: (7630, 3075)\n",
      "all_data shape: (9918, 3075)\n",
      "all_data shape: (12537, 3075)\n",
      "all_data shape: (15182, 3075)\n",
      "all_data shape: (18824, 3075)\n",
      "all_data shape: (21470, 3075)\n",
      "all_data shape: (24895, 3075)\n",
      "all_data shape: (26458, 3075)\n",
      "all_data shape: (29081, 3075)\n",
      "all_data shape: (31297, 3075)\n",
      "all_data shape: (34358, 3075)\n",
      "all_data shape: (37251, 3075)\n",
      "all_data shape: (40158, 3075)\n",
      "all_data shape: (43471, 3075)\n",
      "all_data shape: (46449, 3075)\n",
      "all_data shape: (49687, 3075)\n",
      "all_data shape: (52466, 3075)\n",
      "all_data shape: (55853, 3075)\n",
      "all_data shape: (58785, 3075)\n",
      "all_data shape: (61548, 3075)\n",
      "all_data shape: (64894, 3075)\n",
      "all_data shape: (67304, 3075)\n",
      "all_data shape: (69957, 3075)\n",
      "all_data shape: (72125, 3075)\n",
      "all_data shape: (75585, 3075)\n",
      "all_data shape: (78211, 3075)\n",
      "all_data shape: (80591, 3075)\n",
      "all_data shape: (84381, 3075)\n",
      "Saved file 103 == 8437760 examples\n",
      "all_data shape: (4118, 3075)\n",
      "all_data shape: (7021, 3075)\n",
      "all_data shape: (10452, 3075)\n",
      "all_data shape: (13023, 3075)\n",
      "all_data shape: (16065, 3075)\n",
      "all_data shape: (18767, 3075)\n",
      "all_data shape: (21723, 3075)\n",
      "all_data shape: (24034, 3075)\n",
      "all_data shape: (26049, 3075)\n",
      "all_data shape: (29685, 3075)\n",
      "all_data shape: (32799, 3075)\n",
      "all_data shape: (36315, 3075)\n",
      "all_data shape: (39129, 3075)\n",
      "all_data shape: (41535, 3075)\n",
      "all_data shape: (44849, 3075)\n",
      "all_data shape: (47775, 3075)\n",
      "all_data shape: (50215, 3075)\n",
      "all_data shape: (53231, 3075)\n",
      "all_data shape: (56572, 3075)\n",
      "all_data shape: (59332, 3075)\n",
      "all_data shape: (63125, 3075)\n",
      "all_data shape: (66423, 3075)\n",
      "all_data shape: (69236, 3075)\n",
      "all_data shape: (71964, 3075)\n",
      "all_data shape: (74856, 3075)\n",
      "all_data shape: (77200, 3075)\n",
      "all_data shape: (80250, 3075)\n",
      "all_data shape: (83545, 3075)\n",
      "Saved file 104 == 8519680 examples\n",
      "all_data shape: (4316, 3075)\n",
      "all_data shape: (7427, 3075)\n",
      "all_data shape: (10913, 3075)\n",
      "all_data shape: (12927, 3075)\n",
      "all_data shape: (15878, 3075)\n",
      "all_data shape: (18106, 3075)\n",
      "all_data shape: (21336, 3075)\n",
      "all_data shape: (23591, 3075)\n",
      "all_data shape: (26037, 3075)\n",
      "all_data shape: (29030, 3075)\n",
      "all_data shape: (32106, 3075)\n",
      "all_data shape: (35165, 3075)\n",
      "all_data shape: (37290, 3075)\n",
      "all_data shape: (39713, 3075)\n",
      "all_data shape: (42976, 3075)\n",
      "all_data shape: (46505, 3075)\n",
      "all_data shape: (49380, 3075)\n",
      "all_data shape: (52584, 3075)\n",
      "all_data shape: (55989, 3075)\n",
      "all_data shape: (59166, 3075)\n",
      "all_data shape: (61899, 3075)\n",
      "all_data shape: (64138, 3075)\n",
      "all_data shape: (66060, 3075)\n",
      "all_data shape: (68053, 3075)\n",
      "all_data shape: (70556, 3075)\n",
      "all_data shape: (73338, 3075)\n",
      "all_data shape: (76050, 3075)\n",
      "all_data shape: (78420, 3075)\n",
      "all_data shape: (80119, 3075)\n",
      "all_data shape: (83337, 3075)\n",
      "Saved file 105 == 8601600 examples\n",
      "all_data shape: (4239, 3075)\n",
      "all_data shape: (6840, 3075)\n",
      "all_data shape: (9654, 3075)\n",
      "all_data shape: (12550, 3075)\n",
      "all_data shape: (14860, 3075)\n",
      "all_data shape: (18002, 3075)\n",
      "all_data shape: (20621, 3075)\n",
      "all_data shape: (24030, 3075)\n",
      "all_data shape: (27384, 3075)\n",
      "all_data shape: (30255, 3075)\n",
      "all_data shape: (33124, 3075)\n",
      "all_data shape: (36795, 3075)\n",
      "all_data shape: (38722, 3075)\n",
      "all_data shape: (40247, 3075)\n",
      "all_data shape: (43147, 3075)\n",
      "all_data shape: (45429, 3075)\n",
      "all_data shape: (48177, 3075)\n",
      "all_data shape: (51666, 3075)\n",
      "all_data shape: (54234, 3075)\n",
      "all_data shape: (56836, 3075)\n",
      "all_data shape: (59722, 3075)\n",
      "all_data shape: (62404, 3075)\n",
      "all_data shape: (64263, 3075)\n",
      "all_data shape: (66856, 3075)\n",
      "all_data shape: (69268, 3075)\n",
      "all_data shape: (72641, 3075)\n",
      "all_data shape: (76233, 3075)\n",
      "all_data shape: (78951, 3075)\n",
      "all_data shape: (81941, 3075)\n",
      "Saved file 106 == 8683520 examples\n",
      "all_data shape: (3456, 3075)\n",
      "all_data shape: (6842, 3075)\n",
      "all_data shape: (9359, 3075)\n",
      "all_data shape: (11509, 3075)\n",
      "all_data shape: (13793, 3075)\n",
      "all_data shape: (17017, 3075)\n",
      "all_data shape: (19648, 3075)\n",
      "all_data shape: (21578, 3075)\n",
      "all_data shape: (24520, 3075)\n",
      "all_data shape: (26884, 3075)\n",
      "all_data shape: (29616, 3075)\n",
      "all_data shape: (32691, 3075)\n",
      "all_data shape: (35632, 3075)\n",
      "all_data shape: (38090, 3075)\n",
      "all_data shape: (41362, 3075)\n",
      "all_data shape: (44415, 3075)\n",
      "all_data shape: (47737, 3075)\n",
      "all_data shape: (50900, 3075)\n",
      "all_data shape: (53958, 3075)\n",
      "all_data shape: (56633, 3075)\n",
      "all_data shape: (59627, 3075)\n",
      "all_data shape: (63096, 3075)\n",
      "all_data shape: (65303, 3075)\n",
      "all_data shape: (67709, 3075)\n",
      "all_data shape: (70042, 3075)\n",
      "all_data shape: (72304, 3075)\n",
      "all_data shape: (74840, 3075)\n",
      "all_data shape: (77583, 3075)\n",
      "all_data shape: (80854, 3075)\n",
      "all_data shape: (84318, 3075)\n",
      "Saved file 107 == 8765440 examples\n",
      "all_data shape: (4214, 3075)\n",
      "all_data shape: (7227, 3075)\n",
      "all_data shape: (10164, 3075)\n",
      "all_data shape: (13757, 3075)\n",
      "all_data shape: (17543, 3075)\n",
      "all_data shape: (20350, 3075)\n",
      "all_data shape: (22616, 3075)\n",
      "all_data shape: (25498, 3075)\n",
      "all_data shape: (28876, 3075)\n",
      "all_data shape: (32335, 3075)\n",
      "all_data shape: (35378, 3075)\n",
      "all_data shape: (37782, 3075)\n",
      "all_data shape: (40024, 3075)\n",
      "all_data shape: (42375, 3075)\n",
      "all_data shape: (45391, 3075)\n",
      "all_data shape: (48217, 3075)\n",
      "all_data shape: (50951, 3075)\n",
      "all_data shape: (53970, 3075)\n",
      "all_data shape: (57148, 3075)\n",
      "all_data shape: (59885, 3075)\n",
      "all_data shape: (62045, 3075)\n",
      "all_data shape: (64481, 3075)\n",
      "all_data shape: (68263, 3075)\n",
      "all_data shape: (71301, 3075)\n",
      "all_data shape: (75058, 3075)\n",
      "all_data shape: (78234, 3075)\n",
      "all_data shape: (81558, 3075)\n",
      "all_data shape: (83592, 3075)\n",
      "Saved file 108 == 8847360 examples\n",
      "all_data shape: (4549, 3075)\n",
      "all_data shape: (7416, 3075)\n",
      "all_data shape: (10607, 3075)\n",
      "all_data shape: (13230, 3075)\n",
      "all_data shape: (16295, 3075)\n",
      "all_data shape: (19775, 3075)\n",
      "all_data shape: (21616, 3075)\n",
      "all_data shape: (23635, 3075)\n",
      "all_data shape: (26754, 3075)\n",
      "all_data shape: (29698, 3075)\n",
      "all_data shape: (33011, 3075)\n",
      "all_data shape: (35511, 3075)\n",
      "all_data shape: (38696, 3075)\n",
      "all_data shape: (41334, 3075)\n",
      "all_data shape: (44340, 3075)\n",
      "all_data shape: (46781, 3075)\n",
      "all_data shape: (49395, 3075)\n",
      "all_data shape: (51892, 3075)\n",
      "all_data shape: (53750, 3075)\n",
      "all_data shape: (56486, 3075)\n",
      "all_data shape: (59364, 3075)\n",
      "all_data shape: (62279, 3075)\n",
      "all_data shape: (64880, 3075)\n",
      "all_data shape: (68111, 3075)\n",
      "all_data shape: (70271, 3075)\n",
      "all_data shape: (72726, 3075)\n",
      "all_data shape: (76292, 3075)\n",
      "all_data shape: (78898, 3075)\n",
      "all_data shape: (82426, 3075)\n",
      "Saved file 109 == 8929280 examples\n",
      "all_data shape: (3868, 3075)\n",
      "all_data shape: (6337, 3075)\n",
      "all_data shape: (8746, 3075)\n",
      "all_data shape: (11487, 3075)\n",
      "all_data shape: (15427, 3075)\n",
      "all_data shape: (18433, 3075)\n",
      "all_data shape: (21367, 3075)\n",
      "all_data shape: (24287, 3075)\n",
      "all_data shape: (26947, 3075)\n",
      "all_data shape: (29907, 3075)\n",
      "all_data shape: (32937, 3075)\n",
      "all_data shape: (35206, 3075)\n",
      "all_data shape: (38033, 3075)\n",
      "all_data shape: (41940, 3075)\n",
      "all_data shape: (44567, 3075)\n",
      "all_data shape: (47685, 3075)\n",
      "all_data shape: (49736, 3075)\n",
      "all_data shape: (52221, 3075)\n",
      "all_data shape: (54581, 3075)\n",
      "all_data shape: (58227, 3075)\n",
      "all_data shape: (61424, 3075)\n",
      "all_data shape: (63945, 3075)\n",
      "all_data shape: (66912, 3075)\n",
      "all_data shape: (69712, 3075)\n",
      "all_data shape: (72955, 3075)\n",
      "all_data shape: (75005, 3075)\n",
      "all_data shape: (77579, 3075)\n",
      "all_data shape: (81311, 3075)\n",
      "all_data shape: (84517, 3075)\n",
      "Saved file 110 == 9011200 examples\n",
      "all_data shape: (5920, 3075)\n",
      "all_data shape: (8400, 3075)\n",
      "all_data shape: (11829, 3075)\n",
      "all_data shape: (14894, 3075)\n",
      "all_data shape: (16979, 3075)\n",
      "all_data shape: (19626, 3075)\n",
      "all_data shape: (23011, 3075)\n",
      "all_data shape: (25924, 3075)\n",
      "all_data shape: (28774, 3075)\n",
      "all_data shape: (31516, 3075)\n",
      "all_data shape: (33914, 3075)\n",
      "all_data shape: (37018, 3075)\n",
      "all_data shape: (39992, 3075)\n",
      "all_data shape: (42883, 3075)\n",
      "all_data shape: (46019, 3075)\n",
      "all_data shape: (48252, 3075)\n",
      "all_data shape: (50777, 3075)\n",
      "all_data shape: (53087, 3075)\n",
      "all_data shape: (55019, 3075)\n",
      "all_data shape: (57757, 3075)\n",
      "all_data shape: (60637, 3075)\n",
      "all_data shape: (63873, 3075)\n",
      "all_data shape: (66525, 3075)\n",
      "all_data shape: (69770, 3075)\n",
      "all_data shape: (71686, 3075)\n",
      "all_data shape: (73877, 3075)\n",
      "all_data shape: (76862, 3075)\n",
      "all_data shape: (80293, 3075)\n",
      "all_data shape: (82989, 3075)\n",
      "Saved file 111 == 9093120 examples\n",
      "all_data shape: (4197, 3075)\n",
      "all_data shape: (7264, 3075)\n",
      "all_data shape: (10313, 3075)\n",
      "all_data shape: (13442, 3075)\n",
      "all_data shape: (15639, 3075)\n",
      "all_data shape: (18743, 3075)\n",
      "all_data shape: (22549, 3075)\n",
      "all_data shape: (24865, 3075)\n",
      "all_data shape: (28172, 3075)\n",
      "all_data shape: (31317, 3075)\n",
      "all_data shape: (34405, 3075)\n",
      "all_data shape: (36730, 3075)\n",
      "all_data shape: (39423, 3075)\n",
      "all_data shape: (42573, 3075)\n",
      "all_data shape: (46076, 3075)\n",
      "all_data shape: (48514, 3075)\n",
      "all_data shape: (50556, 3075)\n",
      "all_data shape: (52777, 3075)\n",
      "all_data shape: (54558, 3075)\n",
      "all_data shape: (57399, 3075)\n",
      "all_data shape: (59975, 3075)\n",
      "all_data shape: (62293, 3075)\n",
      "all_data shape: (65464, 3075)\n",
      "all_data shape: (68560, 3075)\n",
      "all_data shape: (72530, 3075)\n",
      "all_data shape: (75212, 3075)\n",
      "all_data shape: (77960, 3075)\n",
      "all_data shape: (80252, 3075)\n",
      "all_data shape: (83935, 3075)\n",
      "Saved file 112 == 9175040 examples\n",
      "all_data shape: (4941, 3075)\n",
      "all_data shape: (8192, 3075)\n",
      "all_data shape: (11596, 3075)\n",
      "all_data shape: (14011, 3075)\n",
      "all_data shape: (16924, 3075)\n",
      "all_data shape: (19905, 3075)\n",
      "all_data shape: (21542, 3075)\n",
      "all_data shape: (24499, 3075)\n",
      "all_data shape: (27079, 3075)\n",
      "all_data shape: (29204, 3075)\n",
      "all_data shape: (31761, 3075)\n",
      "all_data shape: (34522, 3075)\n",
      "all_data shape: (36973, 3075)\n",
      "all_data shape: (39782, 3075)\n",
      "all_data shape: (41999, 3075)\n",
      "all_data shape: (44474, 3075)\n",
      "all_data shape: (47660, 3075)\n",
      "all_data shape: (51569, 3075)\n",
      "all_data shape: (54089, 3075)\n",
      "all_data shape: (57620, 3075)\n",
      "all_data shape: (60885, 3075)\n",
      "all_data shape: (63999, 3075)\n",
      "all_data shape: (67144, 3075)\n",
      "all_data shape: (69934, 3075)\n",
      "all_data shape: (73592, 3075)\n",
      "all_data shape: (76985, 3075)\n",
      "all_data shape: (79344, 3075)\n",
      "all_data shape: (81503, 3075)\n",
      "all_data shape: (84368, 3075)\n",
      "Saved file 113 == 9256960 examples\n",
      "all_data shape: (5881, 3075)\n",
      "all_data shape: (8928, 3075)\n",
      "all_data shape: (11728, 3075)\n",
      "all_data shape: (14231, 3075)\n",
      "all_data shape: (16930, 3075)\n",
      "all_data shape: (19794, 3075)\n",
      "all_data shape: (23477, 3075)\n",
      "all_data shape: (27049, 3075)\n",
      "all_data shape: (30592, 3075)\n",
      "all_data shape: (33321, 3075)\n",
      "all_data shape: (35620, 3075)\n",
      "all_data shape: (39139, 3075)\n",
      "all_data shape: (41508, 3075)\n",
      "all_data shape: (43792, 3075)\n",
      "all_data shape: (46929, 3075)\n",
      "all_data shape: (50007, 3075)\n",
      "all_data shape: (53581, 3075)\n",
      "all_data shape: (56420, 3075)\n",
      "all_data shape: (59459, 3075)\n",
      "all_data shape: (62283, 3075)\n",
      "all_data shape: (65329, 3075)\n",
      "all_data shape: (67169, 3075)\n",
      "all_data shape: (70864, 3075)\n",
      "all_data shape: (73537, 3075)\n",
      "all_data shape: (75654, 3075)\n",
      "all_data shape: (78530, 3075)\n",
      "all_data shape: (81682, 3075)\n",
      "all_data shape: (84624, 3075)\n",
      "Saved file 114 == 9338880 examples\n",
      "all_data shape: (5947, 3075)\n",
      "all_data shape: (9443, 3075)\n",
      "all_data shape: (12055, 3075)\n",
      "all_data shape: (14939, 3075)\n",
      "all_data shape: (17957, 3075)\n",
      "all_data shape: (20889, 3075)\n",
      "all_data shape: (24153, 3075)\n",
      "all_data shape: (27451, 3075)\n",
      "all_data shape: (30565, 3075)\n",
      "all_data shape: (33596, 3075)\n",
      "all_data shape: (36378, 3075)\n",
      "all_data shape: (39269, 3075)\n",
      "all_data shape: (42569, 3075)\n",
      "all_data shape: (45787, 3075)\n",
      "all_data shape: (48616, 3075)\n",
      "all_data shape: (51880, 3075)\n",
      "all_data shape: (55133, 3075)\n",
      "all_data shape: (57832, 3075)\n",
      "all_data shape: (61029, 3075)\n",
      "all_data shape: (64113, 3075)\n",
      "all_data shape: (66991, 3075)\n",
      "all_data shape: (69443, 3075)\n",
      "all_data shape: (72824, 3075)\n",
      "all_data shape: (75733, 3075)\n",
      "all_data shape: (78467, 3075)\n",
      "all_data shape: (81843, 3075)\n",
      "all_data shape: (84820, 3075)\n",
      "Saved file 115 == 9420800 examples\n",
      "all_data shape: (6138, 3075)\n",
      "all_data shape: (8655, 3075)\n",
      "all_data shape: (11438, 3075)\n",
      "all_data shape: (14179, 3075)\n",
      "all_data shape: (16698, 3075)\n",
      "all_data shape: (19604, 3075)\n",
      "all_data shape: (22936, 3075)\n",
      "all_data shape: (26188, 3075)\n",
      "all_data shape: (29200, 3075)\n",
      "all_data shape: (32266, 3075)\n",
      "all_data shape: (34943, 3075)\n",
      "all_data shape: (37705, 3075)\n",
      "all_data shape: (40942, 3075)\n",
      "all_data shape: (43913, 3075)\n",
      "all_data shape: (46424, 3075)\n",
      "all_data shape: (48905, 3075)\n",
      "all_data shape: (52111, 3075)\n",
      "all_data shape: (55379, 3075)\n",
      "all_data shape: (57798, 3075)\n",
      "all_data shape: (59926, 3075)\n",
      "all_data shape: (62876, 3075)\n",
      "all_data shape: (65684, 3075)\n",
      "all_data shape: (68364, 3075)\n",
      "all_data shape: (71480, 3075)\n",
      "all_data shape: (74498, 3075)\n",
      "all_data shape: (76847, 3075)\n",
      "all_data shape: (78372, 3075)\n",
      "all_data shape: (81841, 3075)\n",
      "all_data shape: (84124, 3075)\n",
      "Saved file 116 == 9502720 examples\n",
      "all_data shape: (4546, 3075)\n",
      "all_data shape: (7634, 3075)\n",
      "all_data shape: (10195, 3075)\n",
      "all_data shape: (12996, 3075)\n",
      "all_data shape: (14687, 3075)\n",
      "all_data shape: (17878, 3075)\n",
      "all_data shape: (20303, 3075)\n",
      "all_data shape: (22185, 3075)\n",
      "all_data shape: (25076, 3075)\n",
      "all_data shape: (27341, 3075)\n",
      "all_data shape: (29552, 3075)\n",
      "all_data shape: (32907, 3075)\n",
      "all_data shape: (36478, 3075)\n",
      "all_data shape: (39546, 3075)\n",
      "all_data shape: (42208, 3075)\n",
      "all_data shape: (44936, 3075)\n",
      "all_data shape: (48471, 3075)\n",
      "all_data shape: (50826, 3075)\n",
      "all_data shape: (53424, 3075)\n",
      "all_data shape: (56517, 3075)\n",
      "all_data shape: (59064, 3075)\n",
      "all_data shape: (61576, 3075)\n",
      "all_data shape: (64463, 3075)\n",
      "all_data shape: (67588, 3075)\n",
      "all_data shape: (70608, 3075)\n",
      "all_data shape: (74328, 3075)\n",
      "all_data shape: (77286, 3075)\n",
      "all_data shape: (80211, 3075)\n",
      "all_data shape: (83268, 3075)\n",
      "Saved file 117 == 9584640 examples\n",
      "all_data shape: (3921, 3075)\n",
      "all_data shape: (5999, 3075)\n",
      "all_data shape: (9283, 3075)\n",
      "all_data shape: (11195, 3075)\n",
      "all_data shape: (14584, 3075)\n",
      "all_data shape: (16078, 3075)\n",
      "all_data shape: (19139, 3075)\n",
      "all_data shape: (22216, 3075)\n",
      "all_data shape: (24700, 3075)\n",
      "all_data shape: (28249, 3075)\n",
      "all_data shape: (31021, 3075)\n",
      "all_data shape: (34195, 3075)\n",
      "all_data shape: (37650, 3075)\n",
      "all_data shape: (40533, 3075)\n",
      "all_data shape: (43021, 3075)\n",
      "all_data shape: (46182, 3075)\n",
      "all_data shape: (48664, 3075)\n",
      "all_data shape: (51916, 3075)\n",
      "all_data shape: (55197, 3075)\n",
      "all_data shape: (58117, 3075)\n",
      "all_data shape: (60154, 3075)\n",
      "all_data shape: (62363, 3075)\n",
      "all_data shape: (65316, 3075)\n",
      "all_data shape: (68440, 3075)\n",
      "all_data shape: (70856, 3075)\n",
      "all_data shape: (72742, 3075)\n",
      "all_data shape: (75587, 3075)\n",
      "all_data shape: (78311, 3075)\n",
      "all_data shape: (81373, 3075)\n",
      "all_data shape: (84224, 3075)\n",
      "Saved file 118 == 9666560 examples\n",
      "all_data shape: (5223, 3075)\n",
      "all_data shape: (8068, 3075)\n",
      "all_data shape: (10833, 3075)\n",
      "all_data shape: (12772, 3075)\n",
      "all_data shape: (15934, 3075)\n",
      "all_data shape: (17985, 3075)\n",
      "all_data shape: (21271, 3075)\n",
      "all_data shape: (24937, 3075)\n",
      "all_data shape: (27369, 3075)\n",
      "all_data shape: (30400, 3075)\n",
      "all_data shape: (33062, 3075)\n",
      "all_data shape: (36201, 3075)\n",
      "all_data shape: (39376, 3075)\n",
      "all_data shape: (42204, 3075)\n",
      "all_data shape: (44926, 3075)\n",
      "all_data shape: (47747, 3075)\n",
      "all_data shape: (50058, 3075)\n",
      "all_data shape: (52581, 3075)\n",
      "all_data shape: (55146, 3075)\n",
      "all_data shape: (58049, 3075)\n",
      "all_data shape: (60028, 3075)\n",
      "all_data shape: (62790, 3075)\n",
      "all_data shape: (66303, 3075)\n",
      "all_data shape: (69014, 3075)\n",
      "all_data shape: (72099, 3075)\n",
      "all_data shape: (74650, 3075)\n",
      "all_data shape: (77319, 3075)\n",
      "all_data shape: (79674, 3075)\n",
      "all_data shape: (81949, 3075)\n",
      "Saved file 119 == 9748480 examples\n",
      "all_data shape: (2387, 3075)\n",
      "all_data shape: (5911, 3075)\n",
      "all_data shape: (8365, 3075)\n",
      "all_data shape: (10812, 3075)\n",
      "all_data shape: (14111, 3075)\n",
      "all_data shape: (16599, 3075)\n",
      "all_data shape: (18792, 3075)\n",
      "all_data shape: (20768, 3075)\n",
      "all_data shape: (23611, 3075)\n",
      "all_data shape: (26059, 3075)\n",
      "all_data shape: (29545, 3075)\n",
      "all_data shape: (32889, 3075)\n",
      "all_data shape: (35067, 3075)\n",
      "all_data shape: (38816, 3075)\n",
      "all_data shape: (41783, 3075)\n",
      "all_data shape: (44507, 3075)\n",
      "all_data shape: (46847, 3075)\n",
      "all_data shape: (49545, 3075)\n",
      "all_data shape: (52824, 3075)\n",
      "all_data shape: (55412, 3075)\n",
      "all_data shape: (57777, 3075)\n",
      "all_data shape: (59368, 3075)\n",
      "all_data shape: (61964, 3075)\n",
      "all_data shape: (65323, 3075)\n",
      "all_data shape: (67894, 3075)\n",
      "all_data shape: (70441, 3075)\n",
      "all_data shape: (73296, 3075)\n",
      "all_data shape: (76096, 3075)\n",
      "all_data shape: (79405, 3075)\n",
      "all_data shape: (82493, 3075)\n",
      "Saved file 120 == 9830400 examples\n",
      "all_data shape: (3040, 3075)\n",
      "all_data shape: (4477, 3075)\n",
      "all_data shape: (6783, 3075)\n",
      "all_data shape: (9706, 3075)\n",
      "all_data shape: (12927, 3075)\n",
      "all_data shape: (16263, 3075)\n",
      "all_data shape: (19755, 3075)\n",
      "all_data shape: (22620, 3075)\n",
      "all_data shape: (26331, 3075)\n",
      "all_data shape: (28707, 3075)\n",
      "all_data shape: (31545, 3075)\n",
      "all_data shape: (34906, 3075)\n",
      "all_data shape: (37264, 3075)\n",
      "all_data shape: (40622, 3075)\n",
      "all_data shape: (43084, 3075)\n",
      "all_data shape: (45873, 3075)\n",
      "all_data shape: (47876, 3075)\n",
      "all_data shape: (50330, 3075)\n",
      "all_data shape: (52624, 3075)\n",
      "all_data shape: (55182, 3075)\n",
      "all_data shape: (58202, 3075)\n",
      "all_data shape: (61238, 3075)\n",
      "all_data shape: (64199, 3075)\n",
      "all_data shape: (67164, 3075)\n",
      "all_data shape: (70121, 3075)\n",
      "all_data shape: (73249, 3075)\n",
      "all_data shape: (76687, 3075)\n",
      "all_data shape: (80024, 3075)\n",
      "all_data shape: (82351, 3075)\n",
      "Saved file 121 == 9912320 examples\n",
      "all_data shape: (3252, 3075)\n",
      "all_data shape: (6626, 3075)\n",
      "all_data shape: (9462, 3075)\n",
      "all_data shape: (11896, 3075)\n",
      "all_data shape: (15292, 3075)\n",
      "all_data shape: (18652, 3075)\n",
      "all_data shape: (21227, 3075)\n",
      "all_data shape: (24225, 3075)\n",
      "all_data shape: (26844, 3075)\n",
      "all_data shape: (30041, 3075)\n",
      "all_data shape: (32273, 3075)\n",
      "all_data shape: (34988, 3075)\n",
      "all_data shape: (37636, 3075)\n",
      "all_data shape: (41519, 3075)\n",
      "all_data shape: (43625, 3075)\n",
      "all_data shape: (46034, 3075)\n",
      "all_data shape: (49052, 3075)\n",
      "all_data shape: (51838, 3075)\n",
      "all_data shape: (54720, 3075)\n",
      "all_data shape: (57327, 3075)\n",
      "all_data shape: (59852, 3075)\n",
      "all_data shape: (63320, 3075)\n",
      "all_data shape: (66345, 3075)\n",
      "all_data shape: (70129, 3075)\n",
      "all_data shape: (73288, 3075)\n",
      "all_data shape: (76015, 3075)\n",
      "all_data shape: (78526, 3075)\n",
      "all_data shape: (81497, 3075)\n",
      "all_data shape: (83855, 3075)\n",
      "Saved file 122 == 9994240 examples\n",
      "all_data shape: (4123, 3075)\n",
      "all_data shape: (7935, 3075)\n",
      "all_data shape: (10834, 3075)\n",
      "all_data shape: (13254, 3075)\n",
      "all_data shape: (16550, 3075)\n",
      "all_data shape: (19490, 3075)\n",
      "all_data shape: (22086, 3075)\n",
      "all_data shape: (25082, 3075)\n",
      "all_data shape: (28345, 3075)\n",
      "all_data shape: (30456, 3075)\n",
      "all_data shape: (32272, 3075)\n",
      "all_data shape: (35572, 3075)\n",
      "all_data shape: (38006, 3075)\n",
      "all_data shape: (41340, 3075)\n",
      "all_data shape: (43838, 3075)\n",
      "all_data shape: (46606, 3075)\n",
      "all_data shape: (48982, 3075)\n",
      "all_data shape: (52334, 3075)\n",
      "all_data shape: (54600, 3075)\n",
      "all_data shape: (57215, 3075)\n",
      "all_data shape: (60308, 3075)\n",
      "all_data shape: (63418, 3075)\n",
      "all_data shape: (65755, 3075)\n",
      "all_data shape: (68583, 3075)\n",
      "all_data shape: (71049, 3075)\n",
      "all_data shape: (74527, 3075)\n",
      "all_data shape: (77586, 3075)\n",
      "all_data shape: (80928, 3075)\n",
      "all_data shape: (83116, 3075)\n",
      "Saved file 123 == 10076160 examples\n",
      "all_data shape: (3996, 3075)\n",
      "all_data shape: (6608, 3075)\n",
      "all_data shape: (9878, 3075)\n",
      "all_data shape: (13172, 3075)\n",
      "all_data shape: (15697, 3075)\n",
      "all_data shape: (18179, 3075)\n",
      "all_data shape: (21260, 3075)\n",
      "all_data shape: (24116, 3075)\n",
      "all_data shape: (27135, 3075)\n",
      "all_data shape: (29951, 3075)\n",
      "all_data shape: (33651, 3075)\n",
      "all_data shape: (37252, 3075)\n",
      "all_data shape: (40516, 3075)\n",
      "all_data shape: (43561, 3075)\n",
      "all_data shape: (47034, 3075)\n",
      "all_data shape: (49815, 3075)\n",
      "all_data shape: (52470, 3075)\n",
      "all_data shape: (55475, 3075)\n",
      "all_data shape: (58270, 3075)\n",
      "all_data shape: (61174, 3075)\n",
      "all_data shape: (63815, 3075)\n",
      "all_data shape: (65983, 3075)\n",
      "all_data shape: (68667, 3075)\n",
      "all_data shape: (72326, 3075)\n",
      "all_data shape: (74594, 3075)\n",
      "all_data shape: (77842, 3075)\n",
      "all_data shape: (80407, 3075)\n",
      "all_data shape: (83509, 3075)\n",
      "Saved file 124 == 10158080 examples\n",
      "all_data shape: (4622, 3075)\n",
      "all_data shape: (8074, 3075)\n",
      "all_data shape: (11012, 3075)\n",
      "all_data shape: (13751, 3075)\n",
      "all_data shape: (15883, 3075)\n",
      "all_data shape: (18912, 3075)\n",
      "all_data shape: (21075, 3075)\n",
      "all_data shape: (23525, 3075)\n",
      "all_data shape: (27103, 3075)\n",
      "all_data shape: (30344, 3075)\n",
      "all_data shape: (32919, 3075)\n",
      "all_data shape: (35002, 3075)\n",
      "all_data shape: (38004, 3075)\n",
      "all_data shape: (40744, 3075)\n",
      "all_data shape: (43616, 3075)\n",
      "all_data shape: (46692, 3075)\n",
      "all_data shape: (49423, 3075)\n",
      "all_data shape: (52157, 3075)\n",
      "all_data shape: (54649, 3075)\n",
      "all_data shape: (57840, 3075)\n",
      "all_data shape: (61097, 3075)\n",
      "all_data shape: (63789, 3075)\n",
      "all_data shape: (65788, 3075)\n",
      "all_data shape: (68271, 3075)\n",
      "all_data shape: (70854, 3075)\n",
      "all_data shape: (74292, 3075)\n",
      "all_data shape: (76889, 3075)\n",
      "all_data shape: (79002, 3075)\n",
      "all_data shape: (82250, 3075)\n",
      "Saved file 125 == 10240000 examples\n",
      "all_data shape: (3150, 3075)\n",
      "all_data shape: (5956, 3075)\n",
      "all_data shape: (8702, 3075)\n",
      "all_data shape: (11185, 3075)\n",
      "all_data shape: (13632, 3075)\n",
      "all_data shape: (16938, 3075)\n",
      "all_data shape: (19719, 3075)\n",
      "all_data shape: (22197, 3075)\n",
      "all_data shape: (25228, 3075)\n",
      "all_data shape: (28541, 3075)\n",
      "all_data shape: (30946, 3075)\n",
      "all_data shape: (33632, 3075)\n",
      "all_data shape: (35835, 3075)\n",
      "all_data shape: (38835, 3075)\n",
      "all_data shape: (41225, 3075)\n",
      "all_data shape: (44064, 3075)\n",
      "all_data shape: (47101, 3075)\n",
      "all_data shape: (49005, 3075)\n",
      "all_data shape: (50676, 3075)\n",
      "all_data shape: (53322, 3075)\n",
      "all_data shape: (56454, 3075)\n",
      "all_data shape: (58864, 3075)\n",
      "all_data shape: (61250, 3075)\n",
      "all_data shape: (64168, 3075)\n",
      "all_data shape: (66604, 3075)\n",
      "all_data shape: (69339, 3075)\n",
      "all_data shape: (73162, 3075)\n",
      "all_data shape: (75993, 3075)\n",
      "all_data shape: (78797, 3075)\n",
      "all_data shape: (81806, 3075)\n",
      "all_data shape: (83710, 3075)\n",
      "Saved file 126 == 10321920 examples\n",
      "all_data shape: (3642, 3075)\n",
      "all_data shape: (7085, 3075)\n",
      "all_data shape: (10310, 3075)\n",
      "all_data shape: (12784, 3075)\n",
      "all_data shape: (15583, 3075)\n",
      "all_data shape: (19022, 3075)\n",
      "all_data shape: (22299, 3075)\n",
      "all_data shape: (25300, 3075)\n",
      "all_data shape: (27839, 3075)\n",
      "all_data shape: (30821, 3075)\n",
      "all_data shape: (34193, 3075)\n",
      "all_data shape: (37253, 3075)\n",
      "all_data shape: (40083, 3075)\n",
      "all_data shape: (43618, 3075)\n",
      "all_data shape: (47178, 3075)\n",
      "all_data shape: (50533, 3075)\n",
      "all_data shape: (53793, 3075)\n",
      "all_data shape: (56932, 3075)\n",
      "all_data shape: (59064, 3075)\n",
      "all_data shape: (61038, 3075)\n",
      "all_data shape: (63732, 3075)\n",
      "all_data shape: (66598, 3075)\n",
      "all_data shape: (70007, 3075)\n",
      "all_data shape: (72031, 3075)\n",
      "all_data shape: (74970, 3075)\n",
      "all_data shape: (78379, 3075)\n",
      "all_data shape: (81101, 3075)\n",
      "all_data shape: (84631, 3075)\n",
      "Saved file 127 == 10403840 examples\n",
      "all_data shape: (5541, 3075)\n",
      "all_data shape: (8630, 3075)\n",
      "all_data shape: (11029, 3075)\n",
      "all_data shape: (13803, 3075)\n",
      "all_data shape: (16356, 3075)\n",
      "all_data shape: (18662, 3075)\n",
      "all_data shape: (21505, 3075)\n",
      "all_data shape: (24406, 3075)\n",
      "all_data shape: (27420, 3075)\n",
      "all_data shape: (30206, 3075)\n",
      "all_data shape: (33805, 3075)\n",
      "all_data shape: (36598, 3075)\n",
      "all_data shape: (39949, 3075)\n",
      "all_data shape: (42709, 3075)\n",
      "all_data shape: (45603, 3075)\n",
      "all_data shape: (48038, 3075)\n",
      "all_data shape: (50830, 3075)\n",
      "all_data shape: (53524, 3075)\n",
      "all_data shape: (56331, 3075)\n",
      "all_data shape: (58471, 3075)\n",
      "all_data shape: (62144, 3075)\n",
      "all_data shape: (64232, 3075)\n",
      "all_data shape: (66935, 3075)\n",
      "all_data shape: (69873, 3075)\n",
      "all_data shape: (71959, 3075)\n",
      "all_data shape: (75286, 3075)\n",
      "all_data shape: (77005, 3075)\n",
      "all_data shape: (79809, 3075)\n",
      "all_data shape: (82960, 3075)\n",
      "Saved file 128 == 10485760 examples\n",
      "all_data shape: (3205, 3075)\n",
      "all_data shape: (6176, 3075)\n",
      "all_data shape: (9453, 3075)\n",
      "all_data shape: (12656, 3075)\n",
      "all_data shape: (14834, 3075)\n",
      "all_data shape: (17536, 3075)\n",
      "all_data shape: (19973, 3075)\n",
      "all_data shape: (21878, 3075)\n",
      "all_data shape: (24574, 3075)\n",
      "all_data shape: (28002, 3075)\n",
      "all_data shape: (31728, 3075)\n",
      "all_data shape: (34767, 3075)\n",
      "all_data shape: (37497, 3075)\n",
      "all_data shape: (40752, 3075)\n",
      "all_data shape: (43113, 3075)\n",
      "all_data shape: (44720, 3075)\n",
      "all_data shape: (48043, 3075)\n",
      "all_data shape: (51477, 3075)\n",
      "all_data shape: (54744, 3075)\n",
      "all_data shape: (57345, 3075)\n",
      "all_data shape: (60106, 3075)\n",
      "all_data shape: (63014, 3075)\n",
      "all_data shape: (65732, 3075)\n",
      "all_data shape: (69629, 3075)\n",
      "all_data shape: (72016, 3075)\n",
      "all_data shape: (75108, 3075)\n",
      "all_data shape: (78279, 3075)\n",
      "all_data shape: (80614, 3075)\n",
      "all_data shape: (84200, 3075)\n",
      "Saved file 129 == 10567680 examples\n",
      "all_data shape: (4947, 3075)\n",
      "all_data shape: (7542, 3075)\n",
      "all_data shape: (11500, 3075)\n",
      "all_data shape: (14648, 3075)\n",
      "all_data shape: (16848, 3075)\n",
      "all_data shape: (20299, 3075)\n",
      "all_data shape: (23065, 3075)\n",
      "all_data shape: (25998, 3075)\n",
      "all_data shape: (28155, 3075)\n",
      "all_data shape: (30856, 3075)\n",
      "all_data shape: (33438, 3075)\n",
      "all_data shape: (36777, 3075)\n",
      "all_data shape: (40005, 3075)\n",
      "all_data shape: (42445, 3075)\n",
      "all_data shape: (46012, 3075)\n",
      "all_data shape: (47646, 3075)\n",
      "all_data shape: (50205, 3075)\n",
      "all_data shape: (53548, 3075)\n",
      "Finished processing and saving all batches\n"
     ]
    }
   ],
   "source": [
    "# Load the first 30 million examples from 'The Pile' dataset \n",
    "# https://huggingface.co/docs/datasets/loading#slice-splits\n",
    "# data_len = 30_000_000\n",
    "data_len = 30_000\n",
    "# split_str = f\"train[:{data_len}]\"\n",
    "dataset = load_dataset(\"monology/pile-uncopyrighted\", split=\"train\", streaming=True)\n",
    "\n",
    "# Set up processing parameters\n",
    "os.makedirs(\"activations_data\", exist_ok=True)\n",
    "\n",
    "# Initialize accumulators and parameters\n",
    "batch_size = 8 # number of sentences in a batch\n",
    "file_size = 10*8192 # number of examples in a file\n",
    "files_saved = 0\n",
    "batch_texts = []\n",
    "activation_cache = [] # cache of activations for a batch\n",
    "all_data = np.empty((0, 3075), dtype=np.float16)  # 3072 + 3 (sent_idx, token_idx, token)\n",
    "\n",
    "# Create batches from the dataset\n",
    "print(\"Processing dataset and saving activations in batches...\")\n",
    "for i, example in enumerate(dataset):\n",
    "    batch_texts.append(example['text'])\n",
    "    \n",
    "    if (i + 1) % batch_size == 0 or i + 1 >= data_len:\n",
    "        # Process full batch or final partial batch\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding=\"max_length\",\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model(**inputs)\n",
    "        \n",
    "        # Convert activation_cache to numpy array and reshape\n",
    "        batch_activations = np.array(activation_cache)\n",
    "        \n",
    "        # Reshape batch_activations from (1, 8, 42, 3072) to (8*42, 3072)\n",
    "        batch_activations = batch_activations.reshape(batch_activations.shape[1] * batch_activations.shape[2], -1)\n",
    "\n",
    "        # Create sentence index array (sent_idx) and token index array (token_idx)\n",
    "        # sent_idx = [1 1 1 1 1; 2 2 2 2 2; 3 3 3 3 3; ...]\n",
    "        # token_idx = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; ...]\n",
    "        num_sentences, num_tokens = inputs['attention_mask'].shape # (8, 42)\n",
    "        sent_idx = np.repeat(np.arange(1, num_sentences + 1), num_tokens).reshape(-1, 1)  # Shape: (8*42, 1)\n",
    "        sent_idx = sent_idx + (i - batch_size) # offset by batch index\n",
    "        token_idx = np.tile(np.arange(1, num_tokens + 1), num_sentences).reshape(-1, 1)    # Shape: (8*42, 1)\n",
    "        token_idx = token_idx - 1 # offset by 1\n",
    "        # also save tokens id from tokenizer\n",
    "        tokens = inputs['input_ids'].cpu().numpy().reshape(-1, 1)\n",
    "        tokens = tokens - 64000 # offset by 64000 to not overflow float16\n",
    "                                # 128000 is the vocab size of Llama 3.2 3B, float16 is [-65504, 65504]\n",
    "                \n",
    "        # Stack activations, sent_idx, and token_idx\n",
    "        batch_activations = np.hstack((batch_activations, sent_idx, token_idx, tokens)).astype(np.float16)\n",
    "\n",
    "        # Remove rows where attention mask is 0\n",
    "        attention_mask = inputs['attention_mask'].cpu().numpy().reshape(-1)\n",
    "        batch_activations = batch_activations[attention_mask != 0]\n",
    "\n",
    "        # Stack to all_data\n",
    "        all_data = np.vstack((all_data, batch_activations))\n",
    "        print(f\"all_data shape: {all_data.shape}\")\n",
    "\n",
    "        # Save to file if file_size limit is reached\n",
    "        if all_data.shape[0] >= file_size:\n",
    "            data_to_save = all_data[:file_size, :]\n",
    "            np.save(f\"activations_data/activations_batch_{files_saved:04d}.npy\", data_to_save)\n",
    "            files_saved += 1\n",
    "            print(f\"Saved file {files_saved} == {file_size*files_saved} examples\")\n",
    "            all_data = all_data[file_size:, :]  # Retain any remaining rows\n",
    "            \n",
    "        # Reset for next batch\n",
    "        batch_texts = []\n",
    "        activation_cache = []\n",
    "\n",
    "    if i + 1 >= data_len:\n",
    "        break\n",
    "\n",
    "# Save any remaining data\n",
    "if all_data.shape[0] > 0:\n",
    "    np.save(f\"activations_data/activations_batch_{files_saved:04d}.npy\", all_data)\n",
    "    del all_data\n",
    "\n",
    "print(\"Finished processing and saving all batches\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 14743 / 60 /60 # 4.095277 hours for 10mil\n",
    "# 129*81920 + 53548 # total num of tokens 10621228\n",
    "0.5 *130 # 65 GB for 10mil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Unregister hook\n",
    "hook_handle.remove()\n",
    "# Shutdown the model use del and free gpu\n",
    "del model\n",
    "del tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Model deleted and GPU memory freed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3466077266.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    brek here\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "brek here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = np.load(\"activations_data/activations_batch_0000.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81920, 3075)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations.shape # of float16 = 2 bytes per element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float16')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# activations type\n",
    "activations.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.503808"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations.nbytes / 1e9 # in GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95718176.0 81920\n",
      "194965616.0 163840\n",
      "290686304.0 245760\n",
      "392115416.0 327680\n",
      "487003408.0 409600\n",
      "581323960.0 491520\n",
      "674693928.0 573440\n",
      "772323344.0 655360\n",
      "869020536.0 737280\n",
      "963158856.0 819200\n",
      "1055575520.0 901120\n",
      "1156739136.0 983040\n",
      "1248846448.0 1064960\n",
      "1342967088.0 1146880\n",
      "1436348912.0 1228800\n",
      "1533048672.0 1310720\n",
      "1626288944.0 1392640\n",
      "1722772768.0 1474560\n",
      "1813710912.0 1556480\n",
      "1909017640.0 1638400\n",
      "2003626568.0 1720320\n",
      "2098572992.0 1802240\n",
      "2193434464.0 1884160\n",
      "2289586952.0 1966080\n",
      "2383168488.0 2048000\n",
      "2474523280.0 2129920\n",
      "2566138408.0 2211840\n",
      "2666422000.0 2293760\n",
      "2761687616.0 2375680\n",
      "2858369776.0 2457600\n",
      "2953958904.0 2539520\n",
      "3049486184.0 2621440\n",
      "3143026904.0 2703360\n",
      "3236812736.0 2785280\n",
      "3330295576.0 2867200\n",
      "3424728160.0 2949120\n",
      "3524788032.0 3031040\n",
      "3620458672.0 3112960\n",
      "3717172168.0 3194880\n",
      "3812773792.0 3276800\n",
      "3908757920.0 3358720\n",
      "4002240016.0 3440640\n",
      "4097820840.0 3522560\n",
      "4192781464.0 3604480\n",
      "4287644648.0 3686400\n",
      "4382614632.0 3768320\n",
      "4475721568.0 3850240\n",
      "4577440336.0 3932160\n",
      "4675331136.0 4014080\n",
      "4765898320.0 4096000\n",
      "4863665576.0 4177920\n",
      "4957104592.0 4259840\n",
      "5049170320.0 4341760\n",
      "5147751408.0 4423680\n",
      "5243633592.0 4505600\n",
      "5336354320.0 4587520\n",
      "5431618312.0 4669440\n",
      "5524468872.0 4751360\n",
      "5618286696.0 4833280\n",
      "5712525800.0 4915200\n",
      "5810390288.0 4997120\n",
      "5906374736.0 5079040\n",
      "5998354656.0 5160960\n",
      "6096562272.0 5242880\n",
      "6192150384.0 5324800\n",
      "6287430640.0 5406720\n",
      "6384483088.0 5488640\n",
      "6480131072.0 5570560\n",
      "6572035936.0 5652480\n",
      "6666906264.0 5734400\n",
      "6763918296.0 5816320\n",
      "6857768800.0 5898240\n",
      "6954926616.0 5980160\n",
      "7049048544.0 6062080\n",
      "7148343760.0 6144000\n",
      "7241017600.0 6225920\n",
      "7335186448.0 6307840\n",
      "7436213752.0 6389760\n",
      "7530422344.0 6471680\n",
      "7631582648.0 6553600\n",
      "7725058888.0 6635520\n",
      "7819265432.0 6717440\n",
      "7918126512.0 6799360\n",
      "8012313392.0 6881280\n",
      "8105205352.0 6963200\n",
      "8200465640.0 7045120\n",
      "8296770952.0 7127040\n",
      "8395313616.0 7208960\n",
      "8490572216.0 7290880\n",
      "8585097832.0 7372800\n",
      "8678859256.0 7454720\n",
      "8773362000.0 7536640\n",
      "8869826352.0 7618560\n",
      "8964018960.0 7700480\n",
      "9056067584.0 7782400\n",
      "9155078544.0 7864320\n",
      "9253014000.0 7946240\n",
      "9346798408.0 8028160\n",
      "9440652992.0 8110080\n",
      "9532972440.0 8192000\n",
      "9632674808.0 8273920\n",
      "9730531120.0 8355840\n",
      "9825936896.0 8437760\n",
      "9919411432.0 8519680\n",
      "10017961072.0 8601600\n",
      "10114609064.0 8683520\n",
      "10211366272.0 8765440\n",
      "10303308800.0 8847360\n",
      "10401450816.0 8929280\n",
      "10495215864.0 9011200\n",
      "10591697960.0 9093120\n",
      "10687031464.0 9175040\n",
      "10781919144.0 9256960\n",
      "10874403224.0 9338880\n",
      "10964151736.0 9420800\n",
      "11059846168.0 9502720\n",
      "11156864632.0 9584640\n",
      "11254319504.0 9666560\n",
      "11352088688.0 9748480\n",
      "11450297280.0 9830400\n",
      "11545934656.0 9912320\n",
      "11639885504.0 9994240\n",
      "11735806856.0 10076160\n",
      "11829347048.0 10158080\n",
      "11926111944.0 10240000\n",
      "12025057032.0 10321920\n",
      "12118052440.0 10403840\n",
      "12215134728.0 10485760\n",
      "12309676992.0 10567680\n",
      "12371277652.0 10621228\n",
      "Scale factor: 34.128712991170886\n"
     ]
    }
   ],
   "source": [
    "# Calculate scale factor\n",
    "import glob\n",
    "\n",
    "data_dir = \"activations_data\"\n",
    " # Load all batches\n",
    "all_batches = sorted(glob.glob(f\"{data_dir}/activations_batch_*.npy\"))\n",
    "\n",
    "# Load and concatenate batches\n",
    "total_norm_squared = 0\n",
    "num_samples = 0\n",
    "for f in all_batches:\n",
    "    activations = np.load(f)\n",
    "    activations = activations[:, :-3] # remove last 3 columns (sent_idx, token_idx, and token)\n",
    "    activations = activations.astype(np.float32)\n",
    "    # max_a, min_a = np.finfo(activations.dtype).max, np.finfo(activations.dtype).min\n",
    "    # activations = np.nan_to_num(activations, nan=0, posinf=max_a, neginf=min_a) # replace inf with max/min float value\n",
    "    total_norm_squared += np.sum(np.linalg.norm(activations, axis=1)**2)\n",
    "    num_samples += activations.shape[0]\n",
    "    print(total_norm_squared, num_samples)\n",
    "mean_squared_norm = total_norm_squared / num_samples\n",
    "scale_factor = np.sqrt(mean_squared_norm)\n",
    "print(f\"Scale factor: {scale_factor}\")\n",
    "\n",
    "# Save scale factor - empty txt with factor in filename\n",
    "with open(f\"{data_dir}/scale_factor_{scale_factor:.4f}.txt\", \"w\") as f:\n",
    "    f.write(f\"{scale_factor}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale_factor = 34.12206415510119 # at 1.6mil tokens\n",
    "scale_factor = 34.128712991170886 # at 10.6mil tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
    "        # Decoder\n",
    "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = torch.relu(self.encoder(x))\n",
    "        # encoded = torch.nn.LeakyReLU(0.01)(self.encoder(x))\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded, encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class ActivationDataset(Dataset):\n",
    "    def __init__(self, data_dir, batch_size, f_type, test_fraction=0.01, scale_factor=1.0, seed=42):\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = seed\n",
    "\n",
    "        if f_type in [\"train\", \"test\", \"all\"]:\n",
    "            self.f_type = f_type\n",
    "        else:\n",
    "            raise ValueError(\"f_type must be 'train' or 'test' or 'all'\")\n",
    "        \n",
    "        if not 0 <= test_fraction <= 1:\n",
    "            raise ValueError(\"test_fraction must be between 0 and 1\")\n",
    "        self.test_fraction = test_fraction\n",
    "\n",
    "        self.scale_factor = scale_factor\n",
    "        self.file_names = sorted([f for f in os.listdir(data_dir) if f.endswith('.npy') and f.startswith('activations_batch')])\n",
    "        \n",
    "        split_idx = int(len(self.file_names) * (1 - test_fraction))\n",
    "        if f_type == \"train\":\n",
    "            self.file_names = self.file_names[:split_idx]\n",
    "        elif f_type == \"test\":\n",
    "            self.file_names = self.file_names[split_idx:]\n",
    "        else: # all\n",
    "            pass\n",
    "\n",
    "        print(f\"Loaded {len(self.file_names)} batches for {f_type} set\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.data_dir, self.file_names[idx])\n",
    "        activations = np.load(file_path)\n",
    "        if self.f_type == \"all\":\n",
    "            sent_idx = activations[:, -3]\n",
    "            token_idx = activations[:, -2] \n",
    "            token = activations[:, -1]\n",
    "        # remove last 3 columns (sent_idx, token_idx, and token)\n",
    "        activations = activations[:, :-3]\n",
    "        # normalize activations\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        activations = torch.tensor(activations, dtype=torch.float32, device=device)\n",
    "        # print(\"Activation Range Before Normalization:\", torch.min(activations).item(), torch.max(activations).item())\n",
    "        activations = activations / self.scale_factor * np.sqrt(activations.shape[1])\n",
    "        # print(\"Activation Range After Normalization:\", torch.min(activations).item(), torch.max(activations).item())\n",
    "\n",
    "        if self.f_type == \"train\":\n",
    "            # Set seed for reproducibility\n",
    "            np.random.seed(self.seed)\n",
    "            # random subsample 8192 examples\n",
    "            indices = torch.randperm(activations.shape[0], device=activations.device)[:self.batch_size]\n",
    "            activations = activations[indices]\n",
    "        \n",
    "        if self.f_type == \"all\":\n",
    "            return activations, sent_idx, token_idx, token\n",
    "        else:\n",
    "            return activations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 128 batches for train set\n",
      "Training Parameters:\n",
      "Data Directory: activations_data\n",
      "Batch Size: 2048\n",
      "Test Fraction: 0.01\n",
      "Scale Factor: 34.128712991170886\n",
      "Seed: 42\n",
      "Optimizer: Adam\n",
      "Learning Rate: 0.0001\n",
      "L1 Lambda: 0.01\n",
      "Number of Epochs: 10\n",
      "SAE Input Dimension: 3072\n",
      "SAE Hidden Dimension: 20000\n",
      "-----------------------------------\n",
      "Batch number:  0\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.2262667864561081\n",
      "MSE Loss: 1.0828, L1 Loss: 0.01*42.6108\n",
      "Batch number:  1\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.22530965507030487\n",
      "MSE Loss: 0.9027, L1 Loss: 0.01*38.5126\n",
      "Batch number:  2\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.22439603507518768\n",
      "MSE Loss: 0.9701, L1 Loss: 0.01*36.5797\n",
      "Batch number:  3\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.2233605831861496\n",
      "MSE Loss: 0.6493, L1 Loss: 0.01*31.7898\n",
      "Batch number:  4\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.2222653031349182\n",
      "MSE Loss: 0.6370, L1 Loss: 0.01*29.1621\n",
      "Batch number:  5\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.2211592197418213\n",
      "MSE Loss: 1.1017, L1 Loss: 0.01*30.5729\n",
      "Batch number:  6\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.22007030248641968\n",
      "MSE Loss: 0.6645, L1 Loss: 0.01*25.3384\n",
      "Batch number:  7\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.21898788213729858\n",
      "MSE Loss: 0.7357, L1 Loss: 0.01*24.4972\n",
      "Batch number:  8\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.2179117649793625\n",
      "MSE Loss: 0.7154, L1 Loss: 0.01*22.5021\n",
      "Batch number:  9\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.21684770286083221\n",
      "MSE Loss: 0.3988, L1 Loss: 0.01*18.0896\n",
      "Batch number:  10\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.21580524742603302\n",
      "MSE Loss: 0.5601, L1 Loss: 0.01*18.4752\n",
      "Batch number:  11\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.21479061245918274\n",
      "MSE Loss: 0.5187, L1 Loss: 0.01*17.1578\n",
      "Batch number:  12\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.213809534907341\n",
      "MSE Loss: 0.5563, L1 Loss: 0.01*16.9494\n",
      "Batch number:  13\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.21286429464817047\n",
      "MSE Loss: 0.3874, L1 Loss: 0.01*13.9449\n",
      "Batch number:  14\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.2119564414024353\n",
      "MSE Loss: 0.6387, L1 Loss: 0.01*16.8184\n",
      "Batch number:  15\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.21108710765838623\n",
      "MSE Loss: 0.3513, L1 Loss: 0.01*11.9969\n",
      "Batch number:  16\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.21025751531124115\n",
      "MSE Loss: 0.4362, L1 Loss: 0.01*13.2600\n",
      "Batch number:  17\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20946776866912842\n",
      "MSE Loss: 0.4046, L1 Loss: 0.01*12.5894\n",
      "Batch number:  18\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20871718227863312\n",
      "MSE Loss: 0.4225, L1 Loss: 0.01*13.1780\n",
      "Batch number:  19\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20800304412841797\n",
      "MSE Loss: 0.4710, L1 Loss: 0.01*14.8784\n",
      "Batch number:  20\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20732387900352478\n",
      "MSE Loss: 0.4239, L1 Loss: 0.01*14.4271\n",
      "Batch number:  21\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20667928457260132\n",
      "MSE Loss: 0.2944, L1 Loss: 0.01*10.8349\n",
      "Batch number:  22\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.2060691863298416\n",
      "MSE Loss: 0.2468, L1 Loss: 0.01*9.3921\n",
      "Batch number:  23\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20549249649047852\n",
      "MSE Loss: 0.2545, L1 Loss: 0.01*10.3793\n",
      "Batch number:  24\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20494696497917175\n",
      "MSE Loss: 0.1538, L1 Loss: 0.01*5.5292\n",
      "Batch number:  25\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20443366467952728\n",
      "MSE Loss: 0.1953, L1 Loss: 0.01*8.7031\n",
      "Batch number:  26\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.2039492130279541\n",
      "MSE Loss: 0.1946, L1 Loss: 0.01*9.6564\n",
      "Batch number:  27\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20349173247814178\n",
      "MSE Loss: 0.1460, L1 Loss: 0.01*6.0002\n",
      "Batch number:  28\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20306116342544556\n",
      "MSE Loss: 0.1579, L1 Loss: 0.01*8.2793\n",
      "Batch number:  29\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20265448093414307\n",
      "MSE Loss: 0.1499, L1 Loss: 0.01*8.2390\n",
      "Batch number:  30\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.2022695690393448\n",
      "MSE Loss: 0.1561, L1 Loss: 0.01*11.7659\n",
      "Batch number:  31\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20190449059009552\n",
      "MSE Loss: 0.1226, L1 Loss: 0.01*4.2722\n",
      "Batch number:  32\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20156167447566986\n",
      "MSE Loss: 0.1419, L1 Loss: 0.01*11.6842\n",
      "Batch number:  33\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.2012360841035843\n",
      "MSE Loss: 0.1335, L1 Loss: 0.01*12.9972\n",
      "Batch number:  34\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.2009255290031433\n",
      "MSE Loss: 0.1213, L1 Loss: 0.01*6.5020\n",
      "Batch number:  35\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20063278079032898\n",
      "MSE Loss: 0.1211, L1 Loss: 0.01*5.1960\n",
      "Batch number:  36\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.2003563642501831\n",
      "MSE Loss: 0.1315, L1 Loss: 0.01*10.3521\n",
      "Batch number:  37\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20009295642375946\n",
      "MSE Loss: 0.1211, L1 Loss: 0.01*11.6542\n",
      "Batch number:  38\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.19984149932861328\n",
      "MSE Loss: 0.1236, L1 Loss: 0.01*10.2979\n",
      "Batch number:  39\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.19960170984268188\n",
      "MSE Loss: 0.1205, L1 Loss: 0.01*3.6904\n",
      "Batch number:  40\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.19937552511692047\n",
      "MSE Loss: 0.1305, L1 Loss: 0.01*10.2899\n",
      "Batch number:  41\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.19915850460529327\n",
      "MSE Loss: 0.1210, L1 Loss: 0.01*6.2431\n",
      "Batch number:  42\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.1989528238773346\n",
      "MSE Loss: 0.1334, L1 Loss: 0.01*16.8504\n",
      "Batch number:  43\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.19875194132328033\n",
      "MSE Loss: 0.1264, L1 Loss: 0.01*10.1438\n",
      "Batch number:  44\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.19855883717536926\n",
      "MSE Loss: 0.1260, L1 Loss: 0.01*7.4413\n",
      "Batch number:  45\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.19837404787540436\n",
      "MSE Loss: 0.1284, L1 Loss: 0.01*8.7404\n",
      "Batch number:  46\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.19819621741771698\n",
      "MSE Loss: 0.1209, L1 Loss: 0.01*3.4167\n",
      "Batch number:  47\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.19802795350551605\n",
      "MSE Loss: 0.1298, L1 Loss: 0.01*12.5049\n",
      "Batch number:  48\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.19786378741264343\n",
      "MSE Loss: 0.1248, L1 Loss: 0.01*11.1165\n",
      "Batch number:  49\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.19770421087741852\n",
      "MSE Loss: 0.1222, L1 Loss: 0.01*7.1673\n",
      "Batch number:  50\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.1975507289171219\n",
      "MSE Loss: 0.1249, L1 Loss: 0.01*8.3806\n",
      "Batch number:  51\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.1974019706249237\n",
      "MSE Loss: 0.1239, L1 Loss: 0.01*9.5890\n",
      "Batch number:  52\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.19725710153579712\n",
      "MSE Loss: 0.1214, L1 Loss: 0.01*5.6887\n",
      "Batch number:  53\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.19711844623088837\n",
      "MSE Loss: 0.1247, L1 Loss: 0.01*10.7241\n",
      "Batch number:  54\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.1969819962978363\n",
      "MSE Loss: 0.1201, L1 Loss: 0.01*10.5881\n",
      "Batch number:  55\n",
      "Active Features: 99.99%\n",
      "Decoder Weight Norm (Mean): 0.196848064661026\n",
      "MSE Loss: 0.1190, L1 Loss: 0.01*4.3040\n",
      "Batch number:  56\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.19672003388404846\n",
      "MSE Loss: 0.1220, L1 Loss: 0.01*7.9189\n",
      "Batch number:  57\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.19659531116485596\n",
      "MSE Loss: 0.1226, L1 Loss: 0.01*12.7446\n",
      "Batch number:  58\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.19647108018398285\n",
      "MSE Loss: 0.1193, L1 Loss: 0.01*7.8040\n",
      "Batch number:  59\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.1963503062725067\n",
      "MSE Loss: 0.1242, L1 Loss: 0.01*8.9289\n",
      "Batch number:  60\n",
      "Active Features: 99.98%\n",
      "Decoder Weight Norm (Mean): 0.19623194634914398\n",
      "MSE Loss: 0.1215, L1 Loss: 0.01*10.0496\n",
      "Batch number:  61\n",
      "Active Features: 99.99%\n",
      "Decoder Weight Norm (Mean): 0.19611525535583496\n",
      "MSE Loss: 0.1205, L1 Loss: 0.01*11.1454\n",
      "Batch number:  62\n",
      "Active Features: 99.99%\n",
      "Decoder Weight Norm (Mean): 0.19599924981594086\n",
      "MSE Loss: 0.1215, L1 Loss: 0.01*8.7065\n",
      "Batch number:  63\n",
      "Active Features: 99.99%\n",
      "Decoder Weight Norm (Mean): 0.1958852857351303\n",
      "MSE Loss: 0.1227, L1 Loss: 0.01*10.9765\n",
      "Batch number:  64\n",
      "Active Features: 99.98%\n",
      "Decoder Weight Norm (Mean): 0.19577188789844513\n",
      "MSE Loss: 0.1207, L1 Loss: 0.01*8.6023\n",
      "Batch number:  65\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.19566035270690918\n",
      "MSE Loss: 0.1206, L1 Loss: 0.01*3.9365\n",
      "Batch number:  66\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.19555334746837616\n",
      "MSE Loss: 0.1212, L1 Loss: 0.01*5.0333\n",
      "Batch number:  67\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.19544976949691772\n",
      "MSE Loss: 0.1245, L1 Loss: 0.01*9.5328\n",
      "Batch number:  68\n",
      "Active Features: 99.99%\n",
      "Decoder Weight Norm (Mean): 0.1953468769788742\n",
      "MSE Loss: 0.1203, L1 Loss: 0.01*7.2250\n",
      "Batch number:  69\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.1952461302280426\n",
      "MSE Loss: 0.1244, L1 Loss: 0.01*11.6646\n",
      "Batch number:  70\n",
      "Active Features: 99.98%\n",
      "Decoder Weight Norm (Mean): 0.19514451920986176\n",
      "MSE Loss: 0.1193, L1 Loss: 0.01*4.9334\n",
      "Batch number:  71\n",
      "Active Features: 99.99%\n",
      "Decoder Weight Norm (Mean): 0.19504596292972565\n",
      "MSE Loss: 0.1211, L1 Loss: 0.01*4.9129\n",
      "Batch number:  72\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.19495020806789398\n",
      "MSE Loss: 0.1255, L1 Loss: 0.01*12.6225\n",
      "Batch number:  73\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.19485273957252502\n",
      "MSE Loss: 0.1189, L1 Loss: 0.01*3.7572\n",
      "Batch number:  74\n",
      "Active Features: 99.98%\n",
      "Decoder Weight Norm (Mean): 0.19475887715816498\n",
      "MSE Loss: 0.1233, L1 Loss: 0.01*10.3203\n",
      "Batch number:  75\n",
      "Active Features: 99.99%\n",
      "Decoder Weight Norm (Mean): 0.19466447830200195\n",
      "MSE Loss: 0.1207, L1 Loss: 0.01*8.1233\n",
      "Batch number:  76\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.1945706009864807\n",
      "MSE Loss: 0.1192, L1 Loss: 0.01*4.8121\n",
      "Batch number:  77\n",
      "Active Features: 99.99%\n",
      "Decoder Weight Norm (Mean): 0.19447928667068481\n",
      "MSE Loss: 0.1217, L1 Loss: 0.01*8.0708\n",
      "Batch number:  78\n",
      "Active Features: 99.98%\n",
      "Decoder Weight Norm (Mean): 0.19438818097114563\n",
      "MSE Loss: 0.1209, L1 Loss: 0.01*8.0454\n",
      "Batch number:  79\n",
      "Active Features: 99.98%\n",
      "Decoder Weight Norm (Mean): 0.1942974179983139\n",
      "MSE Loss: 0.1215, L1 Loss: 0.01*10.1875\n",
      "Batch number:  80\n",
      "Active Features: 99.97%\n",
      "Decoder Weight Norm (Mean): 0.19420583546161652\n",
      "MSE Loss: 0.1200, L1 Loss: 0.01*5.8042\n",
      "Batch number:  81\n",
      "Active Features: 99.98%\n",
      "Decoder Weight Norm (Mean): 0.1941162794828415\n",
      "MSE Loss: 0.1211, L1 Loss: 0.01*7.9671\n",
      "Batch number:  82\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.19402694702148438\n",
      "MSE Loss: 0.1195, L1 Loss: 0.01*6.8748\n",
      "Batch number:  83\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.1939384937286377\n",
      "MSE Loss: 0.1189, L1 Loss: 0.01*6.8645\n",
      "Batch number:  84\n",
      "Active Features: 99.97%\n",
      "Decoder Weight Norm (Mean): 0.19385077059268951\n",
      "MSE Loss: 0.1184, L1 Loss: 0.01*7.8866\n",
      "Batch number:  85\n",
      "Active Features: 99.95%\n",
      "Decoder Weight Norm (Mean): 0.19376333057880402\n",
      "MSE Loss: 0.1200, L1 Loss: 0.01*11.0636\n",
      "Batch number:  86\n",
      "Active Features: 99.97%\n",
      "Decoder Weight Norm (Mean): 0.1936742514371872\n",
      "MSE Loss: 0.1178, L1 Loss: 0.01*2.4962\n",
      "Batch number:  87\n",
      "Active Features: 99.99%\n",
      "Decoder Weight Norm (Mean): 0.19358913600444794\n",
      "MSE Loss: 0.1207, L1 Loss: 0.01*10.9891\n",
      "Batch number:  88\n",
      "Active Features: 99.94%\n",
      "Decoder Weight Norm (Mean): 0.19350214302539825\n",
      "MSE Loss: 0.1189, L1 Loss: 0.01*5.6551\n",
      "Batch number:  89\n",
      "Active Features: 99.97%\n",
      "Decoder Weight Norm (Mean): 0.19341698288917542\n",
      "MSE Loss: 0.1198, L1 Loss: 0.01*11.9835\n",
      "Batch number:  90\n",
      "Active Features: 99.98%\n",
      "Decoder Weight Norm (Mean): 0.19332939386367798\n",
      "MSE Loss: 0.1175, L1 Loss: 0.01*8.7926\n",
      "Batch number:  91\n",
      "Active Features: 99.98%\n",
      "Decoder Weight Norm (Mean): 0.19324128329753876\n",
      "MSE Loss: 0.1181, L1 Loss: 0.01*4.5610\n",
      "Batch number:  92\n",
      "Active Features: 99.98%\n",
      "Decoder Weight Norm (Mean): 0.193155437707901\n",
      "MSE Loss: 0.1198, L1 Loss: 0.01*3.4883\n",
      "Batch number:  93\n",
      "Active Features: 99.95%\n",
      "Decoder Weight Norm (Mean): 0.19307248294353485\n",
      "MSE Loss: 0.1180, L1 Loss: 0.01*2.4304\n",
      "Batch number:  94\n",
      "Active Features: 99.97%\n",
      "Decoder Weight Norm (Mean): 0.19299286603927612\n",
      "MSE Loss: 0.1204, L1 Loss: 0.01*6.5714\n",
      "Batch number:  95\n",
      "Active Features: 99.94%\n",
      "Decoder Weight Norm (Mean): 0.19291356205940247\n",
      "MSE Loss: 0.1198, L1 Loss: 0.01*7.5598\n",
      "Batch number:  96\n",
      "Active Features: 99.98%\n",
      "Decoder Weight Norm (Mean): 0.19283412396907806\n",
      "MSE Loss: 0.1190, L1 Loss: 0.01*5.5056\n",
      "Batch number:  97\n",
      "Active Features: 99.96%\n",
      "Decoder Weight Norm (Mean): 0.1927555650472641\n",
      "MSE Loss: 0.1185, L1 Loss: 0.01*4.4454\n",
      "Batch number:  98\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.19267858564853668\n",
      "MSE Loss: 0.1201, L1 Loss: 0.01*9.5370\n",
      "Batch number:  99\n",
      "Active Features: 99.98%\n",
      "Decoder Weight Norm (Mean): 0.19259949028491974\n",
      "MSE Loss: 0.1184, L1 Loss: 0.01*2.3813\n",
      "Batch number:  100\n",
      "Active Features: 99.95%\n",
      "Decoder Weight Norm (Mean): 0.19252344965934753\n",
      "MSE Loss: 0.1194, L1 Loss: 0.01*2.3762\n",
      "Batch number:  101\n",
      "Active Features: 99.96%\n",
      "Decoder Weight Norm (Mean): 0.19245006144046783\n",
      "MSE Loss: 0.1199, L1 Loss: 0.01*5.3773\n",
      "Batch number:  102\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.1923770159482956\n",
      "MSE Loss: 0.1167, L1 Loss: 0.01*8.3737\n",
      "Batch number:  103\n",
      "Active Features: 99.94%\n",
      "Decoder Weight Norm (Mean): 0.19230234622955322\n",
      "MSE Loss: 0.1188, L1 Loss: 0.01*3.3347\n",
      "Batch number:  104\n",
      "Active Features: 99.94%\n",
      "Decoder Weight Norm (Mean): 0.19222989678382874\n",
      "MSE Loss: 0.1203, L1 Loss: 0.01*8.3024\n",
      "Batch number:  105\n",
      "Active Features: 99.98%\n",
      "Decoder Weight Norm (Mean): 0.1921558976173401\n",
      "MSE Loss: 0.1198, L1 Loss: 0.01*14.2269\n",
      "Batch number:  106\n",
      "Active Features: 99.94%\n",
      "Decoder Weight Norm (Mean): 0.19207677245140076\n",
      "MSE Loss: 0.1199, L1 Loss: 0.01*7.2304\n",
      "Batch number:  107\n",
      "Active Features: 99.98%\n",
      "Decoder Weight Norm (Mean): 0.19199778139591217\n",
      "MSE Loss: 0.1231, L1 Loss: 0.01*11.1529\n",
      "Batch number:  108\n",
      "Active Features: 99.92%\n",
      "Decoder Weight Norm (Mean): 0.19191572070121765\n",
      "MSE Loss: 0.1184, L1 Loss: 0.01*3.2540\n",
      "Batch number:  109\n",
      "Active Features: 99.98%\n",
      "Decoder Weight Norm (Mean): 0.19183695316314697\n",
      "MSE Loss: 0.1184, L1 Loss: 0.01*2.2868\n",
      "Batch number:  110\n",
      "Active Features: 99.97%\n",
      "Decoder Weight Norm (Mean): 0.19176127016544342\n",
      "MSE Loss: 0.1253, L1 Loss: 0.01*6.1640\n",
      "Batch number:  111\n",
      "Active Features: 99.94%\n",
      "Decoder Weight Norm (Mean): 0.19168533384799957\n",
      "MSE Loss: 0.1215, L1 Loss: 0.01*5.1574\n",
      "Batch number:  112\n",
      "Active Features: 99.99%\n",
      "Decoder Weight Norm (Mean): 0.19161055982112885\n",
      "MSE Loss: 0.1183, L1 Loss: 0.01*5.1372\n",
      "Batch number:  113\n",
      "Active Features: 99.92%\n",
      "Decoder Weight Norm (Mean): 0.1915368288755417\n",
      "MSE Loss: 0.1221, L1 Loss: 0.01*7.9800\n",
      "Batch number:  114\n",
      "Active Features: 99.96%\n",
      "Decoder Weight Norm (Mean): 0.19146201014518738\n",
      "MSE Loss: 0.1196, L1 Loss: 0.01*7.9471\n",
      "Batch number:  115\n",
      "Active Features: 99.94%\n",
      "Decoder Weight Norm (Mean): 0.19138622283935547\n",
      "MSE Loss: 0.1208, L1 Loss: 0.01*8.8764\n",
      "Batch number:  116\n",
      "Active Features: 99.95%\n",
      "Decoder Weight Norm (Mean): 0.19130882620811462\n",
      "MSE Loss: 0.1196, L1 Loss: 0.01*5.9863\n",
      "Batch number:  117\n",
      "Active Features: 99.95%\n",
      "Decoder Weight Norm (Mean): 0.19123217463493347\n",
      "MSE Loss: 0.1196, L1 Loss: 0.01*5.0392\n",
      "Batch number:  118\n",
      "Active Features: 99.94%\n",
      "Decoder Weight Norm (Mean): 0.19115649163722992\n",
      "MSE Loss: 0.1208, L1 Loss: 0.01*9.6987\n",
      "Batch number:  119\n",
      "Active Features: 99.92%\n",
      "Decoder Weight Norm (Mean): 0.1910787969827652\n",
      "MSE Loss: 0.1190, L1 Loss: 0.01*7.7848\n",
      "Batch number:  120\n",
      "Active Features: 99.94%\n",
      "Decoder Weight Norm (Mean): 0.19100084900856018\n",
      "MSE Loss: 0.1194, L1 Loss: 0.01*5.9008\n",
      "Batch number:  121\n",
      "Active Features: 99.94%\n",
      "Decoder Weight Norm (Mean): 0.1909235268831253\n",
      "MSE Loss: 0.1227, L1 Loss: 0.01*8.6538\n",
      "Batch number:  122\n",
      "Active Features: 99.97%\n",
      "Decoder Weight Norm (Mean): 0.19084493815898895\n",
      "MSE Loss: 0.1175, L1 Loss: 0.01*8.6195\n",
      "Batch number:  123\n",
      "Active Features: 99.96%\n",
      "Decoder Weight Norm (Mean): 0.19076547026634216\n",
      "MSE Loss: 0.1213, L1 Loss: 0.01*8.5977\n",
      "Batch number:  124\n",
      "Active Features: 99.94%\n",
      "Decoder Weight Norm (Mean): 0.19068481028079987\n",
      "MSE Loss: 0.1197, L1 Loss: 0.01*5.8021\n",
      "Batch number:  125\n",
      "Active Features: 99.92%\n",
      "Decoder Weight Norm (Mean): 0.1906053125858307\n",
      "MSE Loss: 0.1187, L1 Loss: 0.01*5.7900\n",
      "Batch number:  126\n",
      "Active Features: 99.94%\n",
      "Decoder Weight Norm (Mean): 0.1905267983675003\n",
      "MSE Loss: 0.1183, L1 Loss: 0.01*9.3791\n",
      "Batch number:  127\n",
      "Active Features: 99.94%\n",
      "Decoder Weight Norm (Mean): 0.19044692814350128\n",
      "MSE Loss: 0.1179, L1 Loss: 0.01*3.9563\n",
      "Epoch [1/10], Loss: 39.7447 ----------------------------\n",
      "Batch number:  0\n",
      "Active Features: 99.91%\n",
      "Decoder Weight Norm (Mean): 0.19036909937858582\n",
      "MSE Loss: 0.1192, L1 Loss: 0.01*5.7253\n",
      "Batch number:  1\n",
      "Active Features: 99.92%\n",
      "Decoder Weight Norm (Mean): 0.19029201567173004\n",
      "MSE Loss: 0.1184, L1 Loss: 0.01*5.7152\n",
      "Batch number:  2\n",
      "Active Features: 99.94%\n",
      "Decoder Weight Norm (Mean): 0.190215602517128\n",
      "MSE Loss: 0.1181, L1 Loss: 0.01*3.9046\n",
      "Batch number:  3\n",
      "Active Features: 99.91%\n",
      "Decoder Weight Norm (Mean): 0.19014127552509308\n",
      "MSE Loss: 0.1180, L1 Loss: 0.01*10.9883\n",
      "Batch number:  4\n",
      "Active Features: 99.89%\n",
      "Decoder Weight Norm (Mean): 0.19006386399269104\n",
      "MSE Loss: 0.1176, L1 Loss: 0.01*7.4099\n",
      "Batch number:  5\n",
      "Active Features: 99.91%\n",
      "Decoder Weight Norm (Mean): 0.1899861842393875\n",
      "MSE Loss: 0.1194, L1 Loss: 0.01*6.5084\n",
      "Batch number:  6\n",
      "Active Features: 99.92%\n",
      "Decoder Weight Norm (Mean): 0.18990881741046906\n",
      "MSE Loss: 0.1160, L1 Loss: 0.01*2.9664\n",
      "Batch number:  7\n",
      "Active Features: 99.93%\n",
      "Decoder Weight Norm (Mean): 0.18983440101146698\n",
      "MSE Loss: 0.1190, L1 Loss: 0.01*5.5991\n",
      "Batch number:  8\n",
      "Active Features: 99.89%\n",
      "Decoder Weight Norm (Mean): 0.18976044654846191\n",
      "MSE Loss: 0.1169, L1 Loss: 0.01*2.9633\n",
      "Batch number:  9\n",
      "Active Features: 99.94%\n",
      "Decoder Weight Norm (Mean): 0.18968898057937622\n",
      "MSE Loss: 0.1169, L1 Loss: 0.01*5.5571\n",
      "Batch number:  10\n",
      "Active Features: 99.97%\n",
      "Decoder Weight Norm (Mean): 0.18961787223815918\n",
      "MSE Loss: 0.1164, L1 Loss: 0.01*5.5477\n",
      "Batch number:  11\n",
      "Active Features: 99.91%\n",
      "Decoder Weight Norm (Mean): 0.18954679369926453\n",
      "MSE Loss: 0.1176, L1 Loss: 0.01*9.8251\n",
      "Batch number:  12\n",
      "Active Features: 99.89%\n",
      "Decoder Weight Norm (Mean): 0.18947282433509827\n",
      "MSE Loss: 0.1169, L1 Loss: 0.01*4.6555\n",
      "Batch number:  13\n",
      "Active Features: 99.94%\n",
      "Decoder Weight Norm (Mean): 0.18939997255802155\n",
      "MSE Loss: 0.1153, L1 Loss: 0.01*5.4935\n",
      "Batch number:  14\n",
      "Active Features: 99.93%\n",
      "Decoder Weight Norm (Mean): 0.18932737410068512\n",
      "MSE Loss: 0.1157, L1 Loss: 0.01*3.7780\n",
      "Batch number:  15\n",
      "Active Features: 99.91%\n",
      "Decoder Weight Norm (Mean): 0.18925635516643524\n",
      "MSE Loss: 0.1161, L1 Loss: 0.01*2.0901\n",
      "Batch number:  16\n",
      "Active Features: 99.91%\n",
      "Decoder Weight Norm (Mean): 0.18918780982494354\n",
      "MSE Loss: 0.1167, L1 Loss: 0.01*4.6243\n",
      "Batch number:  17\n",
      "Active Features: 99.85%\n",
      "Decoder Weight Norm (Mean): 0.18911953270435333\n",
      "MSE Loss: 0.1167, L1 Loss: 0.01*4.6186\n",
      "Batch number:  18\n",
      "Active Features: 99.89%\n",
      "Decoder Weight Norm (Mean): 0.18905140459537506\n",
      "MSE Loss: 0.1151, L1 Loss: 0.01*5.4409\n",
      "Batch number:  19\n",
      "Active Features: 99.94%\n",
      "Decoder Weight Norm (Mean): 0.1889827847480774\n",
      "MSE Loss: 0.1152, L1 Loss: 0.01*4.5941\n",
      "Batch number:  20\n",
      "Active Features: 99.91%\n",
      "Decoder Weight Norm (Mean): 0.1889142543077469\n",
      "MSE Loss: 0.1151, L1 Loss: 0.01*3.7650\n",
      "Batch number:  21\n",
      "Active Features: 99.84%\n",
      "Decoder Weight Norm (Mean): 0.18884636461734772\n",
      "MSE Loss: 0.1150, L1 Loss: 0.01*3.7587\n",
      "Batch number:  22\n",
      "Active Features: 99.91%\n",
      "Decoder Weight Norm (Mean): 0.18877898156642914\n",
      "MSE Loss: 0.1138, L1 Loss: 0.01*8.7011\n",
      "Batch number:  23\n",
      "Active Features: 99.84%\n",
      "Decoder Weight Norm (Mean): 0.18870842456817627\n",
      "MSE Loss: 0.1150, L1 Loss: 0.01*5.4168\n",
      "Batch number:  24\n",
      "Active Features: 99.83%\n",
      "Decoder Weight Norm (Mean): 0.188636913895607\n",
      "MSE Loss: 0.1144, L1 Loss: 0.01*2.9441\n",
      "Batch number:  25\n",
      "Active Features: 99.83%\n",
      "Decoder Weight Norm (Mean): 0.18856647610664368\n",
      "MSE Loss: 0.1135, L1 Loss: 0.01*7.0177\n",
      "Batch number:  26\n",
      "Active Features: 99.91%\n",
      "Decoder Weight Norm (Mean): 0.18849405646324158\n",
      "MSE Loss: 0.1124, L1 Loss: 0.01*4.5626\n",
      "Batch number:  27\n",
      "Active Features: 99.84%\n",
      "Decoder Weight Norm (Mean): 0.1884218156337738\n",
      "MSE Loss: 0.1122, L1 Loss: 0.01*3.7491\n",
      "Batch number:  28\n",
      "Active Features: 99.86%\n",
      "Decoder Weight Norm (Mean): 0.18835017085075378\n",
      "MSE Loss: 0.1122, L1 Loss: 0.01*5.3707\n",
      "Batch number:  29\n",
      "Active Features: 99.91%\n",
      "Decoder Weight Norm (Mean): 0.18827766180038452\n",
      "MSE Loss: 0.1115, L1 Loss: 0.01*6.9716\n",
      "Batch number:  30\n",
      "Active Features: 99.83%\n",
      "Decoder Weight Norm (Mean): 0.18820300698280334\n",
      "MSE Loss: 0.1113, L1 Loss: 0.01*6.1473\n",
      "Batch number:  31\n",
      "Active Features: 99.81%\n",
      "Decoder Weight Norm (Mean): 0.1881272941827774\n",
      "MSE Loss: 0.1099, L1 Loss: 0.01*5.3381\n",
      "Batch number:  32\n",
      "Active Features: 99.83%\n",
      "Decoder Weight Norm (Mean): 0.1880510151386261\n",
      "MSE Loss: 0.1123, L1 Loss: 0.01*3.7573\n",
      "Batch number:  33\n",
      "Active Features: 99.83%\n",
      "Decoder Weight Norm (Mean): 0.1879754364490509\n",
      "MSE Loss: 0.1106, L1 Loss: 0.01*6.1366\n",
      "Batch number:  34\n",
      "Active Features: 99.72%\n",
      "Decoder Weight Norm (Mean): 0.18789838254451752\n",
      "MSE Loss: 0.1108, L1 Loss: 0.01*9.2679\n",
      "Batch number:  35\n",
      "Active Features: 99.87%\n",
      "Decoder Weight Norm (Mean): 0.18781813979148865\n",
      "MSE Loss: 0.1096, L1 Loss: 0.01*4.5210\n",
      "Batch number:  36\n",
      "Active Features: 99.78%\n",
      "Decoder Weight Norm (Mean): 0.1877385377883911\n",
      "MSE Loss: 0.1100, L1 Loss: 0.01*5.3203\n",
      "Batch number:  37\n",
      "Active Features: 99.75%\n",
      "Decoder Weight Norm (Mean): 0.18765893578529358\n",
      "MSE Loss: 0.1088, L1 Loss: 0.01*6.0678\n",
      "Batch number:  38\n",
      "Active Features: 99.85%\n",
      "Decoder Weight Norm (Mean): 0.18757882714271545\n",
      "MSE Loss: 0.1090, L1 Loss: 0.01*4.5010\n",
      "Batch number:  39\n",
      "Active Features: 99.77%\n",
      "Decoder Weight Norm (Mean): 0.18749958276748657\n",
      "MSE Loss: 0.1091, L1 Loss: 0.01*8.3835\n",
      "Batch number:  40\n",
      "Active Features: 99.86%\n",
      "Decoder Weight Norm (Mean): 0.18741804361343384\n",
      "MSE Loss: 0.1081, L1 Loss: 0.01*5.2740\n",
      "Batch number:  41\n",
      "Active Features: 99.76%\n",
      "Decoder Weight Norm (Mean): 0.18733671307563782\n",
      "MSE Loss: 0.1089, L1 Loss: 0.01*6.8046\n",
      "Batch number:  42\n",
      "Active Features: 99.80%\n",
      "Decoder Weight Norm (Mean): 0.18725474178791046\n",
      "MSE Loss: 0.1072, L1 Loss: 0.01*8.3066\n",
      "Batch number:  43\n",
      "Active Features: 99.83%\n",
      "Decoder Weight Norm (Mean): 0.1871708631515503\n",
      "MSE Loss: 0.1078, L1 Loss: 0.01*6.7679\n",
      "Batch number:  44\n",
      "Active Features: 99.87%\n",
      "Decoder Weight Norm (Mean): 0.18708652257919312\n",
      "MSE Loss: 0.1080, L1 Loss: 0.01*5.9935\n",
      "Batch number:  45\n",
      "Active Features: 99.76%\n",
      "Decoder Weight Norm (Mean): 0.18700218200683594\n",
      "MSE Loss: 0.1082, L1 Loss: 0.01*5.2151\n",
      "Batch number:  46\n",
      "Active Features: 99.80%\n",
      "Decoder Weight Norm (Mean): 0.18691876530647278\n",
      "MSE Loss: 0.1076, L1 Loss: 0.01*6.7223\n",
      "Batch number:  47\n",
      "Active Features: 99.75%\n",
      "Decoder Weight Norm (Mean): 0.18683497607707977\n",
      "MSE Loss: 0.1060, L1 Loss: 0.01*5.1897\n",
      "Batch number:  48\n",
      "Active Features: 99.77%\n",
      "Decoder Weight Norm (Mean): 0.1867523193359375\n",
      "MSE Loss: 0.1082, L1 Loss: 0.01*5.1831\n",
      "Batch number:  49\n",
      "Active Features: 99.78%\n",
      "Decoder Weight Norm (Mean): 0.1866704821586609\n",
      "MSE Loss: 0.1067, L1 Loss: 0.01*4.4064\n",
      "Batch number:  50\n",
      "Active Features: 99.74%\n",
      "Decoder Weight Norm (Mean): 0.18659013509750366\n",
      "MSE Loss: 0.1064, L1 Loss: 0.01*5.8840\n",
      "Batch number:  51\n",
      "Active Features: 99.74%\n",
      "Decoder Weight Norm (Mean): 0.1865098774433136\n",
      "MSE Loss: 0.1062, L1 Loss: 0.01*4.4048\n",
      "Batch number:  52\n",
      "Active Features: 99.69%\n",
      "Decoder Weight Norm (Mean): 0.18643073737621307\n",
      "MSE Loss: 0.1070, L1 Loss: 0.01*7.3466\n",
      "Batch number:  53\n",
      "Active Features: 99.78%\n",
      "Decoder Weight Norm (Mean): 0.1863507479429245\n",
      "MSE Loss: 0.1056, L1 Loss: 0.01*5.8561\n",
      "Batch number:  54\n",
      "Active Features: 99.67%\n",
      "Decoder Weight Norm (Mean): 0.1862708330154419\n",
      "MSE Loss: 0.1045, L1 Loss: 0.01*5.0882\n",
      "Batch number:  55\n",
      "Active Features: 99.80%\n",
      "Decoder Weight Norm (Mean): 0.18619196116924286\n",
      "MSE Loss: 0.1056, L1 Loss: 0.01*4.3710\n",
      "Batch number:  56\n",
      "Active Features: 99.78%\n",
      "Decoder Weight Norm (Mean): 0.18611447513103485\n",
      "MSE Loss: 0.1049, L1 Loss: 0.01*7.2595\n",
      "Batch number:  57\n",
      "Active Features: 99.72%\n",
      "Decoder Weight Norm (Mean): 0.18603605031967163\n",
      "MSE Loss: 0.1057, L1 Loss: 0.01*6.5175\n",
      "Batch number:  58\n",
      "Active Features: 99.66%\n",
      "Decoder Weight Norm (Mean): 0.18595702946186066\n",
      "MSE Loss: 0.1048, L1 Loss: 0.01*4.3308\n",
      "Batch number:  59\n",
      "Active Features: 99.67%\n",
      "Decoder Weight Norm (Mean): 0.18587975203990936\n",
      "MSE Loss: 0.1048, L1 Loss: 0.01*7.1990\n",
      "Batch number:  60\n",
      "Active Features: 99.67%\n",
      "Decoder Weight Norm (Mean): 0.1858016550540924\n",
      "MSE Loss: 0.1054, L1 Loss: 0.01*3.6078\n",
      "Batch number:  61\n",
      "Active Features: 99.59%\n",
      "Decoder Weight Norm (Mean): 0.18572621047496796\n",
      "MSE Loss: 0.1039, L1 Loss: 0.01*2.1625\n",
      "Batch number:  62\n",
      "Active Features: 99.76%\n",
      "Decoder Weight Norm (Mean): 0.18565431237220764\n",
      "MSE Loss: 0.1042, L1 Loss: 0.01*8.5332\n",
      "Batch number:  63\n",
      "Active Features: 99.77%\n",
      "Decoder Weight Norm (Mean): 0.1855800449848175\n",
      "MSE Loss: 0.1047, L1 Loss: 0.01*6.4007\n",
      "Batch number:  64\n",
      "Active Features: 99.76%\n",
      "Decoder Weight Norm (Mean): 0.1855054348707199\n",
      "MSE Loss: 0.1034, L1 Loss: 0.01*4.2636\n",
      "Batch number:  65\n",
      "Active Features: 99.64%\n",
      "Decoder Weight Norm (Mean): 0.18543222546577454\n",
      "MSE Loss: 0.1040, L1 Loss: 0.01*6.3439\n",
      "Batch number:  66\n",
      "Active Features: 99.61%\n",
      "Decoder Weight Norm (Mean): 0.18535885214805603\n",
      "MSE Loss: 0.1040, L1 Loss: 0.01*4.9411\n",
      "Batch number:  67\n",
      "Active Features: 99.63%\n",
      "Decoder Weight Norm (Mean): 0.1852867603302002\n",
      "MSE Loss: 0.1034, L1 Loss: 0.01*4.2408\n",
      "Batch number:  68\n",
      "Active Features: 99.61%\n",
      "Decoder Weight Norm (Mean): 0.18521623313426971\n",
      "MSE Loss: 0.1034, L1 Loss: 0.01*7.6764\n",
      "Batch number:  69\n",
      "Active Features: 99.66%\n",
      "Decoder Weight Norm (Mean): 0.1851440966129303\n",
      "MSE Loss: 0.1033, L1 Loss: 0.01*3.5166\n",
      "Batch number:  70\n",
      "Active Features: 99.62%\n",
      "Decoder Weight Norm (Mean): 0.18507441878318787\n",
      "MSE Loss: 0.1033, L1 Loss: 0.01*4.1779\n",
      "Batch number:  71\n",
      "Active Features: 99.65%\n",
      "Decoder Weight Norm (Mean): 0.1850060075521469\n",
      "MSE Loss: 0.1031, L1 Loss: 0.01*2.8283\n",
      "Batch number:  72\n",
      "Active Features: 99.61%\n",
      "Decoder Weight Norm (Mean): 0.1849404126405716\n",
      "MSE Loss: 0.1035, L1 Loss: 0.01*4.8637\n",
      "Batch number:  73\n",
      "Active Features: 99.52%\n",
      "Decoder Weight Norm (Mean): 0.18487519025802612\n",
      "MSE Loss: 0.1034, L1 Loss: 0.01*2.7930\n",
      "Batch number:  74\n",
      "Active Features: 99.48%\n",
      "Decoder Weight Norm (Mean): 0.18481236696243286\n",
      "MSE Loss: 0.1023, L1 Loss: 0.01*3.4870\n",
      "Batch number:  75\n",
      "Active Features: 99.62%\n",
      "Decoder Weight Norm (Mean): 0.18475133180618286\n",
      "MSE Loss: 0.1028, L1 Loss: 0.01*6.1717\n",
      "Batch number:  76\n",
      "Active Features: 99.58%\n",
      "Decoder Weight Norm (Mean): 0.1846892237663269\n",
      "MSE Loss: 0.1018, L1 Loss: 0.01*5.4937\n",
      "Batch number:  77\n",
      "Active Features: 99.67%\n",
      "Decoder Weight Norm (Mean): 0.1846267133951187\n",
      "MSE Loss: 0.1020, L1 Loss: 0.01*3.4495\n",
      "Batch number:  78\n",
      "Active Features: 99.47%\n",
      "Decoder Weight Norm (Mean): 0.18456558883190155\n",
      "MSE Loss: 0.1026, L1 Loss: 0.01*2.7916\n",
      "Batch number:  79\n",
      "Active Features: 99.54%\n",
      "Decoder Weight Norm (Mean): 0.18450669944286346\n",
      "MSE Loss: 0.1023, L1 Loss: 0.01*2.1287\n",
      "Batch number:  80\n",
      "Active Features: 99.39%\n",
      "Decoder Weight Norm (Mean): 0.18445035815238953\n",
      "MSE Loss: 0.1027, L1 Loss: 0.01*6.0903\n",
      "Batch number:  81\n",
      "Active Features: 99.51%\n",
      "Decoder Weight Norm (Mean): 0.1843927800655365\n",
      "MSE Loss: 0.1024, L1 Loss: 0.01*3.4496\n",
      "Batch number:  82\n",
      "Active Features: 99.48%\n",
      "Decoder Weight Norm (Mean): 0.18433643877506256\n",
      "MSE Loss: 0.1019, L1 Loss: 0.01*6.0564\n",
      "Batch number:  83\n",
      "Active Features: 99.62%\n",
      "Decoder Weight Norm (Mean): 0.18427881598472595\n",
      "MSE Loss: 0.1020, L1 Loss: 0.01*1.4510\n",
      "Batch number:  84\n",
      "Active Features: 99.48%\n",
      "Decoder Weight Norm (Mean): 0.18422411382198334\n",
      "MSE Loss: 0.0996, L1 Loss: 0.01*2.0834\n",
      "Batch number:  85\n",
      "Active Features: 99.37%\n",
      "Decoder Weight Norm (Mean): 0.18417184054851532\n",
      "MSE Loss: 0.1010, L1 Loss: 0.01*3.4111\n",
      "Batch number:  86\n",
      "Active Features: 99.33%\n",
      "Decoder Weight Norm (Mean): 0.18412065505981445\n",
      "MSE Loss: 0.1011, L1 Loss: 0.01*4.0595\n",
      "Batch number:  87\n",
      "Active Features: 99.48%\n",
      "Decoder Weight Norm (Mean): 0.18406984210014343\n",
      "MSE Loss: 0.1009, L1 Loss: 0.01*2.7552\n",
      "Batch number:  88\n",
      "Active Features: 99.36%\n",
      "Decoder Weight Norm (Mean): 0.1840209811925888\n",
      "MSE Loss: 0.1014, L1 Loss: 0.01*2.7508\n",
      "Batch number:  89\n",
      "Active Features: 99.33%\n",
      "Decoder Weight Norm (Mean): 0.18397383391857147\n",
      "MSE Loss: 0.1009, L1 Loss: 0.01*5.9748\n",
      "Batch number:  90\n",
      "Active Features: 99.45%\n",
      "Decoder Weight Norm (Mean): 0.1839246302843094\n",
      "MSE Loss: 0.1006, L1 Loss: 0.01*7.2505\n",
      "Batch number:  91\n",
      "Active Features: 99.30%\n",
      "Decoder Weight Norm (Mean): 0.1838722825050354\n",
      "MSE Loss: 0.1004, L1 Loss: 0.01*2.0965\n",
      "Batch number:  92\n",
      "Active Features: 99.14%\n",
      "Decoder Weight Norm (Mean): 0.18382228910923004\n",
      "MSE Loss: 0.1012, L1 Loss: 0.01*4.0380\n",
      "Batch number:  93\n",
      "Active Features: 99.37%\n",
      "Decoder Weight Norm (Mean): 0.1837727427482605\n",
      "MSE Loss: 0.1007, L1 Loss: 0.01*4.0114\n",
      "Batch number:  94\n",
      "Active Features: 99.31%\n",
      "Decoder Weight Norm (Mean): 0.18372350931167603\n",
      "MSE Loss: 0.1009, L1 Loss: 0.01*4.6322\n",
      "Batch number:  95\n",
      "Active Features: 99.11%\n",
      "Decoder Weight Norm (Mean): 0.1836739033460617\n",
      "MSE Loss: 0.1016, L1 Loss: 0.01*4.0102\n",
      "Batch number:  96\n",
      "Active Features: 99.22%\n",
      "Decoder Weight Norm (Mean): 0.18362471461296082\n",
      "MSE Loss: 0.1010, L1 Loss: 0.01*4.0060\n",
      "Batch number:  97\n",
      "Active Features: 99.27%\n",
      "Decoder Weight Norm (Mean): 0.18357573449611664\n",
      "MSE Loss: 0.0997, L1 Loss: 0.01*4.6119\n",
      "Batch number:  98\n",
      "Active Features: 99.31%\n",
      "Decoder Weight Norm (Mean): 0.18352638185024261\n",
      "MSE Loss: 0.1008, L1 Loss: 0.01*6.4846\n",
      "Batch number:  99\n",
      "Active Features: 99.01%\n",
      "Decoder Weight Norm (Mean): 0.18347476422786713\n",
      "MSE Loss: 0.1000, L1 Loss: 0.01*0.8168\n",
      "Batch number:  100\n",
      "Active Features: 99.30%\n",
      "Decoder Weight Norm (Mean): 0.18342693150043488\n",
      "MSE Loss: 0.1018, L1 Loss: 0.01*8.3322\n",
      "Batch number:  101\n",
      "Active Features: 99.07%\n",
      "Decoder Weight Norm (Mean): 0.1833747774362564\n",
      "MSE Loss: 0.1002, L1 Loss: 0.01*4.5746\n",
      "Batch number:  102\n",
      "Active Features: 99.06%\n",
      "Decoder Weight Norm (Mean): 0.1833226978778839\n",
      "MSE Loss: 0.0983, L1 Loss: 0.01*5.7651\n",
      "Batch number:  103\n",
      "Active Features: 98.98%\n",
      "Decoder Weight Norm (Mean): 0.18327029049396515\n",
      "MSE Loss: 0.0999, L1 Loss: 0.01*4.5491\n",
      "Batch number:  104\n",
      "Active Features: 99.08%\n",
      "Decoder Weight Norm (Mean): 0.18321798741817474\n",
      "MSE Loss: 0.0998, L1 Loss: 0.01*3.9271\n",
      "Batch number:  105\n",
      "Active Features: 99.11%\n",
      "Decoder Weight Norm (Mean): 0.1831664741039276\n",
      "MSE Loss: 0.0990, L1 Loss: 0.01*5.1227\n",
      "Batch number:  106\n",
      "Active Features: 99.29%\n",
      "Decoder Weight Norm (Mean): 0.1831141859292984\n",
      "MSE Loss: 0.0997, L1 Loss: 0.01*5.1496\n",
      "Batch number:  107\n",
      "Active Features: 99.20%\n",
      "Decoder Weight Norm (Mean): 0.18306158483028412\n",
      "MSE Loss: 0.0992, L1 Loss: 0.01*3.2796\n",
      "Batch number:  108\n",
      "Active Features: 98.77%\n",
      "Decoder Weight Norm (Mean): 0.18301032483577728\n",
      "MSE Loss: 0.0988, L1 Loss: 0.01*6.9364\n",
      "Batch number:  109\n",
      "Active Features: 99.11%\n",
      "Decoder Weight Norm (Mean): 0.18295690417289734\n",
      "MSE Loss: 0.0984, L1 Loss: 0.01*4.4881\n",
      "Batch number:  110\n",
      "Active Features: 99.21%\n",
      "Decoder Weight Norm (Mean): 0.18290385603904724\n",
      "MSE Loss: 0.1003, L1 Loss: 0.01*3.8747\n",
      "Batch number:  111\n",
      "Active Features: 98.86%\n",
      "Decoder Weight Norm (Mean): 0.1828516125679016\n",
      "MSE Loss: 0.0985, L1 Loss: 0.01*5.6720\n",
      "Batch number:  112\n",
      "Active Features: 99.34%\n",
      "Decoder Weight Norm (Mean): 0.18279841542243958\n",
      "MSE Loss: 0.0986, L1 Loss: 0.01*4.4615\n",
      "Batch number:  113\n",
      "Active Features: 98.88%\n",
      "Decoder Weight Norm (Mean): 0.18274538218975067\n",
      "MSE Loss: 0.1002, L1 Loss: 0.01*3.8488\n",
      "Batch number:  114\n",
      "Active Features: 99.03%\n",
      "Decoder Weight Norm (Mean): 0.1826932579278946\n",
      "MSE Loss: 0.0990, L1 Loss: 0.01*4.4003\n",
      "Batch number:  115\n",
      "Active Features: 99.01%\n",
      "Decoder Weight Norm (Mean): 0.18264129757881165\n",
      "MSE Loss: 0.0992, L1 Loss: 0.01*5.0367\n",
      "Batch number:  116\n",
      "Active Features: 99.00%\n",
      "Decoder Weight Norm (Mean): 0.18258891999721527\n",
      "MSE Loss: 0.0982, L1 Loss: 0.01*3.8068\n",
      "Batch number:  117\n",
      "Active Features: 99.03%\n",
      "Decoder Weight Norm (Mean): 0.1825374960899353\n",
      "MSE Loss: 0.0980, L1 Loss: 0.01*3.8177\n",
      "Batch number:  118\n",
      "Active Features: 98.88%\n",
      "Decoder Weight Norm (Mean): 0.18248702585697174\n",
      "MSE Loss: 0.0982, L1 Loss: 0.01*4.3976\n",
      "Batch number:  119\n",
      "Active Features: 98.87%\n",
      "Decoder Weight Norm (Mean): 0.18243683874607086\n",
      "MSE Loss: 0.0985, L1 Loss: 0.01*3.2173\n",
      "Batch number:  120\n",
      "Active Features: 98.70%\n",
      "Decoder Weight Norm (Mean): 0.18238882720470428\n",
      "MSE Loss: 0.0982, L1 Loss: 0.01*6.1218\n",
      "Batch number:  121\n",
      "Active Features: 98.58%\n",
      "Decoder Weight Norm (Mean): 0.18233899772167206\n",
      "MSE Loss: 0.0981, L1 Loss: 0.01*2.6280\n",
      "Batch number:  122\n",
      "Active Features: 99.07%\n",
      "Decoder Weight Norm (Mean): 0.18229132890701294\n",
      "MSE Loss: 0.0976, L1 Loss: 0.01*3.1752\n",
      "Batch number:  123\n",
      "Active Features: 99.01%\n",
      "Decoder Weight Norm (Mean): 0.18224476277828217\n",
      "MSE Loss: 0.0979, L1 Loss: 0.01*3.7826\n",
      "Batch number:  124\n",
      "Active Features: 98.58%\n",
      "Decoder Weight Norm (Mean): 0.18219858407974243\n",
      "MSE Loss: 0.0980, L1 Loss: 0.01*4.9062\n",
      "Batch number:  125\n",
      "Active Features: 98.66%\n",
      "Decoder Weight Norm (Mean): 0.1821519434452057\n",
      "MSE Loss: 0.0973, L1 Loss: 0.01*3.7642\n",
      "Batch number:  126\n",
      "Active Features: 98.76%\n",
      "Decoder Weight Norm (Mean): 0.1821058988571167\n",
      "MSE Loss: 0.0971, L1 Loss: 0.01*4.3027\n",
      "Batch number:  127\n",
      "Active Features: 98.97%\n",
      "Decoder Weight Norm (Mean): 0.18206004798412323\n",
      "MSE Loss: 0.0977, L1 Loss: 0.01*3.1687\n",
      "Epoch [2/10], Loss: 19.9056 ----------------------------\n",
      "Batch number:  0\n",
      "Active Features: 98.79%\n",
      "Decoder Weight Norm (Mean): 0.1820150762796402\n",
      "MSE Loss: 0.0969, L1 Loss: 0.01*3.7415\n",
      "Batch number:  1\n",
      "Active Features: 98.89%\n",
      "Decoder Weight Norm (Mean): 0.181970477104187\n",
      "MSE Loss: 0.0968, L1 Loss: 0.01*3.1787\n",
      "Batch number:  2\n",
      "Active Features: 98.70%\n",
      "Decoder Weight Norm (Mean): 0.1819281429052353\n",
      "MSE Loss: 0.0981, L1 Loss: 0.01*4.2998\n",
      "Batch number:  3\n",
      "Active Features: 98.39%\n",
      "Decoder Weight Norm (Mean): 0.18188558518886566\n",
      "MSE Loss: 0.0968, L1 Loss: 0.01*2.0256\n",
      "Batch number:  4\n",
      "Active Features: 98.00%\n",
      "Decoder Weight Norm (Mean): 0.1818452924489975\n",
      "MSE Loss: 0.0960, L1 Loss: 0.01*3.6908\n",
      "Batch number:  5\n",
      "Active Features: 98.37%\n",
      "Decoder Weight Norm (Mean): 0.1818053126335144\n",
      "MSE Loss: 0.0977, L1 Loss: 0.01*3.1699\n",
      "Batch number:  6\n",
      "Active Features: 98.95%\n",
      "Decoder Weight Norm (Mean): 0.18176622688770294\n",
      "MSE Loss: 0.0964, L1 Loss: 0.01*4.7997\n",
      "Batch number:  7\n",
      "Active Features: 98.52%\n",
      "Decoder Weight Norm (Mean): 0.18172672390937805\n",
      "MSE Loss: 0.0975, L1 Loss: 0.01*2.0516\n",
      "Batch number:  8\n",
      "Active Features: 98.55%\n",
      "Decoder Weight Norm (Mean): 0.18168926239013672\n",
      "MSE Loss: 0.0961, L1 Loss: 0.01*2.5672\n",
      "Batch number:  9\n",
      "Active Features: 97.91%\n",
      "Decoder Weight Norm (Mean): 0.1816529482603073\n",
      "MSE Loss: 0.0958, L1 Loss: 0.01*3.6812\n",
      "Batch number:  10\n",
      "Active Features: 98.73%\n",
      "Decoder Weight Norm (Mean): 0.18161644041538239\n",
      "MSE Loss: 0.0969, L1 Loss: 0.01*3.1390\n",
      "Batch number:  11\n",
      "Active Features: 98.39%\n",
      "Decoder Weight Norm (Mean): 0.1815803200006485\n",
      "MSE Loss: 0.0970, L1 Loss: 0.01*5.8978\n",
      "Batch number:  12\n",
      "Active Features: 98.39%\n",
      "Decoder Weight Norm (Mean): 0.18154162168502808\n",
      "MSE Loss: 0.0965, L1 Loss: 0.01*3.6815\n",
      "Batch number:  13\n",
      "Active Features: 98.50%\n",
      "Decoder Weight Norm (Mean): 0.1815030723810196\n",
      "MSE Loss: 0.0954, L1 Loss: 0.01*4.2032\n",
      "Batch number:  14\n",
      "Active Features: 98.32%\n",
      "Decoder Weight Norm (Mean): 0.1814640611410141\n",
      "MSE Loss: 0.0959, L1 Loss: 0.01*4.2069\n",
      "Batch number:  15\n",
      "Active Features: 98.58%\n",
      "Decoder Weight Norm (Mean): 0.18142545223236084\n",
      "MSE Loss: 0.0957, L1 Loss: 0.01*4.7349\n",
      "Batch number:  16\n",
      "Active Features: 98.17%\n",
      "Decoder Weight Norm (Mean): 0.18138588964939117\n",
      "MSE Loss: 0.0964, L1 Loss: 0.01*3.1138\n",
      "Batch number:  17\n",
      "Active Features: 98.21%\n",
      "Decoder Weight Norm (Mean): 0.18134725093841553\n",
      "MSE Loss: 0.0971, L1 Loss: 0.01*4.1959\n",
      "Batch number:  18\n",
      "Active Features: 98.16%\n",
      "Decoder Weight Norm (Mean): 0.18130797147750854\n",
      "MSE Loss: 0.0963, L1 Loss: 0.01*1.4618\n",
      "Batch number:  19\n",
      "Active Features: 98.36%\n",
      "Decoder Weight Norm (Mean): 0.18127143383026123\n",
      "MSE Loss: 0.0968, L1 Loss: 0.01*4.1633\n",
      "Batch number:  20\n",
      "Active Features: 98.53%\n",
      "Decoder Weight Norm (Mean): 0.18123410642147064\n",
      "MSE Loss: 0.0959, L1 Loss: 0.01*2.5596\n",
      "Batch number:  21\n",
      "Active Features: 98.53%\n",
      "Decoder Weight Norm (Mean): 0.18119806051254272\n",
      "MSE Loss: 0.0963, L1 Loss: 0.01*3.0867\n",
      "Batch number:  22\n",
      "Active Features: 98.57%\n",
      "Decoder Weight Norm (Mean): 0.1811635047197342\n",
      "MSE Loss: 0.0957, L1 Loss: 0.01*3.6140\n",
      "Batch number:  23\n",
      "Active Features: 98.02%\n",
      "Decoder Weight Norm (Mean): 0.18112891912460327\n",
      "MSE Loss: 0.0966, L1 Loss: 0.01*2.0249\n",
      "Batch number:  24\n",
      "Active Features: 97.92%\n",
      "Decoder Weight Norm (Mean): 0.18109600245952606\n",
      "MSE Loss: 0.0959, L1 Loss: 0.01*5.2259\n",
      "Batch number:  25\n",
      "Active Features: 97.76%\n",
      "Decoder Weight Norm (Mean): 0.18106094002723694\n",
      "MSE Loss: 0.0952, L1 Loss: 0.01*4.6533\n",
      "Batch number:  26\n",
      "Active Features: 97.78%\n",
      "Decoder Weight Norm (Mean): 0.18102455139160156\n",
      "MSE Loss: 0.0951, L1 Loss: 0.01*3.0654\n",
      "Batch number:  27\n",
      "Active Features: 97.97%\n",
      "Decoder Weight Norm (Mean): 0.18098893761634827\n",
      "MSE Loss: 0.0946, L1 Loss: 0.01*4.1130\n",
      "Batch number:  28\n",
      "Active Features: 97.87%\n",
      "Decoder Weight Norm (Mean): 0.18095269799232483\n",
      "MSE Loss: 0.0951, L1 Loss: 0.01*3.0442\n",
      "Batch number:  29\n",
      "Active Features: 98.50%\n",
      "Decoder Weight Norm (Mean): 0.18091727793216705\n",
      "MSE Loss: 0.0951, L1 Loss: 0.01*2.0020\n",
      "Batch number:  30\n",
      "Active Features: 97.76%\n",
      "Decoder Weight Norm (Mean): 0.1808834820985794\n",
      "MSE Loss: 0.0947, L1 Loss: 0.01*4.6066\n",
      "Batch number:  31\n",
      "Active Features: 97.89%\n",
      "Decoder Weight Norm (Mean): 0.1808481216430664\n",
      "MSE Loss: 0.0933, L1 Loss: 0.01*4.5912\n",
      "Batch number:  32\n",
      "Active Features: 98.10%\n",
      "Decoder Weight Norm (Mean): 0.18081139028072357\n",
      "MSE Loss: 0.0955, L1 Loss: 0.01*5.6484\n",
      "Batch number:  33\n",
      "Active Features: 98.03%\n",
      "Decoder Weight Norm (Mean): 0.18077214062213898\n",
      "MSE Loss: 0.0946, L1 Loss: 0.01*4.0693\n",
      "Batch number:  34\n",
      "Active Features: 97.23%\n",
      "Decoder Weight Norm (Mean): 0.18073254823684692\n",
      "MSE Loss: 0.0945, L1 Loss: 0.01*0.9616\n",
      "Batch number:  35\n",
      "Active Features: 97.72%\n",
      "Decoder Weight Norm (Mean): 0.1806965172290802\n",
      "MSE Loss: 0.0940, L1 Loss: 0.01*3.5208\n",
      "Batch number:  36\n",
      "Active Features: 97.47%\n",
      "Decoder Weight Norm (Mean): 0.1806606650352478\n",
      "MSE Loss: 0.0944, L1 Loss: 0.01*6.1017\n",
      "Batch number:  37\n",
      "Active Features: 97.90%\n",
      "Decoder Weight Norm (Mean): 0.18062181770801544\n",
      "MSE Loss: 0.0948, L1 Loss: 0.01*3.0161\n",
      "Batch number:  38\n",
      "Active Features: 97.13%\n",
      "Decoder Weight Norm (Mean): 0.18058475852012634\n",
      "MSE Loss: 0.0944, L1 Loss: 0.01*3.0088\n",
      "Batch number:  39\n",
      "Active Features: 97.72%\n",
      "Decoder Weight Norm (Mean): 0.18054860830307007\n",
      "MSE Loss: 0.0942, L1 Loss: 0.01*3.0303\n",
      "Batch number:  40\n",
      "Active Features: 97.69%\n",
      "Decoder Weight Norm (Mean): 0.18051327764987946\n",
      "MSE Loss: 0.0939, L1 Loss: 0.01*4.0268\n",
      "Batch number:  41\n",
      "Active Features: 97.62%\n",
      "Decoder Weight Norm (Mean): 0.18047724664211273\n",
      "MSE Loss: 0.0947, L1 Loss: 0.01*6.0342\n",
      "Batch number:  42\n",
      "Active Features: 97.64%\n",
      "Decoder Weight Norm (Mean): 0.18043829500675201\n",
      "MSE Loss: 0.0940, L1 Loss: 0.01*3.9993\n",
      "Batch number:  43\n",
      "Active Features: 97.36%\n",
      "Decoder Weight Norm (Mean): 0.18039943277835846\n",
      "MSE Loss: 0.0937, L1 Loss: 0.01*3.0000\n",
      "Batch number:  44\n",
      "Active Features: 97.80%\n",
      "Decoder Weight Norm (Mean): 0.18036188185214996\n",
      "MSE Loss: 0.0934, L1 Loss: 0.01*1.9863\n",
      "Batch number:  45\n",
      "Active Features: 97.58%\n",
      "Decoder Weight Norm (Mean): 0.18032638728618622\n",
      "MSE Loss: 0.0940, L1 Loss: 0.01*5.4699\n",
      "Batch number:  46\n",
      "Active Features: 97.27%\n",
      "Decoder Weight Norm (Mean): 0.18028859794139862\n",
      "MSE Loss: 0.0941, L1 Loss: 0.01*4.9555\n",
      "Batch number:  47\n",
      "Active Features: 97.06%\n",
      "Decoder Weight Norm (Mean): 0.1802494078874588\n",
      "MSE Loss: 0.0927, L1 Loss: 0.01*2.9805\n",
      "Batch number:  48\n",
      "Active Features: 96.97%\n",
      "Decoder Weight Norm (Mean): 0.1802116185426712\n",
      "MSE Loss: 0.0935, L1 Loss: 0.01*4.9527\n",
      "Batch number:  49\n",
      "Active Features: 97.29%\n",
      "Decoder Weight Norm (Mean): 0.1801726073026657\n",
      "MSE Loss: 0.0936, L1 Loss: 0.01*3.9330\n",
      "Batch number:  50\n",
      "Active Features: 97.15%\n",
      "Decoder Weight Norm (Mean): 0.1801336109638214\n",
      "MSE Loss: 0.0931, L1 Loss: 0.01*3.4378\n",
      "Batch number:  51\n",
      "Active Features: 97.54%\n",
      "Decoder Weight Norm (Mean): 0.18009518086910248\n",
      "MSE Loss: 0.0935, L1 Loss: 0.01*1.4904\n",
      "Batch number:  52\n",
      "Active Features: 97.11%\n",
      "Decoder Weight Norm (Mean): 0.18005964159965515\n",
      "MSE Loss: 0.0927, L1 Loss: 0.01*5.3846\n",
      "Batch number:  53\n",
      "Active Features: 97.59%\n",
      "Decoder Weight Norm (Mean): 0.18002201616764069\n",
      "MSE Loss: 0.0930, L1 Loss: 0.01*4.4175\n",
      "Batch number:  54\n",
      "Active Features: 97.10%\n",
      "Decoder Weight Norm (Mean): 0.17998380959033966\n",
      "MSE Loss: 0.0925, L1 Loss: 0.01*3.4257\n",
      "Batch number:  55\n",
      "Active Features: 97.44%\n",
      "Decoder Weight Norm (Mean): 0.179946631193161\n",
      "MSE Loss: 0.0927, L1 Loss: 0.01*4.8706\n",
      "Batch number:  56\n",
      "Active Features: 96.86%\n",
      "Decoder Weight Norm (Mean): 0.17990805208683014\n",
      "MSE Loss: 0.0924, L1 Loss: 0.01*2.9577\n",
      "Batch number:  57\n",
      "Active Features: 97.41%\n",
      "Decoder Weight Norm (Mean): 0.17987076938152313\n",
      "MSE Loss: 0.0939, L1 Loss: 0.01*4.3829\n",
      "Batch number:  58\n",
      "Active Features: 96.91%\n",
      "Decoder Weight Norm (Mean): 0.17983266711235046\n",
      "MSE Loss: 0.0928, L1 Loss: 0.01*2.9222\n",
      "Batch number:  59\n",
      "Active Features: 97.23%\n",
      "Decoder Weight Norm (Mean): 0.1797957718372345\n",
      "MSE Loss: 0.0930, L1 Loss: 0.01*4.3469\n",
      "Batch number:  60\n",
      "Active Features: 96.72%\n",
      "Decoder Weight Norm (Mean): 0.1797579675912857\n",
      "MSE Loss: 0.0932, L1 Loss: 0.01*5.2709\n",
      "Batch number:  61\n",
      "Active Features: 96.67%\n",
      "Decoder Weight Norm (Mean): 0.17971830070018768\n",
      "MSE Loss: 0.0926, L1 Loss: 0.01*3.3782\n",
      "Batch number:  62\n",
      "Active Features: 97.01%\n",
      "Decoder Weight Norm (Mean): 0.17967936396598816\n",
      "MSE Loss: 0.0921, L1 Loss: 0.01*3.8471\n",
      "Batch number:  63\n",
      "Active Features: 96.92%\n",
      "Decoder Weight Norm (Mean): 0.17964063584804535\n",
      "MSE Loss: 0.0935, L1 Loss: 0.01*3.3526\n",
      "Batch number:  64\n",
      "Active Features: 97.66%\n",
      "Decoder Weight Norm (Mean): 0.1796024590730667\n",
      "MSE Loss: 0.0930, L1 Loss: 0.01*2.8971\n",
      "Batch number:  65\n",
      "Active Features: 96.83%\n",
      "Decoder Weight Norm (Mean): 0.17956532537937164\n",
      "MSE Loss: 0.0925, L1 Loss: 0.01*3.3641\n",
      "Batch number:  66\n",
      "Active Features: 96.05%\n",
      "Decoder Weight Norm (Mean): 0.17952878773212433\n",
      "MSE Loss: 0.0922, L1 Loss: 0.01*3.8279\n",
      "Batch number:  67\n",
      "Active Features: 96.53%\n",
      "Decoder Weight Norm (Mean): 0.17949232459068298\n",
      "MSE Loss: 0.0919, L1 Loss: 0.01*4.2757\n",
      "Batch number:  68\n",
      "Active Features: 96.98%\n",
      "Decoder Weight Norm (Mean): 0.17945514619350433\n",
      "MSE Loss: 0.0913, L1 Loss: 0.01*3.7936\n",
      "Batch number:  69\n",
      "Active Features: 96.97%\n",
      "Decoder Weight Norm (Mean): 0.1794179379940033\n",
      "MSE Loss: 0.0919, L1 Loss: 0.01*2.8671\n",
      "Batch number:  70\n",
      "Active Features: 96.65%\n",
      "Decoder Weight Norm (Mean): 0.1793820708990097\n",
      "MSE Loss: 0.0921, L1 Loss: 0.01*4.2134\n",
      "Batch number:  71\n",
      "Active Features: 96.31%\n",
      "Decoder Weight Norm (Mean): 0.1793452650308609\n",
      "MSE Loss: 0.0918, L1 Loss: 0.01*5.1663\n",
      "Batch number:  72\n",
      "Active Features: 96.51%\n",
      "Decoder Weight Norm (Mean): 0.17930668592453003\n",
      "MSE Loss: 0.0923, L1 Loss: 0.01*2.8794\n",
      "Batch number:  73\n",
      "Active Features: 96.04%\n",
      "Decoder Weight Norm (Mean): 0.17926938831806183\n",
      "MSE Loss: 0.0915, L1 Loss: 0.01*4.2076\n",
      "Batch number:  74\n",
      "Active Features: 95.99%\n",
      "Decoder Weight Norm (Mean): 0.1792317032814026\n",
      "MSE Loss: 0.0917, L1 Loss: 0.01*4.2107\n",
      "Batch number:  75\n",
      "Active Features: 96.58%\n",
      "Decoder Weight Norm (Mean): 0.17919376492500305\n",
      "MSE Loss: 0.0915, L1 Loss: 0.01*3.7299\n",
      "Batch number:  76\n",
      "Active Features: 96.75%\n",
      "Decoder Weight Norm (Mean): 0.17915581166744232\n",
      "MSE Loss: 0.0912, L1 Loss: 0.01*3.7339\n",
      "Batch number:  77\n",
      "Active Features: 97.00%\n",
      "Decoder Weight Norm (Mean): 0.17911793291568756\n",
      "MSE Loss: 0.0917, L1 Loss: 0.01*4.1647\n",
      "Batch number:  78\n",
      "Active Features: 97.02%\n",
      "Decoder Weight Norm (Mean): 0.17907936871051788\n",
      "MSE Loss: 0.0915, L1 Loss: 0.01*2.4152\n",
      "Batch number:  79\n",
      "Active Features: 96.17%\n",
      "Decoder Weight Norm (Mean): 0.1790427714586258\n",
      "MSE Loss: 0.0913, L1 Loss: 0.01*2.4048\n",
      "Batch number:  80\n",
      "Active Features: 95.27%\n",
      "Decoder Weight Norm (Mean): 0.17900808155536652\n",
      "MSE Loss: 0.0916, L1 Loss: 0.01*2.3782\n",
      "Batch number:  81\n",
      "Active Features: 95.89%\n",
      "Decoder Weight Norm (Mean): 0.1789752095937729\n",
      "MSE Loss: 0.0911, L1 Loss: 0.01*3.7136\n",
      "Batch number:  82\n",
      "Active Features: 95.70%\n",
      "Decoder Weight Norm (Mean): 0.17894218862056732\n",
      "MSE Loss: 0.0911, L1 Loss: 0.01*4.1102\n",
      "Batch number:  83\n",
      "Active Features: 96.68%\n",
      "Decoder Weight Norm (Mean): 0.17890828847885132\n",
      "MSE Loss: 0.0923, L1 Loss: 0.01*1.9351\n",
      "Batch number:  84\n",
      "Active Features: 95.57%\n",
      "Decoder Weight Norm (Mean): 0.17887650430202484\n",
      "MSE Loss: 0.0895, L1 Loss: 0.01*3.6627\n",
      "Batch number:  85\n",
      "Active Features: 95.21%\n",
      "Decoder Weight Norm (Mean): 0.1788444221019745\n",
      "MSE Loss: 0.0897, L1 Loss: 0.01*4.5403\n",
      "Batch number:  86\n",
      "Active Features: 95.75%\n",
      "Decoder Weight Norm (Mean): 0.1788109987974167\n",
      "MSE Loss: 0.0900, L1 Loss: 0.01*5.8161\n",
      "Batch number:  87\n",
      "Active Features: 94.92%\n",
      "Decoder Weight Norm (Mean): 0.17877447605133057\n",
      "MSE Loss: 0.0903, L1 Loss: 0.01*2.3811\n",
      "Batch number:  88\n",
      "Active Features: 95.28%\n",
      "Decoder Weight Norm (Mean): 0.17874027788639069\n",
      "MSE Loss: 0.0904, L1 Loss: 0.01*4.0827\n",
      "Batch number:  89\n",
      "Active Features: 95.38%\n",
      "Decoder Weight Norm (Mean): 0.1787053495645523\n",
      "MSE Loss: 0.0903, L1 Loss: 0.01*4.0797\n",
      "Batch number:  90\n",
      "Active Features: 96.30%\n",
      "Decoder Weight Norm (Mean): 0.1786697506904602\n",
      "MSE Loss: 0.0904, L1 Loss: 0.01*4.0431\n",
      "Batch number:  91\n",
      "Active Features: 95.25%\n",
      "Decoder Weight Norm (Mean): 0.1786336600780487\n",
      "MSE Loss: 0.0896, L1 Loss: 0.01*4.4747\n",
      "Batch number:  92\n",
      "Active Features: 95.36%\n",
      "Decoder Weight Norm (Mean): 0.17859651148319244\n",
      "MSE Loss: 0.0907, L1 Loss: 0.01*3.2145\n",
      "Batch number:  93\n",
      "Active Features: 94.77%\n",
      "Decoder Weight Norm (Mean): 0.17856042087078094\n",
      "MSE Loss: 0.0916, L1 Loss: 0.01*1.0967\n",
      "Batch number:  94\n",
      "Active Features: 94.95%\n",
      "Decoder Weight Norm (Mean): 0.17852821946144104\n",
      "MSE Loss: 0.0908, L1 Loss: 0.01*1.5157\n",
      "Batch number:  95\n",
      "Active Features: 95.21%\n",
      "Decoder Weight Norm (Mean): 0.1784987598657608\n",
      "MSE Loss: 0.0912, L1 Loss: 0.01*1.9579\n",
      "Batch number:  96\n",
      "Active Features: 95.20%\n",
      "Decoder Weight Norm (Mean): 0.17847131192684174\n",
      "MSE Loss: 0.0914, L1 Loss: 0.01*3.1827\n",
      "Batch number:  97\n",
      "Active Features: 95.04%\n",
      "Decoder Weight Norm (Mean): 0.17844389379024506\n",
      "MSE Loss: 0.0897, L1 Loss: 0.01*4.4157\n",
      "Batch number:  98\n",
      "Active Features: 95.36%\n",
      "Decoder Weight Norm (Mean): 0.17841455340385437\n",
      "MSE Loss: 0.0902, L1 Loss: 0.01*3.5950\n",
      "Batch number:  99\n",
      "Active Features: 95.19%\n",
      "Decoder Weight Norm (Mean): 0.1783846914768219\n",
      "MSE Loss: 0.0907, L1 Loss: 0.01*1.5047\n",
      "Batch number:  100\n",
      "Active Features: 95.72%\n",
      "Decoder Weight Norm (Mean): 0.1783573180437088\n",
      "MSE Loss: 0.0906, L1 Loss: 0.01*2.7422\n",
      "Batch number:  101\n",
      "Active Features: 94.41%\n",
      "Decoder Weight Norm (Mean): 0.17833034694194794\n",
      "MSE Loss: 0.0907, L1 Loss: 0.01*4.7940\n",
      "Batch number:  102\n",
      "Active Features: 95.64%\n",
      "Decoder Weight Norm (Mean): 0.1783008724451065\n",
      "MSE Loss: 0.0897, L1 Loss: 0.01*3.5251\n",
      "Batch number:  103\n",
      "Active Features: 95.25%\n",
      "Decoder Weight Norm (Mean): 0.17827072739601135\n",
      "MSE Loss: 0.0905, L1 Loss: 0.01*3.5505\n",
      "Batch number:  104\n",
      "Active Features: 93.82%\n",
      "Decoder Weight Norm (Mean): 0.178240105509758\n",
      "MSE Loss: 0.0900, L1 Loss: 0.01*3.9489\n",
      "Batch number:  105\n",
      "Active Features: 94.78%\n",
      "Decoder Weight Norm (Mean): 0.1782086193561554\n",
      "MSE Loss: 0.0902, L1 Loss: 0.01*3.1206\n",
      "Batch number:  106\n",
      "Active Features: 93.97%\n",
      "Decoder Weight Norm (Mean): 0.17817755043506622\n",
      "MSE Loss: 0.0905, L1 Loss: 0.01*2.7356\n",
      "Batch number:  107\n",
      "Active Features: 94.91%\n",
      "Decoder Weight Norm (Mean): 0.17814747989177704\n",
      "MSE Loss: 0.0897, L1 Loss: 0.01*3.9132\n",
      "Batch number:  108\n",
      "Active Features: 93.62%\n",
      "Decoder Weight Norm (Mean): 0.17811645567417145\n",
      "MSE Loss: 0.0891, L1 Loss: 0.01*2.7183\n",
      "Batch number:  109\n",
      "Active Features: 95.06%\n",
      "Decoder Weight Norm (Mean): 0.178086519241333\n",
      "MSE Loss: 0.0898, L1 Loss: 0.01*4.2908\n",
      "Batch number:  110\n",
      "Active Features: 95.64%\n",
      "Decoder Weight Norm (Mean): 0.17805492877960205\n",
      "MSE Loss: 0.0913, L1 Loss: 0.01*2.7004\n",
      "Batch number:  111\n",
      "Active Features: 94.25%\n",
      "Decoder Weight Norm (Mean): 0.178024023771286\n",
      "MSE Loss: 0.0894, L1 Loss: 0.01*1.9325\n",
      "Batch number:  112\n",
      "Active Features: 95.66%\n",
      "Decoder Weight Norm (Mean): 0.17799526453018188\n",
      "MSE Loss: 0.0892, L1 Loss: 0.01*2.7011\n",
      "Batch number:  113\n",
      "Active Features: 93.99%\n",
      "Decoder Weight Norm (Mean): 0.17796696722507477\n",
      "MSE Loss: 0.0901, L1 Loss: 0.01*3.8735\n",
      "Batch number:  114\n",
      "Active Features: 94.80%\n",
      "Decoder Weight Norm (Mean): 0.1779375821352005\n",
      "MSE Loss: 0.0899, L1 Loss: 0.01*3.0522\n",
      "Batch number:  115\n",
      "Active Features: 94.62%\n",
      "Decoder Weight Norm (Mean): 0.17790845036506653\n",
      "MSE Loss: 0.0904, L1 Loss: 0.01*3.0707\n",
      "Batch number:  116\n",
      "Active Features: 94.58%\n",
      "Decoder Weight Norm (Mean): 0.17787951231002808\n",
      "MSE Loss: 0.0885, L1 Loss: 0.01*3.4550\n",
      "Batch number:  117\n",
      "Active Features: 94.59%\n",
      "Decoder Weight Norm (Mean): 0.17785024642944336\n",
      "MSE Loss: 0.0890, L1 Loss: 0.01*5.0108\n",
      "Batch number:  118\n",
      "Active Features: 94.50%\n",
      "Decoder Weight Norm (Mean): 0.1778181791305542\n",
      "MSE Loss: 0.0893, L1 Loss: 0.01*4.5909\n",
      "Batch number:  119\n",
      "Active Features: 95.36%\n",
      "Decoder Weight Norm (Mean): 0.17778407037258148\n",
      "MSE Loss: 0.0893, L1 Loss: 0.01*3.4550\n",
      "Batch number:  120\n",
      "Active Features: 93.56%\n",
      "Decoder Weight Norm (Mean): 0.1777498722076416\n",
      "MSE Loss: 0.0894, L1 Loss: 0.01*3.4322\n",
      "Batch number:  121\n",
      "Active Features: 93.22%\n",
      "Decoder Weight Norm (Mean): 0.17771591246128082\n",
      "MSE Loss: 0.0893, L1 Loss: 0.01*3.0682\n",
      "Batch number:  122\n",
      "Active Features: 94.34%\n",
      "Decoder Weight Norm (Mean): 0.17768287658691406\n",
      "MSE Loss: 0.0893, L1 Loss: 0.01*2.2577\n",
      "Batch number:  123\n",
      "Active Features: 94.78%\n",
      "Decoder Weight Norm (Mean): 0.17765165865421295\n",
      "MSE Loss: 0.0889, L1 Loss: 0.01*2.2986\n",
      "Batch number:  124\n",
      "Active Features: 92.75%\n",
      "Decoder Weight Norm (Mean): 0.17762203514575958\n",
      "MSE Loss: 0.0894, L1 Loss: 0.01*3.0503\n",
      "Batch number:  125\n",
      "Active Features: 93.43%\n",
      "Decoder Weight Norm (Mean): 0.1775929033756256\n",
      "MSE Loss: 0.0888, L1 Loss: 0.01*4.1748\n",
      "Batch number:  126\n",
      "Active Features: 93.34%\n",
      "Decoder Weight Norm (Mean): 0.17756244540214539\n",
      "MSE Loss: 0.0882, L1 Loss: 0.01*3.7482\n",
      "Batch number:  127\n",
      "Active Features: 93.91%\n",
      "Decoder Weight Norm (Mean): 0.17753136157989502\n",
      "MSE Loss: 0.0891, L1 Loss: 0.01*2.6369\n",
      "Epoch [3/10], Loss: 16.4273 ----------------------------\n",
      "Batch number:  0\n",
      "Active Features: 94.45%\n",
      "Decoder Weight Norm (Mean): 0.17750117182731628\n",
      "MSE Loss: 0.0894, L1 Loss: 0.01*1.9124\n",
      "Batch number:  1\n",
      "Active Features: 93.91%\n",
      "Decoder Weight Norm (Mean): 0.1774730235338211\n",
      "MSE Loss: 0.0882, L1 Loss: 0.01*5.9756\n",
      "Batch number:  2\n",
      "Active Features: 92.69%\n",
      "Decoder Weight Norm (Mean): 0.1774405688047409\n",
      "MSE Loss: 0.0892, L1 Loss: 0.01*3.0125\n",
      "Batch number:  3\n",
      "Active Features: 92.44%\n",
      "Decoder Weight Norm (Mean): 0.17740890383720398\n",
      "MSE Loss: 0.0882, L1 Loss: 0.01*2.2609\n",
      "Batch number:  4\n",
      "Active Features: 91.73%\n",
      "Decoder Weight Norm (Mean): 0.17737899720668793\n",
      "MSE Loss: 0.0874, L1 Loss: 0.01*3.3498\n",
      "Batch number:  5\n",
      "Active Features: 93.65%\n",
      "Decoder Weight Norm (Mean): 0.17734912037849426\n",
      "MSE Loss: 0.0886, L1 Loss: 0.01*3.0029\n",
      "Batch number:  6\n",
      "Active Features: 93.75%\n",
      "Decoder Weight Norm (Mean): 0.1773197501897812\n",
      "MSE Loss: 0.0881, L1 Loss: 0.01*3.3099\n",
      "Batch number:  7\n",
      "Active Features: 93.45%\n",
      "Decoder Weight Norm (Mean): 0.17729032039642334\n",
      "MSE Loss: 0.0894, L1 Loss: 0.01*4.1006\n",
      "Batch number:  8\n",
      "Active Features: 93.39%\n",
      "Decoder Weight Norm (Mean): 0.17725954949855804\n",
      "MSE Loss: 0.0887, L1 Loss: 0.01*3.3153\n",
      "Batch number:  9\n",
      "Active Features: 92.49%\n",
      "Decoder Weight Norm (Mean): 0.17722877860069275\n",
      "MSE Loss: 0.0882, L1 Loss: 0.01*4.0390\n",
      "Batch number:  10\n",
      "Active Features: 94.12%\n",
      "Decoder Weight Norm (Mean): 0.17719687521457672\n",
      "MSE Loss: 0.0888, L1 Loss: 0.01*4.0294\n",
      "Batch number:  11\n",
      "Active Features: 92.58%\n",
      "Decoder Weight Norm (Mean): 0.17716380953788757\n",
      "MSE Loss: 0.0885, L1 Loss: 0.01*3.6944\n",
      "Batch number:  12\n",
      "Active Features: 92.25%\n",
      "Decoder Weight Norm (Mean): 0.1771305501461029\n",
      "MSE Loss: 0.0891, L1 Loss: 0.01*3.3046\n",
      "Batch number:  13\n",
      "Active Features: 93.64%\n",
      "Decoder Weight Norm (Mean): 0.1770976334810257\n",
      "MSE Loss: 0.0878, L1 Loss: 0.01*5.4022\n",
      "Batch number:  14\n",
      "Active Features: 92.36%\n",
      "Decoder Weight Norm (Mean): 0.17706137895584106\n",
      "MSE Loss: 0.0880, L1 Loss: 0.01*2.9169\n",
      "Batch number:  15\n",
      "Active Features: 93.60%\n",
      "Decoder Weight Norm (Mean): 0.17702627182006836\n",
      "MSE Loss: 0.0884, L1 Loss: 0.01*3.9655\n",
      "Batch number:  16\n",
      "Active Features: 92.52%\n",
      "Decoder Weight Norm (Mean): 0.1769903302192688\n",
      "MSE Loss: 0.0891, L1 Loss: 0.01*2.9301\n",
      "Batch number:  17\n",
      "Active Features: 92.44%\n",
      "Decoder Weight Norm (Mean): 0.1769556850194931\n",
      "MSE Loss: 0.0882, L1 Loss: 0.01*4.3267\n",
      "Batch number:  18\n",
      "Active Features: 93.47%\n",
      "Decoder Weight Norm (Mean): 0.17691969871520996\n",
      "MSE Loss: 0.0888, L1 Loss: 0.01*2.5559\n",
      "Batch number:  19\n",
      "Active Features: 93.55%\n",
      "Decoder Weight Norm (Mean): 0.17688536643981934\n",
      "MSE Loss: 0.0886, L1 Loss: 0.01*2.1896\n",
      "Batch number:  20\n",
      "Active Features: 92.41%\n",
      "Decoder Weight Norm (Mean): 0.17685306072235107\n",
      "MSE Loss: 0.0891, L1 Loss: 0.01*4.6055\n",
      "Batch number:  21\n",
      "Active Features: 93.67%\n",
      "Decoder Weight Norm (Mean): 0.17681872844696045\n",
      "MSE Loss: 0.0886, L1 Loss: 0.01*3.9044\n",
      "Batch number:  22\n",
      "Active Features: 92.84%\n",
      "Decoder Weight Norm (Mean): 0.1767837405204773\n",
      "MSE Loss: 0.0880, L1 Loss: 0.01*2.5388\n",
      "Batch number:  23\n",
      "Active Features: 92.51%\n",
      "Decoder Weight Norm (Mean): 0.17675040662288666\n",
      "MSE Loss: 0.0895, L1 Loss: 0.01*3.8925\n",
      "Batch number:  24\n",
      "Active Features: 91.81%\n",
      "Decoder Weight Norm (Mean): 0.17671632766723633\n",
      "MSE Loss: 0.0885, L1 Loss: 0.01*4.2199\n",
      "Batch number:  25\n",
      "Active Features: 91.29%\n",
      "Decoder Weight Norm (Mean): 0.17668099701404572\n",
      "MSE Loss: 0.0882, L1 Loss: 0.01*3.1952\n",
      "Batch number:  26\n",
      "Active Features: 92.20%\n",
      "Decoder Weight Norm (Mean): 0.17664635181427002\n",
      "MSE Loss: 0.0871, L1 Loss: 0.01*2.5254\n",
      "Batch number:  27\n",
      "Active Features: 91.50%\n",
      "Decoder Weight Norm (Mean): 0.17661331593990326\n",
      "MSE Loss: 0.0867, L1 Loss: 0.01*3.5142\n",
      "Batch number:  28\n",
      "Active Features: 91.63%\n",
      "Decoder Weight Norm (Mean): 0.17658010125160217\n",
      "MSE Loss: 0.0875, L1 Loss: 0.01*2.8301\n",
      "Batch number:  29\n",
      "Active Features: 94.27%\n",
      "Decoder Weight Norm (Mean): 0.1765478551387787\n",
      "MSE Loss: 0.0883, L1 Loss: 0.01*4.4700\n",
      "Batch number:  30\n",
      "Active Features: 91.40%\n",
      "Decoder Weight Norm (Mean): 0.17651347815990448\n",
      "MSE Loss: 0.0872, L1 Loss: 0.01*2.8302\n",
      "Batch number:  31\n",
      "Active Features: 92.08%\n",
      "Decoder Weight Norm (Mean): 0.17648032307624817\n",
      "MSE Loss: 0.0869, L1 Loss: 0.01*2.1656\n",
      "Batch number:  32\n",
      "Active Features: 91.09%\n",
      "Decoder Weight Norm (Mean): 0.17644916474819183\n",
      "MSE Loss: 0.0875, L1 Loss: 0.01*3.4746\n",
      "Batch number:  33\n",
      "Active Features: 90.84%\n",
      "Decoder Weight Norm (Mean): 0.17641764879226685\n",
      "MSE Loss: 0.0870, L1 Loss: 0.01*3.1268\n",
      "Batch number:  34\n",
      "Active Features: 90.03%\n",
      "Decoder Weight Norm (Mean): 0.176386296749115\n",
      "MSE Loss: 0.0876, L1 Loss: 0.01*1.8342\n",
      "Batch number:  35\n",
      "Active Features: 91.52%\n",
      "Decoder Weight Norm (Mean): 0.17635758221149445\n",
      "MSE Loss: 0.0864, L1 Loss: 0.01*2.7715\n",
      "Batch number:  36\n",
      "Active Features: 92.23%\n",
      "Decoder Weight Norm (Mean): 0.17632938921451569\n",
      "MSE Loss: 0.0872, L1 Loss: 0.01*3.7539\n",
      "Batch number:  37\n",
      "Active Features: 90.62%\n",
      "Decoder Weight Norm (Mean): 0.1763000190258026\n",
      "MSE Loss: 0.0883, L1 Loss: 0.01*1.8239\n",
      "Batch number:  38\n",
      "Active Features: 90.74%\n",
      "Decoder Weight Norm (Mean): 0.17627301812171936\n",
      "MSE Loss: 0.0873, L1 Loss: 0.01*5.3173\n",
      "Batch number:  39\n",
      "Active Features: 91.88%\n",
      "Decoder Weight Norm (Mean): 0.17624211311340332\n",
      "MSE Loss: 0.0878, L1 Loss: 0.01*3.4152\n",
      "Batch number:  40\n",
      "Active Features: 91.22%\n",
      "Decoder Weight Norm (Mean): 0.17621095478534698\n",
      "MSE Loss: 0.0876, L1 Loss: 0.01*3.0874\n",
      "Batch number:  41\n",
      "Active Features: 90.06%\n",
      "Decoder Weight Norm (Mean): 0.17618000507354736\n",
      "MSE Loss: 0.0874, L1 Loss: 0.01*3.4026\n",
      "Batch number:  42\n",
      "Active Features: 92.16%\n",
      "Decoder Weight Norm (Mean): 0.1761489361524582\n",
      "MSE Loss: 0.0864, L1 Loss: 0.01*3.9929\n",
      "Batch number:  43\n",
      "Active Features: 90.44%\n",
      "Decoder Weight Norm (Mean): 0.17611660063266754\n",
      "MSE Loss: 0.0873, L1 Loss: 0.01*2.4458\n",
      "Batch number:  44\n",
      "Active Features: 93.85%\n",
      "Decoder Weight Norm (Mean): 0.17608581483364105\n",
      "MSE Loss: 0.0874, L1 Loss: 0.01*4.9053\n",
      "Batch number:  45\n",
      "Active Features: 91.70%\n",
      "Decoder Weight Norm (Mean): 0.17605186998844147\n",
      "MSE Loss: 0.0878, L1 Loss: 0.01*2.4336\n",
      "Batch number:  46\n",
      "Active Features: 91.45%\n",
      "Decoder Weight Norm (Mean): 0.17601945996284485\n",
      "MSE Loss: 0.0874, L1 Loss: 0.01*2.1114\n",
      "Batch number:  47\n",
      "Active Features: 90.61%\n",
      "Decoder Weight Norm (Mean): 0.175989031791687\n",
      "MSE Loss: 0.0860, L1 Loss: 0.01*3.9557\n",
      "Batch number:  48\n",
      "Active Features: 90.14%\n",
      "Decoder Weight Norm (Mean): 0.17595723271369934\n",
      "MSE Loss: 0.0874, L1 Loss: 0.01*3.3500\n",
      "Batch number:  49\n",
      "Active Features: 90.66%\n",
      "Decoder Weight Norm (Mean): 0.17592550814151764\n",
      "MSE Loss: 0.0871, L1 Loss: 0.01*3.0142\n",
      "Batch number:  50\n",
      "Active Features: 90.31%\n",
      "Decoder Weight Norm (Mean): 0.17589415609836578\n",
      "MSE Loss: 0.0872, L1 Loss: 0.01*4.2115\n",
      "Batch number:  51\n",
      "Active Features: 91.51%\n",
      "Decoder Weight Norm (Mean): 0.17586126923561096\n",
      "MSE Loss: 0.0872, L1 Loss: 0.01*3.0007\n",
      "Batch number:  52\n",
      "Active Features: 89.83%\n",
      "Decoder Weight Norm (Mean): 0.17582885921001434\n",
      "MSE Loss: 0.0874, L1 Loss: 0.01*3.6040\n",
      "Batch number:  53\n",
      "Active Features: 91.33%\n",
      "Decoder Weight Norm (Mean): 0.17579616606235504\n",
      "MSE Loss: 0.0871, L1 Loss: 0.01*2.3985\n",
      "Batch number:  54\n",
      "Active Features: 90.90%\n",
      "Decoder Weight Norm (Mean): 0.17576511204242706\n",
      "MSE Loss: 0.0858, L1 Loss: 0.01*2.0819\n",
      "Batch number:  55\n",
      "Active Features: 92.12%\n",
      "Decoder Weight Norm (Mean): 0.175736203789711\n",
      "MSE Loss: 0.0868, L1 Loss: 0.01*3.5781\n",
      "Batch number:  56\n",
      "Active Features: 89.62%\n",
      "Decoder Weight Norm (Mean): 0.17570629715919495\n",
      "MSE Loss: 0.0864, L1 Loss: 0.01*2.9669\n",
      "Batch number:  57\n",
      "Active Features: 90.38%\n",
      "Decoder Weight Norm (Mean): 0.17567676305770874\n",
      "MSE Loss: 0.0882, L1 Loss: 0.01*1.5303\n",
      "Batch number:  58\n",
      "Active Features: 89.57%\n",
      "Decoder Weight Norm (Mean): 0.1756502091884613\n",
      "MSE Loss: 0.0868, L1 Loss: 0.01*2.6732\n",
      "Batch number:  59\n",
      "Active Features: 89.13%\n",
      "Decoder Weight Norm (Mean): 0.17562419176101685\n",
      "MSE Loss: 0.0866, L1 Loss: 0.01*2.9755\n",
      "Batch number:  60\n",
      "Active Features: 89.99%\n",
      "Decoder Weight Norm (Mean): 0.17559842765331268\n",
      "MSE Loss: 0.0864, L1 Loss: 0.01*2.3804\n",
      "Batch number:  61\n",
      "Active Features: 89.36%\n",
      "Decoder Weight Norm (Mean): 0.1755736917257309\n",
      "MSE Loss: 0.0862, L1 Loss: 0.01*2.6492\n",
      "Batch number:  62\n",
      "Active Features: 90.09%\n",
      "Decoder Weight Norm (Mean): 0.17554931342601776\n",
      "MSE Loss: 0.0868, L1 Loss: 0.01*3.7771\n",
      "Batch number:  63\n",
      "Active Features: 90.25%\n",
      "Decoder Weight Norm (Mean): 0.1755233258008957\n",
      "MSE Loss: 0.0872, L1 Loss: 0.01*4.0577\n",
      "Batch number:  64\n",
      "Active Features: 91.09%\n",
      "Decoder Weight Norm (Mean): 0.17549535632133484\n",
      "MSE Loss: 0.0875, L1 Loss: 0.01*3.4789\n",
      "Batch number:  65\n",
      "Active Features: 89.05%\n",
      "Decoder Weight Norm (Mean): 0.17546647787094116\n",
      "MSE Loss: 0.0867, L1 Loss: 0.01*2.3497\n",
      "Batch number:  66\n",
      "Active Features: 89.72%\n",
      "Decoder Weight Norm (Mean): 0.17543937265872955\n",
      "MSE Loss: 0.0862, L1 Loss: 0.01*3.1850\n",
      "Batch number:  67\n",
      "Active Features: 89.64%\n",
      "Decoder Weight Norm (Mean): 0.1754118800163269\n",
      "MSE Loss: 0.0860, L1 Loss: 0.01*2.6298\n",
      "Batch number:  68\n",
      "Active Features: 90.06%\n",
      "Decoder Weight Norm (Mean): 0.17538517713546753\n",
      "MSE Loss: 0.0861, L1 Loss: 0.01*2.5979\n",
      "Batch number:  69\n",
      "Active Features: 90.37%\n",
      "Decoder Weight Norm (Mean): 0.17535915970802307\n",
      "MSE Loss: 0.0859, L1 Loss: 0.01*2.8801\n",
      "Batch number:  70\n",
      "Active Features: 89.11%\n",
      "Decoder Weight Norm (Mean): 0.1753333956003189\n",
      "MSE Loss: 0.0857, L1 Loss: 0.01*2.8712\n",
      "Batch number:  71\n",
      "Active Features: 89.85%\n",
      "Decoder Weight Norm (Mean): 0.17530766129493713\n",
      "MSE Loss: 0.0863, L1 Loss: 0.01*3.4356\n",
      "Batch number:  72\n",
      "Active Features: 89.53%\n",
      "Decoder Weight Norm (Mean): 0.17528103291988373\n",
      "MSE Loss: 0.0865, L1 Loss: 0.01*2.8767\n",
      "Batch number:  73\n",
      "Active Features: 88.26%\n",
      "Decoder Weight Norm (Mean): 0.17525485157966614\n",
      "MSE Loss: 0.0848, L1 Loss: 0.01*2.0373\n",
      "Batch number:  74\n",
      "Active Features: 88.24%\n",
      "Decoder Weight Norm (Mean): 0.175230473279953\n",
      "MSE Loss: 0.0848, L1 Loss: 0.01*3.9799\n",
      "Batch number:  75\n",
      "Active Features: 90.23%\n",
      "Decoder Weight Norm (Mean): 0.1752041131258011\n",
      "MSE Loss: 0.0860, L1 Loss: 0.01*3.3703\n",
      "Batch number:  76\n",
      "Active Features: 88.89%\n",
      "Decoder Weight Norm (Mean): 0.17517687380313873\n",
      "MSE Loss: 0.0861, L1 Loss: 0.01*3.1290\n",
      "Batch number:  77\n",
      "Active Features: 90.52%\n",
      "Decoder Weight Norm (Mean): 0.17514941096305847\n",
      "MSE Loss: 0.0859, L1 Loss: 0.01*2.2866\n",
      "Batch number:  78\n",
      "Active Features: 90.40%\n",
      "Decoder Weight Norm (Mean): 0.17512325942516327\n",
      "MSE Loss: 0.0863, L1 Loss: 0.01*2.8369\n",
      "Batch number:  79\n",
      "Active Features: 88.56%\n",
      "Decoder Weight Norm (Mean): 0.17509734630584717\n",
      "MSE Loss: 0.0865, L1 Loss: 0.01*2.3092\n",
      "Batch number:  80\n",
      "Active Features: 87.30%\n",
      "Decoder Weight Norm (Mean): 0.17507262527942657\n",
      "MSE Loss: 0.0855, L1 Loss: 0.01*3.0984\n",
      "Batch number:  81\n",
      "Active Features: 88.81%\n",
      "Decoder Weight Norm (Mean): 0.17504757642745972\n",
      "MSE Loss: 0.0854, L1 Loss: 0.01*1.7687\n",
      "Batch number:  82\n",
      "Active Features: 88.46%\n",
      "Decoder Weight Norm (Mean): 0.17502470314502716\n",
      "MSE Loss: 0.0864, L1 Loss: 0.01*1.7343\n",
      "Batch number:  83\n",
      "Active Features: 89.72%\n",
      "Decoder Weight Norm (Mean): 0.17500390112400055\n",
      "MSE Loss: 0.0869, L1 Loss: 0.01*2.2548\n",
      "Batch number:  84\n",
      "Active Features: 88.11%\n",
      "Decoder Weight Norm (Mean): 0.17498372495174408\n",
      "MSE Loss: 0.0847, L1 Loss: 0.01*2.2670\n",
      "Batch number:  85\n",
      "Active Features: 87.92%\n",
      "Decoder Weight Norm (Mean): 0.1749642789363861\n",
      "MSE Loss: 0.0853, L1 Loss: 0.01*3.0664\n",
      "Batch number:  86\n",
      "Active Features: 87.44%\n",
      "Decoder Weight Norm (Mean): 0.1749439835548401\n",
      "MSE Loss: 0.0852, L1 Loss: 0.01*2.5359\n",
      "Batch number:  87\n",
      "Active Features: 88.09%\n",
      "Decoder Weight Norm (Mean): 0.17492397129535675\n",
      "MSE Loss: 0.0848, L1 Loss: 0.01*3.0610\n",
      "Batch number:  88\n",
      "Active Features: 86.95%\n",
      "Decoder Weight Norm (Mean): 0.17490312457084656\n",
      "MSE Loss: 0.0851, L1 Loss: 0.01*1.7553\n",
      "Batch number:  89\n",
      "Active Features: 89.12%\n",
      "Decoder Weight Norm (Mean): 0.174884632229805\n",
      "MSE Loss: 0.0856, L1 Loss: 0.01*2.2729\n",
      "Batch number:  90\n",
      "Active Features: 89.62%\n",
      "Decoder Weight Norm (Mean): 0.17486675083637238\n",
      "MSE Loss: 0.0859, L1 Loss: 0.01*3.2655\n",
      "Batch number:  91\n",
      "Active Features: 87.24%\n",
      "Decoder Weight Norm (Mean): 0.17484740912914276\n",
      "MSE Loss: 0.0852, L1 Loss: 0.01*2.2481\n",
      "Batch number:  92\n",
      "Active Features: 86.79%\n",
      "Decoder Weight Norm (Mean): 0.1748289167881012\n",
      "MSE Loss: 0.0852, L1 Loss: 0.01*3.5521\n",
      "Batch number:  93\n",
      "Active Features: 88.15%\n",
      "Decoder Weight Norm (Mean): 0.1748088002204895\n",
      "MSE Loss: 0.0849, L1 Loss: 0.01*3.5142\n",
      "Batch number:  94\n",
      "Active Features: 87.07%\n",
      "Decoder Weight Norm (Mean): 0.17478704452514648\n",
      "MSE Loss: 0.0861, L1 Loss: 0.01*2.0078\n",
      "Batch number:  95\n",
      "Active Features: 85.18%\n",
      "Decoder Weight Norm (Mean): 0.17476679384708405\n",
      "MSE Loss: 0.0855, L1 Loss: 0.01*2.2706\n",
      "Batch number:  96\n",
      "Active Features: 86.81%\n",
      "Decoder Weight Norm (Mean): 0.17474763095378876\n",
      "MSE Loss: 0.0865, L1 Loss: 0.01*2.7567\n",
      "Batch number:  97\n",
      "Active Features: 87.92%\n",
      "Decoder Weight Norm (Mean): 0.17472824454307556\n",
      "MSE Loss: 0.0852, L1 Loss: 0.01*1.9847\n",
      "Batch number:  98\n",
      "Active Features: 87.61%\n",
      "Decoder Weight Norm (Mean): 0.174710214138031\n",
      "MSE Loss: 0.0856, L1 Loss: 0.01*2.2442\n",
      "Batch number:  99\n",
      "Active Features: 86.98%\n",
      "Decoder Weight Norm (Mean): 0.17469292879104614\n",
      "MSE Loss: 0.0850, L1 Loss: 0.01*2.9729\n",
      "Batch number:  100\n",
      "Active Features: 87.44%\n",
      "Decoder Weight Norm (Mean): 0.17467455565929413\n",
      "MSE Loss: 0.0860, L1 Loss: 0.01*2.2260\n",
      "Batch number:  101\n",
      "Active Features: 86.26%\n",
      "Decoder Weight Norm (Mean): 0.17465680837631226\n",
      "MSE Loss: 0.0847, L1 Loss: 0.01*1.5000\n",
      "Batch number:  102\n",
      "Active Features: 87.33%\n",
      "Decoder Weight Norm (Mean): 0.17464113235473633\n",
      "MSE Loss: 0.0848, L1 Loss: 0.01*2.9197\n",
      "Batch number:  103\n",
      "Active Features: 86.80%\n",
      "Decoder Weight Norm (Mean): 0.17462420463562012\n",
      "MSE Loss: 0.0855, L1 Loss: 0.01*2.4843\n",
      "Batch number:  104\n",
      "Active Features: 85.47%\n",
      "Decoder Weight Norm (Mean): 0.17460764944553375\n",
      "MSE Loss: 0.0853, L1 Loss: 0.01*2.2125\n",
      "Batch number:  105\n",
      "Active Features: 88.19%\n",
      "Decoder Weight Norm (Mean): 0.17459160089492798\n",
      "MSE Loss: 0.0851, L1 Loss: 0.01*3.1673\n",
      "Batch number:  106\n",
      "Active Features: 85.16%\n",
      "Decoder Weight Norm (Mean): 0.17457382380962372\n",
      "MSE Loss: 0.0842, L1 Loss: 0.01*2.2343\n",
      "Batch number:  107\n",
      "Active Features: 86.67%\n",
      "Decoder Weight Norm (Mean): 0.17455703020095825\n",
      "MSE Loss: 0.0846, L1 Loss: 0.01*2.4351\n",
      "Batch number:  108\n",
      "Active Features: 84.77%\n",
      "Decoder Weight Norm (Mean): 0.17454016208648682\n",
      "MSE Loss: 0.0843, L1 Loss: 0.01*2.2195\n",
      "Batch number:  109\n",
      "Active Features: 86.39%\n",
      "Decoder Weight Norm (Mean): 0.1745239496231079\n",
      "MSE Loss: 0.0848, L1 Loss: 0.01*3.1342\n",
      "Batch number:  110\n",
      "Active Features: 88.92%\n",
      "Decoder Weight Norm (Mean): 0.17450620234012604\n",
      "MSE Loss: 0.0867, L1 Loss: 0.01*2.9072\n",
      "Batch number:  111\n",
      "Active Features: 87.26%\n",
      "Decoder Weight Norm (Mean): 0.1744874119758606\n",
      "MSE Loss: 0.0853, L1 Loss: 0.01*1.9820\n",
      "Batch number:  112\n",
      "Active Features: 87.01%\n",
      "Decoder Weight Norm (Mean): 0.1744697391986847\n",
      "MSE Loss: 0.0841, L1 Loss: 0.01*2.2062\n",
      "Batch number:  113\n",
      "Active Features: 85.19%\n",
      "Decoder Weight Norm (Mean): 0.17445255815982819\n",
      "MSE Loss: 0.0861, L1 Loss: 0.01*3.1286\n",
      "Batch number:  114\n",
      "Active Features: 87.01%\n",
      "Decoder Weight Norm (Mean): 0.17443403601646423\n",
      "MSE Loss: 0.0845, L1 Loss: 0.01*2.6243\n",
      "Batch number:  115\n",
      "Active Features: 86.78%\n",
      "Decoder Weight Norm (Mean): 0.17441518604755402\n",
      "MSE Loss: 0.0846, L1 Loss: 0.01*2.6727\n",
      "Batch number:  116\n",
      "Active Features: 85.88%\n",
      "Decoder Weight Norm (Mean): 0.1743960827589035\n",
      "MSE Loss: 0.0841, L1 Loss: 0.01*3.1030\n",
      "Batch number:  117\n",
      "Active Features: 87.17%\n",
      "Decoder Weight Norm (Mean): 0.17437595129013062\n",
      "MSE Loss: 0.0846, L1 Loss: 0.01*2.1810\n",
      "Batch number:  118\n",
      "Active Features: 84.91%\n",
      "Decoder Weight Norm (Mean): 0.17435672879219055\n",
      "MSE Loss: 0.0842, L1 Loss: 0.01*1.4869\n",
      "Batch number:  119\n",
      "Active Features: 86.69%\n",
      "Decoder Weight Norm (Mean): 0.17434003949165344\n",
      "MSE Loss: 0.0845, L1 Loss: 0.01*1.9623\n",
      "Batch number:  120\n",
      "Active Features: 85.98%\n",
      "Decoder Weight Norm (Mean): 0.1743244081735611\n",
      "MSE Loss: 0.0843, L1 Loss: 0.01*3.3105\n",
      "Batch number:  121\n",
      "Active Features: 85.12%\n",
      "Decoder Weight Norm (Mean): 0.1743069589138031\n",
      "MSE Loss: 0.0851, L1 Loss: 0.01*2.8757\n",
      "Batch number:  122\n",
      "Active Features: 85.77%\n",
      "Decoder Weight Norm (Mean): 0.1742890179157257\n",
      "MSE Loss: 0.0848, L1 Loss: 0.01*2.3905\n",
      "Batch number:  123\n",
      "Active Features: 87.58%\n",
      "Decoder Weight Norm (Mean): 0.17427125573158264\n",
      "MSE Loss: 0.0853, L1 Loss: 0.01*2.1775\n",
      "Batch number:  124\n",
      "Active Features: 84.03%\n",
      "Decoder Weight Norm (Mean): 0.17425397038459778\n",
      "MSE Loss: 0.0846, L1 Loss: 0.01*2.1742\n",
      "Batch number:  125\n",
      "Active Features: 85.48%\n",
      "Decoder Weight Norm (Mean): 0.17423751950263977\n",
      "MSE Loss: 0.0837, L1 Loss: 0.01*2.8675\n",
      "Batch number:  126\n",
      "Active Features: 84.92%\n",
      "Decoder Weight Norm (Mean): 0.1742202341556549\n",
      "MSE Loss: 0.0836, L1 Loss: 0.01*1.7207\n",
      "Batch number:  127\n",
      "Active Features: 86.65%\n",
      "Decoder Weight Norm (Mean): 0.1742045283317566\n",
      "MSE Loss: 0.0840, L1 Loss: 0.01*2.6081\n",
      "Epoch [4/10], Loss: 14.8358 ----------------------------\n",
      "Batch number:  0\n",
      "Active Features: 86.33%\n",
      "Decoder Weight Norm (Mean): 0.17418824136257172\n",
      "MSE Loss: 0.0842, L1 Loss: 0.01*2.8333\n",
      "Batch number:  1\n",
      "Active Features: 85.67%\n",
      "Decoder Weight Norm (Mean): 0.1741710752248764\n",
      "MSE Loss: 0.0832, L1 Loss: 0.01*2.1665\n",
      "Batch number:  2\n",
      "Active Features: 85.17%\n",
      "Decoder Weight Norm (Mean): 0.17415465414524078\n",
      "MSE Loss: 0.0850, L1 Loss: 0.01*1.9535\n",
      "Batch number:  3\n",
      "Active Features: 83.95%\n",
      "Decoder Weight Norm (Mean): 0.1741395741701126\n",
      "MSE Loss: 0.0828, L1 Loss: 0.01*3.2704\n",
      "Batch number:  4\n",
      "Active Features: 83.77%\n",
      "Decoder Weight Norm (Mean): 0.17412251234054565\n",
      "MSE Loss: 0.0834, L1 Loss: 0.01*2.6031\n",
      "Batch number:  5\n",
      "Active Features: 84.24%\n",
      "Decoder Weight Norm (Mean): 0.17410540580749512\n",
      "MSE Loss: 0.0855, L1 Loss: 0.01*2.1708\n",
      "Batch number:  6\n",
      "Active Features: 83.99%\n",
      "Decoder Weight Norm (Mean): 0.1740889996290207\n",
      "MSE Loss: 0.0843, L1 Loss: 0.01*1.6796\n",
      "Batch number:  7\n",
      "Active Features: 85.28%\n",
      "Decoder Weight Norm (Mean): 0.1740742027759552\n",
      "MSE Loss: 0.0856, L1 Loss: 0.01*3.0385\n",
      "Batch number:  8\n",
      "Active Features: 86.14%\n",
      "Decoder Weight Norm (Mean): 0.174058198928833\n",
      "MSE Loss: 0.0833, L1 Loss: 0.01*3.6411\n",
      "Batch number:  9\n",
      "Active Features: 85.08%\n",
      "Decoder Weight Norm (Mean): 0.1740397810935974\n",
      "MSE Loss: 0.0829, L1 Loss: 0.01*3.0053\n",
      "Batch number:  10\n",
      "Active Features: 84.91%\n",
      "Decoder Weight Norm (Mean): 0.1740201860666275\n",
      "MSE Loss: 0.0844, L1 Loss: 0.01*2.7813\n",
      "Batch number:  11\n",
      "Active Features: 81.81%\n",
      "Decoder Weight Norm (Mean): 0.17400017380714417\n",
      "MSE Loss: 0.0839, L1 Loss: 0.01*2.1528\n",
      "Batch number:  12\n",
      "Active Features: 82.36%\n",
      "Decoder Weight Norm (Mean): 0.1739814132452011\n",
      "MSE Loss: 0.0843, L1 Loss: 0.01*3.2042\n",
      "Batch number:  13\n",
      "Active Features: 85.61%\n",
      "Decoder Weight Norm (Mean): 0.17396128177642822\n",
      "MSE Loss: 0.0833, L1 Loss: 0.01*3.7829\n",
      "Batch number:  14\n",
      "Active Features: 84.03%\n",
      "Decoder Weight Norm (Mean): 0.1739385426044464\n",
      "MSE Loss: 0.0829, L1 Loss: 0.01*2.5361\n",
      "Batch number:  15\n",
      "Active Features: 85.25%\n",
      "Decoder Weight Norm (Mean): 0.17391647398471832\n",
      "MSE Loss: 0.0838, L1 Loss: 0.01*2.9451\n",
      "Batch number:  16\n",
      "Active Features: 83.34%\n",
      "Decoder Weight Norm (Mean): 0.17389371991157532\n",
      "MSE Loss: 0.0853, L1 Loss: 0.01*2.9496\n",
      "Batch number:  17\n",
      "Active Features: 83.94%\n",
      "Decoder Weight Norm (Mean): 0.17387044429779053\n",
      "MSE Loss: 0.0845, L1 Loss: 0.01*2.5380\n",
      "Batch number:  18\n",
      "Active Features: 86.30%\n",
      "Decoder Weight Norm (Mean): 0.17384777963161469\n",
      "MSE Loss: 0.0848, L1 Loss: 0.01*1.8978\n",
      "Batch number:  19\n",
      "Active Features: 85.58%\n",
      "Decoder Weight Norm (Mean): 0.17382682859897614\n",
      "MSE Loss: 0.0842, L1 Loss: 0.01*3.3058\n",
      "Batch number:  20\n",
      "Active Features: 84.07%\n",
      "Decoder Weight Norm (Mean): 0.1738041639328003\n",
      "MSE Loss: 0.0838, L1 Loss: 0.01*2.7073\n",
      "Batch number:  21\n",
      "Active Features: 84.03%\n",
      "Decoder Weight Norm (Mean): 0.17378166317939758\n",
      "MSE Loss: 0.0851, L1 Loss: 0.01*2.2963\n",
      "Batch number:  22\n",
      "Active Features: 84.29%\n",
      "Decoder Weight Norm (Mean): 0.17376013100147247\n",
      "MSE Loss: 0.0839, L1 Loss: 0.01*1.8936\n",
      "Batch number:  23\n",
      "Active Features: 83.47%\n",
      "Decoder Weight Norm (Mean): 0.1737402230501175\n",
      "MSE Loss: 0.0847, L1 Loss: 0.01*2.6886\n",
      "Batch number:  24\n",
      "Active Features: 84.17%\n",
      "Decoder Weight Norm (Mean): 0.17372003197669983\n",
      "MSE Loss: 0.0843, L1 Loss: 0.01*2.3028\n",
      "Batch number:  25\n",
      "Active Features: 81.86%\n",
      "Decoder Weight Norm (Mean): 0.1737005114555359\n",
      "MSE Loss: 0.0838, L1 Loss: 0.01*1.9017\n",
      "Batch number:  26\n",
      "Active Features: 84.53%\n",
      "Decoder Weight Norm (Mean): 0.17368265986442566\n",
      "MSE Loss: 0.0827, L1 Loss: 0.01*2.2694\n",
      "Batch number:  27\n",
      "Active Features: 82.78%\n",
      "Decoder Weight Norm (Mean): 0.17366504669189453\n",
      "MSE Loss: 0.0830, L1 Loss: 0.01*2.0734\n",
      "Batch number:  28\n",
      "Active Features: 83.24%\n",
      "Decoder Weight Norm (Mean): 0.1736481636762619\n",
      "MSE Loss: 0.0835, L1 Loss: 0.01*2.6702\n",
      "Batch number:  29\n",
      "Active Features: 87.31%\n",
      "Decoder Weight Norm (Mean): 0.17363078892230988\n",
      "MSE Loss: 0.0840, L1 Loss: 0.01*1.8664\n",
      "Batch number:  30\n",
      "Active Features: 80.13%\n",
      "Decoder Weight Norm (Mean): 0.17361445724964142\n",
      "MSE Loss: 0.0832, L1 Loss: 0.01*2.4621\n",
      "Batch number:  31\n",
      "Active Features: 84.21%\n",
      "Decoder Weight Norm (Mean): 0.1735987514257431\n",
      "MSE Loss: 0.0823, L1 Loss: 0.01*2.4499\n",
      "Batch number:  32\n",
      "Active Features: 82.79%\n",
      "Decoder Weight Norm (Mean): 0.17358261346817017\n",
      "MSE Loss: 0.0838, L1 Loss: 0.01*1.8876\n",
      "Batch number:  33\n",
      "Active Features: 83.49%\n",
      "Decoder Weight Norm (Mean): 0.17356757819652557\n",
      "MSE Loss: 0.0830, L1 Loss: 0.01*2.2407\n",
      "Batch number:  34\n",
      "Active Features: 81.88%\n",
      "Decoder Weight Norm (Mean): 0.17355281114578247\n",
      "MSE Loss: 0.0833, L1 Loss: 0.01*2.2583\n",
      "Batch number:  35\n",
      "Active Features: 81.98%\n",
      "Decoder Weight Norm (Mean): 0.17353835701942444\n",
      "MSE Loss: 0.0831, L1 Loss: 0.01*2.2238\n",
      "Batch number:  36\n",
      "Active Features: 83.47%\n",
      "Decoder Weight Norm (Mean): 0.17352408170700073\n",
      "MSE Loss: 0.0833, L1 Loss: 0.01*2.8456\n",
      "Batch number:  37\n",
      "Active Features: 83.89%\n",
      "Decoder Weight Norm (Mean): 0.1735089272260666\n",
      "MSE Loss: 0.0835, L1 Loss: 0.01*2.2516\n",
      "Batch number:  38\n",
      "Active Features: 83.46%\n",
      "Decoder Weight Norm (Mean): 0.17349399626255035\n",
      "MSE Loss: 0.0833, L1 Loss: 0.01*2.4253\n",
      "Batch number:  39\n",
      "Active Features: 82.03%\n",
      "Decoder Weight Norm (Mean): 0.17347878217697144\n",
      "MSE Loss: 0.0838, L1 Loss: 0.01*1.6810\n",
      "Batch number:  40\n",
      "Active Features: 82.56%\n",
      "Decoder Weight Norm (Mean): 0.17346526682376862\n",
      "MSE Loss: 0.0829, L1 Loss: 0.01*2.2434\n",
      "Batch number:  41\n",
      "Active Features: 81.45%\n",
      "Decoder Weight Norm (Mean): 0.17345179617404938\n",
      "MSE Loss: 0.0829, L1 Loss: 0.01*2.0397\n",
      "Batch number:  42\n",
      "Active Features: 81.64%\n",
      "Decoder Weight Norm (Mean): 0.17343871295452118\n",
      "MSE Loss: 0.0827, L1 Loss: 0.01*2.0456\n",
      "Batch number:  43\n",
      "Active Features: 82.12%\n",
      "Decoder Weight Norm (Mean): 0.17342612147331238\n",
      "MSE Loss: 0.0828, L1 Loss: 0.01*2.6075\n",
      "Batch number:  44\n",
      "Active Features: 85.75%\n",
      "Decoder Weight Norm (Mean): 0.17341248691082\n",
      "MSE Loss: 0.0828, L1 Loss: 0.01*2.3995\n",
      "Batch number:  45\n",
      "Active Features: 83.48%\n",
      "Decoder Weight Norm (Mean): 0.1733984351158142\n",
      "MSE Loss: 0.0829, L1 Loss: 0.01*3.1352\n",
      "Batch number:  46\n",
      "Active Features: 82.00%\n",
      "Decoder Weight Norm (Mean): 0.17338229715824127\n",
      "MSE Loss: 0.0836, L1 Loss: 0.01*2.9282\n",
      "Batch number:  47\n",
      "Active Features: 80.44%\n",
      "Decoder Weight Norm (Mean): 0.17336498200893402\n",
      "MSE Loss: 0.0820, L1 Loss: 0.01*2.2117\n",
      "Batch number:  48\n",
      "Active Features: 82.09%\n",
      "Decoder Weight Norm (Mean): 0.1733483374118805\n",
      "MSE Loss: 0.0832, L1 Loss: 0.01*2.2178\n",
      "Batch number:  49\n",
      "Active Features: 82.97%\n",
      "Decoder Weight Norm (Mean): 0.173332080245018\n",
      "MSE Loss: 0.0832, L1 Loss: 0.01*2.1782\n",
      "Batch number:  50\n",
      "Active Features: 82.38%\n",
      "Decoder Weight Norm (Mean): 0.17331627011299133\n",
      "MSE Loss: 0.0825, L1 Loss: 0.01*2.7103\n",
      "Batch number:  51\n",
      "Active Features: 81.48%\n",
      "Decoder Weight Norm (Mean): 0.17329958081245422\n",
      "MSE Loss: 0.0820, L1 Loss: 0.01*2.3672\n",
      "Batch number:  52\n",
      "Active Features: 78.72%\n",
      "Decoder Weight Norm (Mean): 0.17328299582004547\n",
      "MSE Loss: 0.0828, L1 Loss: 0.01*1.8683\n",
      "Batch number:  53\n",
      "Active Features: 82.28%\n",
      "Decoder Weight Norm (Mean): 0.17326794564723969\n",
      "MSE Loss: 0.0832, L1 Loss: 0.01*2.3788\n",
      "Batch number:  54\n",
      "Active Features: 83.69%\n",
      "Decoder Weight Norm (Mean): 0.17325277626514435\n",
      "MSE Loss: 0.0826, L1 Loss: 0.01*2.8754\n",
      "Batch number:  55\n",
      "Active Features: 82.03%\n",
      "Decoder Weight Norm (Mean): 0.17323610186576843\n",
      "MSE Loss: 0.0828, L1 Loss: 0.01*1.8444\n",
      "Batch number:  56\n",
      "Active Features: 81.03%\n",
      "Decoder Weight Norm (Mean): 0.17322072386741638\n",
      "MSE Loss: 0.0818, L1 Loss: 0.01*3.0613\n",
      "Batch number:  57\n",
      "Active Features: 81.23%\n",
      "Decoder Weight Norm (Mean): 0.1732034534215927\n",
      "MSE Loss: 0.0837, L1 Loss: 0.01*2.0156\n",
      "Batch number:  58\n",
      "Active Features: 80.05%\n",
      "Decoder Weight Norm (Mean): 0.17318713665008545\n",
      "MSE Loss: 0.0817, L1 Loss: 0.01*2.0094\n",
      "Batch number:  59\n",
      "Active Features: 80.72%\n",
      "Decoder Weight Norm (Mean): 0.17317193746566772\n",
      "MSE Loss: 0.0824, L1 Loss: 0.01*1.8469\n",
      "Batch number:  60\n",
      "Active Features: 81.02%\n",
      "Decoder Weight Norm (Mean): 0.1731581687927246\n",
      "MSE Loss: 0.0829, L1 Loss: 0.01*1.8340\n",
      "Batch number:  61\n",
      "Active Features: 81.90%\n",
      "Decoder Weight Norm (Mean): 0.17314539849758148\n",
      "MSE Loss: 0.0828, L1 Loss: 0.01*2.5138\n",
      "Batch number:  62\n",
      "Active Features: 80.69%\n",
      "Decoder Weight Norm (Mean): 0.17313234508037567\n",
      "MSE Loss: 0.0831, L1 Loss: 0.01*2.1597\n",
      "Batch number:  63\n",
      "Active Features: 81.11%\n",
      "Decoder Weight Norm (Mean): 0.1731199324131012\n",
      "MSE Loss: 0.0832, L1 Loss: 0.01*2.4796\n",
      "Batch number:  64\n",
      "Active Features: 83.25%\n",
      "Decoder Weight Norm (Mean): 0.17310678958892822\n",
      "MSE Loss: 0.0823, L1 Loss: 0.01*2.9849\n",
      "Batch number:  65\n",
      "Active Features: 80.27%\n",
      "Decoder Weight Norm (Mean): 0.1730917990207672\n",
      "MSE Loss: 0.0826, L1 Loss: 0.01*1.8134\n",
      "Batch number:  66\n",
      "Active Features: 80.11%\n",
      "Decoder Weight Norm (Mean): 0.17307822406291962\n",
      "MSE Loss: 0.0822, L1 Loss: 0.01*1.6519\n",
      "Batch number:  67\n",
      "Active Features: 78.11%\n",
      "Decoder Weight Norm (Mean): 0.17306610941886902\n",
      "MSE Loss: 0.0822, L1 Loss: 0.01*2.6518\n",
      "Batch number:  68\n",
      "Active Features: 81.31%\n",
      "Decoder Weight Norm (Mean): 0.17305314540863037\n",
      "MSE Loss: 0.0817, L1 Loss: 0.01*2.9566\n",
      "Batch number:  69\n",
      "Active Features: 82.08%\n",
      "Decoder Weight Norm (Mean): 0.17303846776485443\n",
      "MSE Loss: 0.0825, L1 Loss: 0.01*2.7820\n",
      "Batch number:  70\n",
      "Active Features: 81.20%\n",
      "Decoder Weight Norm (Mean): 0.1730225831270218\n",
      "MSE Loss: 0.0822, L1 Loss: 0.01*1.7923\n",
      "Batch number:  71\n",
      "Active Features: 79.91%\n",
      "Decoder Weight Norm (Mean): 0.17300806939601898\n",
      "MSE Loss: 0.0823, L1 Loss: 0.01*2.3065\n",
      "Batch number:  72\n",
      "Active Features: 80.46%\n",
      "Decoder Weight Norm (Mean): 0.17299367487430573\n",
      "MSE Loss: 0.0821, L1 Loss: 0.01*1.9667\n",
      "Batch number:  73\n",
      "Active Features: 78.42%\n",
      "Decoder Weight Norm (Mean): 0.17298008501529694\n",
      "MSE Loss: 0.0825, L1 Loss: 0.01*1.9745\n",
      "Batch number:  74\n",
      "Active Features: 78.81%\n",
      "Decoder Weight Norm (Mean): 0.17296725511550903\n",
      "MSE Loss: 0.0816, L1 Loss: 0.01*2.0039\n",
      "Batch number:  75\n",
      "Active Features: 81.86%\n",
      "Decoder Weight Norm (Mean): 0.1729552000761032\n",
      "MSE Loss: 0.0817, L1 Loss: 0.01*2.4375\n",
      "Batch number:  76\n",
      "Active Features: 82.03%\n",
      "Decoder Weight Norm (Mean): 0.1729424148797989\n",
      "MSE Loss: 0.0816, L1 Loss: 0.01*2.1433\n",
      "Batch number:  77\n",
      "Active Features: 79.91%\n",
      "Decoder Weight Norm (Mean): 0.1729297637939453\n",
      "MSE Loss: 0.0818, L1 Loss: 0.01*1.9446\n",
      "Batch number:  78\n",
      "Active Features: 82.16%\n",
      "Decoder Weight Norm (Mean): 0.17291778326034546\n",
      "MSE Loss: 0.0820, L1 Loss: 0.01*1.6531\n",
      "Batch number:  79\n",
      "Active Features: 79.42%\n",
      "Decoder Weight Norm (Mean): 0.1729070544242859\n",
      "MSE Loss: 0.0818, L1 Loss: 0.01*1.6745\n",
      "Batch number:  80\n",
      "Active Features: 78.67%\n",
      "Decoder Weight Norm (Mean): 0.17289777100086212\n",
      "MSE Loss: 0.0815, L1 Loss: 0.01*2.1224\n",
      "Batch number:  81\n",
      "Active Features: 78.75%\n",
      "Decoder Weight Norm (Mean): 0.17288844287395477\n",
      "MSE Loss: 0.0813, L1 Loss: 0.01*1.9653\n",
      "Batch number:  82\n",
      "Active Features: 79.81%\n",
      "Decoder Weight Norm (Mean): 0.17287951707839966\n",
      "MSE Loss: 0.0817, L1 Loss: 0.01*2.2677\n",
      "Batch number:  83\n",
      "Active Features: 82.23%\n",
      "Decoder Weight Norm (Mean): 0.1728702038526535\n",
      "MSE Loss: 0.0832, L1 Loss: 0.01*2.3854\n",
      "Batch number:  84\n",
      "Active Features: 78.15%\n",
      "Decoder Weight Norm (Mean): 0.17285998165607452\n",
      "MSE Loss: 0.0803, L1 Loss: 0.01*2.7011\n",
      "Batch number:  85\n",
      "Active Features: 78.53%\n",
      "Decoder Weight Norm (Mean): 0.1728483885526657\n",
      "MSE Loss: 0.0809, L1 Loss: 0.01*2.2807\n",
      "Batch number:  86\n",
      "Active Features: 78.20%\n",
      "Decoder Weight Norm (Mean): 0.17283658683300018\n",
      "MSE Loss: 0.0815, L1 Loss: 0.01*2.2541\n",
      "Batch number:  87\n",
      "Active Features: 76.37%\n",
      "Decoder Weight Norm (Mean): 0.17282478511333466\n",
      "MSE Loss: 0.0811, L1 Loss: 0.01*3.0322\n",
      "Batch number:  88\n",
      "Active Features: 77.52%\n",
      "Decoder Weight Norm (Mean): 0.1728111207485199\n",
      "MSE Loss: 0.0819, L1 Loss: 0.01*2.2280\n",
      "Batch number:  89\n",
      "Active Features: 76.87%\n",
      "Decoder Weight Norm (Mean): 0.17279760539531708\n",
      "MSE Loss: 0.0820, L1 Loss: 0.01*2.2424\n",
      "Batch number:  90\n",
      "Active Features: 81.22%\n",
      "Decoder Weight Norm (Mean): 0.17278406023979187\n",
      "MSE Loss: 0.0818, L1 Loss: 0.01*2.3694\n",
      "Batch number:  91\n",
      "Active Features: 79.35%\n",
      "Decoder Weight Norm (Mean): 0.1727699637413025\n",
      "MSE Loss: 0.0818, L1 Loss: 0.01*2.0797\n",
      "Batch number:  92\n",
      "Active Features: 77.70%\n",
      "Decoder Weight Norm (Mean): 0.1727563738822937\n",
      "MSE Loss: 0.0822, L1 Loss: 0.01*2.6974\n",
      "Batch number:  93\n",
      "Active Features: 79.25%\n",
      "Decoder Weight Norm (Mean): 0.17274178564548492\n",
      "MSE Loss: 0.0809, L1 Loss: 0.01*2.2306\n",
      "Batch number:  94\n",
      "Active Features: 77.28%\n",
      "Decoder Weight Norm (Mean): 0.17272751033306122\n",
      "MSE Loss: 0.0817, L1 Loss: 0.01*2.5307\n",
      "Batch number:  95\n",
      "Active Features: 75.99%\n",
      "Decoder Weight Norm (Mean): 0.17271263897418976\n",
      "MSE Loss: 0.0817, L1 Loss: 0.01*2.3950\n",
      "Batch number:  96\n",
      "Active Features: 77.72%\n",
      "Decoder Weight Norm (Mean): 0.17269764840602875\n",
      "MSE Loss: 0.0823, L1 Loss: 0.01*1.8046\n",
      "Batch number:  97\n",
      "Active Features: 77.72%\n",
      "Decoder Weight Norm (Mean): 0.1726842224597931\n",
      "MSE Loss: 0.0816, L1 Loss: 0.01*2.6478\n",
      "Batch number:  98\n",
      "Active Features: 81.81%\n",
      "Decoder Weight Norm (Mean): 0.17266975343227386\n",
      "MSE Loss: 0.0824, L1 Loss: 0.01*2.2046\n",
      "Batch number:  99\n",
      "Active Features: 77.57%\n",
      "Decoder Weight Norm (Mean): 0.17265529930591583\n",
      "MSE Loss: 0.0806, L1 Loss: 0.01*2.3369\n",
      "Batch number:  100\n",
      "Active Features: 79.22%\n",
      "Decoder Weight Norm (Mean): 0.1726405769586563\n",
      "MSE Loss: 0.0824, L1 Loss: 0.01*2.0651\n",
      "Batch number:  101\n",
      "Active Features: 75.94%\n",
      "Decoder Weight Norm (Mean): 0.17262640595436096\n",
      "MSE Loss: 0.0807, L1 Loss: 0.01*2.6289\n",
      "Batch number:  102\n",
      "Active Features: 78.94%\n",
      "Decoder Weight Norm (Mean): 0.1726112961769104\n",
      "MSE Loss: 0.0805, L1 Loss: 0.01*2.3017\n",
      "Batch number:  103\n",
      "Active Features: 77.83%\n",
      "Decoder Weight Norm (Mean): 0.1725960671901703\n",
      "MSE Loss: 0.0810, L1 Loss: 0.01*2.7592\n",
      "Batch number:  104\n",
      "Active Features: 75.03%\n",
      "Decoder Weight Norm (Mean): 0.17257963120937347\n",
      "MSE Loss: 0.0814, L1 Loss: 0.01*2.1921\n",
      "Batch number:  105\n",
      "Active Features: 78.55%\n",
      "Decoder Weight Norm (Mean): 0.17256374657154083\n",
      "MSE Loss: 0.0809, L1 Loss: 0.01*2.1595\n",
      "Batch number:  106\n",
      "Active Features: 75.41%\n",
      "Decoder Weight Norm (Mean): 0.17254826426506042\n",
      "MSE Loss: 0.0807, L1 Loss: 0.01*2.4791\n",
      "Batch number:  107\n",
      "Active Features: 77.35%\n",
      "Decoder Weight Norm (Mean): 0.1725325584411621\n",
      "MSE Loss: 0.0806, L1 Loss: 0.01*2.0306\n",
      "Batch number:  108\n",
      "Active Features: 75.33%\n",
      "Decoder Weight Norm (Mean): 0.17251770198345184\n",
      "MSE Loss: 0.0806, L1 Loss: 0.01*1.9004\n",
      "Batch number:  109\n",
      "Active Features: 76.80%\n",
      "Decoder Weight Norm (Mean): 0.17250394821166992\n",
      "MSE Loss: 0.0804, L1 Loss: 0.01*2.4277\n",
      "Batch number:  110\n",
      "Active Features: 78.39%\n",
      "Decoder Weight Norm (Mean): 0.1724896878004074\n",
      "MSE Loss: 0.0833, L1 Loss: 0.01*1.7463\n",
      "Batch number:  111\n",
      "Active Features: 76.42%\n",
      "Decoder Weight Norm (Mean): 0.17247697710990906\n",
      "MSE Loss: 0.0813, L1 Loss: 0.01*2.3155\n",
      "Batch number:  112\n",
      "Active Features: 80.03%\n",
      "Decoder Weight Norm (Mean): 0.17246408760547638\n",
      "MSE Loss: 0.0809, L1 Loss: 0.01*1.8993\n",
      "Batch number:  113\n",
      "Active Features: 75.99%\n",
      "Decoder Weight Norm (Mean): 0.17245206236839294\n",
      "MSE Loss: 0.0822, L1 Loss: 0.01*1.6398\n",
      "Batch number:  114\n",
      "Active Features: 75.46%\n",
      "Decoder Weight Norm (Mean): 0.17244157195091248\n",
      "MSE Loss: 0.0826, L1 Loss: 0.01*1.3164\n",
      "Batch number:  115\n",
      "Active Features: 77.51%\n",
      "Decoder Weight Norm (Mean): 0.17243334650993347\n",
      "MSE Loss: 0.0813, L1 Loss: 0.01*1.9066\n",
      "Batch number:  116\n",
      "Active Features: 76.25%\n",
      "Decoder Weight Norm (Mean): 0.17242556810379028\n",
      "MSE Loss: 0.0804, L1 Loss: 0.01*2.5444\n",
      "Batch number:  117\n",
      "Active Features: 77.58%\n",
      "Decoder Weight Norm (Mean): 0.1724162995815277\n",
      "MSE Loss: 0.0815, L1 Loss: 0.01*1.7647\n",
      "Batch number:  118\n",
      "Active Features: 76.87%\n",
      "Decoder Weight Norm (Mean): 0.17240795493125916\n",
      "MSE Loss: 0.0809, L1 Loss: 0.01*1.9925\n",
      "Batch number:  119\n",
      "Active Features: 77.28%\n",
      "Decoder Weight Norm (Mean): 0.17239977419376373\n",
      "MSE Loss: 0.0807, L1 Loss: 0.01*2.0075\n",
      "Batch number:  120\n",
      "Active Features: 74.78%\n",
      "Decoder Weight Norm (Mean): 0.17239169776439667\n",
      "MSE Loss: 0.0810, L1 Loss: 0.01*1.8795\n",
      "Batch number:  121\n",
      "Active Features: 74.24%\n",
      "Decoder Weight Norm (Mean): 0.1723840832710266\n",
      "MSE Loss: 0.0814, L1 Loss: 0.01*1.8883\n",
      "Batch number:  122\n",
      "Active Features: 76.89%\n",
      "Decoder Weight Norm (Mean): 0.1723770648241043\n",
      "MSE Loss: 0.0799, L1 Loss: 0.01*2.2509\n",
      "Batch number:  123\n",
      "Active Features: 77.64%\n",
      "Decoder Weight Norm (Mean): 0.17236924171447754\n",
      "MSE Loss: 0.0807, L1 Loss: 0.01*2.3988\n",
      "Batch number:  124\n",
      "Active Features: 73.88%\n",
      "Decoder Weight Norm (Mean): 0.17236049473285675\n",
      "MSE Loss: 0.0806, L1 Loss: 0.01*2.2540\n",
      "Batch number:  125\n",
      "Active Features: 76.49%\n",
      "Decoder Weight Norm (Mean): 0.17235136032104492\n",
      "MSE Loss: 0.0805, L1 Loss: 0.01*1.8895\n",
      "Batch number:  126\n",
      "Active Features: 74.80%\n",
      "Decoder Weight Norm (Mean): 0.17234279215335846\n",
      "MSE Loss: 0.0794, L1 Loss: 0.01*1.9875\n",
      "Batch number:  127\n",
      "Active Features: 77.00%\n",
      "Decoder Weight Norm (Mean): 0.17233425378799438\n",
      "MSE Loss: 0.0814, L1 Loss: 0.01*2.1059\n",
      "Epoch [5/10], Loss: 13.5154 ----------------------------\n",
      "Batch number:  0\n",
      "Active Features: 77.28%\n",
      "Decoder Weight Norm (Mean): 0.17232538759708405\n",
      "MSE Loss: 0.0814, L1 Loss: 0.01*1.6262\n",
      "Batch number:  1\n",
      "Active Features: 76.03%\n",
      "Decoder Weight Norm (Mean): 0.1723177433013916\n",
      "MSE Loss: 0.0799, L1 Loss: 0.01*2.2715\n",
      "Batch number:  2\n",
      "Active Features: 76.05%\n",
      "Decoder Weight Norm (Mean): 0.17230968177318573\n",
      "MSE Loss: 0.0809, L1 Loss: 0.01*2.3771\n",
      "Batch number:  3\n",
      "Active Features: 73.96%\n",
      "Decoder Weight Norm (Mean): 0.17230091989040375\n",
      "MSE Loss: 0.0800, L1 Loss: 0.01*2.2524\n",
      "Batch number:  4\n",
      "Active Features: 72.88%\n",
      "Decoder Weight Norm (Mean): 0.1722918599843979\n",
      "MSE Loss: 0.0799, L1 Loss: 0.01*2.1259\n",
      "Batch number:  5\n",
      "Active Features: 73.73%\n",
      "Decoder Weight Norm (Mean): 0.17228281497955322\n",
      "MSE Loss: 0.0818, L1 Loss: 0.01*2.2510\n",
      "Batch number:  6\n",
      "Active Features: 76.62%\n",
      "Decoder Weight Norm (Mean): 0.17227348685264587\n",
      "MSE Loss: 0.0809, L1 Loss: 0.01*1.8483\n",
      "Batch number:  7\n",
      "Active Features: 73.83%\n",
      "Decoder Weight Norm (Mean): 0.17226473987102509\n",
      "MSE Loss: 0.0807, L1 Loss: 0.01*2.0266\n",
      "Batch number:  8\n",
      "Active Features: 77.55%\n",
      "Decoder Weight Norm (Mean): 0.17225663363933563\n",
      "MSE Loss: 0.0798, L1 Loss: 0.01*2.2100\n",
      "Batch number:  9\n",
      "Active Features: 73.12%\n",
      "Decoder Weight Norm (Mean): 0.17224803566932678\n",
      "MSE Loss: 0.0803, L1 Loss: 0.01*2.2068\n",
      "Batch number:  10\n",
      "Active Features: 79.05%\n",
      "Decoder Weight Norm (Mean): 0.17223891615867615\n",
      "MSE Loss: 0.0809, L1 Loss: 0.01*2.0998\n",
      "Batch number:  11\n",
      "Active Features: 73.54%\n",
      "Decoder Weight Norm (Mean): 0.17222963273525238\n",
      "MSE Loss: 0.0803, L1 Loss: 0.01*2.1191\n",
      "Batch number:  12\n",
      "Active Features: 73.66%\n",
      "Decoder Weight Norm (Mean): 0.1722203642129898\n",
      "MSE Loss: 0.0803, L1 Loss: 0.01*1.7548\n",
      "Batch number:  13\n",
      "Active Features: 75.62%\n",
      "Decoder Weight Norm (Mean): 0.17221204936504364\n",
      "MSE Loss: 0.0805, L1 Loss: 0.01*2.1970\n",
      "Batch number:  14\n",
      "Active Features: 75.46%\n",
      "Decoder Weight Norm (Mean): 0.17220336198806763\n",
      "MSE Loss: 0.0800, L1 Loss: 0.01*1.6127\n",
      "Batch number:  15\n",
      "Active Features: 76.06%\n",
      "Decoder Weight Norm (Mean): 0.1721959114074707\n",
      "MSE Loss: 0.0807, L1 Loss: 0.01*1.8628\n",
      "Batch number:  16\n",
      "Active Features: 75.87%\n",
      "Decoder Weight Norm (Mean): 0.1721889078617096\n",
      "MSE Loss: 0.0809, L1 Loss: 0.01*2.0921\n",
      "Batch number:  17\n",
      "Active Features: 72.72%\n",
      "Decoder Weight Norm (Mean): 0.17218166589736938\n",
      "MSE Loss: 0.0806, L1 Loss: 0.01*2.2094\n",
      "Batch number:  18\n",
      "Active Features: 75.12%\n",
      "Decoder Weight Norm (Mean): 0.17217402160167694\n",
      "MSE Loss: 0.0804, L1 Loss: 0.01*1.6331\n",
      "Batch number:  19\n",
      "Active Features: 74.20%\n",
      "Decoder Weight Norm (Mean): 0.17216764390468597\n",
      "MSE Loss: 0.0813, L1 Loss: 0.01*2.4026\n",
      "Batch number:  20\n",
      "Active Features: 73.00%\n",
      "Decoder Weight Norm (Mean): 0.17216013371944427\n",
      "MSE Loss: 0.0803, L1 Loss: 0.01*1.6236\n",
      "Batch number:  21\n",
      "Active Features: 75.36%\n",
      "Decoder Weight Norm (Mean): 0.17215386033058167\n",
      "MSE Loss: 0.0807, L1 Loss: 0.01*1.7374\n",
      "Batch number:  22\n",
      "Active Features: 75.02%\n",
      "Decoder Weight Norm (Mean): 0.17214852571487427\n",
      "MSE Loss: 0.0800, L1 Loss: 0.01*2.3997\n",
      "Batch number:  23\n",
      "Active Features: 74.25%\n",
      "Decoder Weight Norm (Mean): 0.17214199900627136\n",
      "MSE Loss: 0.0812, L1 Loss: 0.01*2.3089\n",
      "Batch number:  24\n",
      "Active Features: 71.84%\n",
      "Decoder Weight Norm (Mean): 0.1721346527338028\n",
      "MSE Loss: 0.0809, L1 Loss: 0.01*1.6311\n",
      "Batch number:  25\n",
      "Active Features: 73.20%\n",
      "Decoder Weight Norm (Mean): 0.17212852835655212\n",
      "MSE Loss: 0.0804, L1 Loss: 0.01*2.0675\n",
      "Batch number:  26\n",
      "Active Features: 74.07%\n",
      "Decoder Weight Norm (Mean): 0.1721222847700119\n",
      "MSE Loss: 0.0800, L1 Loss: 0.01*2.1767\n",
      "Batch number:  27\n",
      "Active Features: 73.76%\n",
      "Decoder Weight Norm (Mean): 0.17211562395095825\n",
      "MSE Loss: 0.0796, L1 Loss: 0.01*1.5115\n",
      "Batch number:  28\n",
      "Active Features: 73.15%\n",
      "Decoder Weight Norm (Mean): 0.17211031913757324\n",
      "MSE Loss: 0.0806, L1 Loss: 0.01*2.4949\n",
      "Batch number:  29\n",
      "Active Features: 77.81%\n",
      "Decoder Weight Norm (Mean): 0.17210331559181213\n",
      "MSE Loss: 0.0805, L1 Loss: 0.01*2.0281\n",
      "Batch number:  30\n",
      "Active Features: 74.07%\n",
      "Decoder Weight Norm (Mean): 0.17209625244140625\n",
      "MSE Loss: 0.0802, L1 Loss: 0.01*1.8299\n",
      "Batch number:  31\n",
      "Active Features: 73.73%\n",
      "Decoder Weight Norm (Mean): 0.17208971083164215\n",
      "MSE Loss: 0.0796, L1 Loss: 0.01*1.5918\n",
      "Batch number:  32\n",
      "Active Features: 73.14%\n",
      "Decoder Weight Norm (Mean): 0.17208397388458252\n",
      "MSE Loss: 0.0808, L1 Loss: 0.01*1.8310\n",
      "Batch number:  33\n",
      "Active Features: 74.62%\n",
      "Decoder Weight Norm (Mean): 0.17207853496074677\n",
      "MSE Loss: 0.0803, L1 Loss: 0.01*1.5947\n",
      "Batch number:  34\n",
      "Active Features: 71.18%\n",
      "Decoder Weight Norm (Mean): 0.17207394540309906\n",
      "MSE Loss: 0.0790, L1 Loss: 0.01*2.1527\n",
      "Batch number:  35\n",
      "Active Features: 72.58%\n",
      "Decoder Weight Norm (Mean): 0.17206884920597076\n",
      "MSE Loss: 0.0791, L1 Loss: 0.01*2.0235\n",
      "Batch number:  36\n",
      "Active Features: 75.02%\n",
      "Decoder Weight Norm (Mean): 0.17206351459026337\n",
      "MSE Loss: 0.0801, L1 Loss: 0.01*1.8468\n",
      "Batch number:  37\n",
      "Active Features: 73.76%\n",
      "Decoder Weight Norm (Mean): 0.17205849289894104\n",
      "MSE Loss: 0.0802, L1 Loss: 0.01*1.8128\n",
      "Batch number:  38\n",
      "Active Features: 71.31%\n",
      "Decoder Weight Norm (Mean): 0.17205379903316498\n",
      "MSE Loss: 0.0802, L1 Loss: 0.01*2.1421\n",
      "Batch number:  39\n",
      "Active Features: 74.21%\n",
      "Decoder Weight Norm (Mean): 0.17204850912094116\n",
      "MSE Loss: 0.0797, L1 Loss: 0.01*2.3362\n",
      "Batch number:  40\n",
      "Active Features: 74.53%\n",
      "Decoder Weight Norm (Mean): 0.17204199731349945\n",
      "MSE Loss: 0.0798, L1 Loss: 0.01*2.1417\n",
      "Batch number:  41\n",
      "Active Features: 70.79%\n",
      "Decoder Weight Norm (Mean): 0.17203518748283386\n",
      "MSE Loss: 0.0796, L1 Loss: 0.01*1.6174\n",
      "Batch number:  42\n",
      "Active Features: 73.77%\n",
      "Decoder Weight Norm (Mean): 0.1720295548439026\n",
      "MSE Loss: 0.0792, L1 Loss: 0.01*2.0205\n",
      "Batch number:  43\n",
      "Active Features: 72.67%\n",
      "Decoder Weight Norm (Mean): 0.17202380299568176\n",
      "MSE Loss: 0.0791, L1 Loss: 0.01*2.2301\n",
      "Batch number:  44\n",
      "Active Features: 74.59%\n",
      "Decoder Weight Norm (Mean): 0.17201749980449677\n",
      "MSE Loss: 0.0803, L1 Loss: 0.01*1.7007\n",
      "Batch number:  45\n",
      "Active Features: 75.98%\n",
      "Decoder Weight Norm (Mean): 0.17201218008995056\n",
      "MSE Loss: 0.0803, L1 Loss: 0.01*2.4438\n",
      "Batch number:  46\n",
      "Active Features: 72.88%\n",
      "Decoder Weight Norm (Mean): 0.17200534045696259\n",
      "MSE Loss: 0.0803, L1 Loss: 0.01*2.1032\n",
      "Batch number:  47\n",
      "Active Features: 68.89%\n",
      "Decoder Weight Norm (Mean): 0.17199836671352386\n",
      "MSE Loss: 0.0786, L1 Loss: 0.01*2.3300\n",
      "Batch number:  48\n",
      "Active Features: 69.72%\n",
      "Decoder Weight Norm (Mean): 0.17199070751667023\n",
      "MSE Loss: 0.0792, L1 Loss: 0.01*2.3433\n",
      "Batch number:  49\n",
      "Active Features: 71.79%\n",
      "Decoder Weight Norm (Mean): 0.17198261618614197\n",
      "MSE Loss: 0.0798, L1 Loss: 0.01*2.0144\n",
      "Batch number:  50\n",
      "Active Features: 71.86%\n",
      "Decoder Weight Norm (Mean): 0.1719748079776764\n",
      "MSE Loss: 0.0800, L1 Loss: 0.01*1.7883\n",
      "Batch number:  51\n",
      "Active Features: 74.67%\n",
      "Decoder Weight Norm (Mean): 0.17196761071681976\n",
      "MSE Loss: 0.0795, L1 Loss: 0.01*2.0844\n",
      "Batch number:  52\n",
      "Active Features: 73.27%\n",
      "Decoder Weight Norm (Mean): 0.17196018993854523\n",
      "MSE Loss: 0.0793, L1 Loss: 0.01*2.0359\n",
      "Batch number:  53\n",
      "Active Features: 73.98%\n",
      "Decoder Weight Norm (Mean): 0.17195291817188263\n",
      "MSE Loss: 0.0794, L1 Loss: 0.01*2.4125\n",
      "Batch number:  54\n",
      "Active Features: 73.69%\n",
      "Decoder Weight Norm (Mean): 0.1719447523355484\n",
      "MSE Loss: 0.0792, L1 Loss: 0.01*1.6086\n",
      "Batch number:  55\n",
      "Active Features: 72.51%\n",
      "Decoder Weight Norm (Mean): 0.1719377338886261\n",
      "MSE Loss: 0.0792, L1 Loss: 0.01*1.5025\n",
      "Batch number:  56\n",
      "Active Features: 70.50%\n",
      "Decoder Weight Norm (Mean): 0.17193207144737244\n",
      "MSE Loss: 0.0792, L1 Loss: 0.01*1.7237\n",
      "Batch number:  57\n",
      "Active Features: 70.93%\n",
      "Decoder Weight Norm (Mean): 0.17192715406417847\n",
      "MSE Loss: 0.0798, L1 Loss: 0.01*1.9191\n",
      "Batch number:  58\n",
      "Active Features: 71.83%\n",
      "Decoder Weight Norm (Mean): 0.17192238569259644\n",
      "MSE Loss: 0.0795, L1 Loss: 0.01*2.3077\n",
      "Batch number:  59\n",
      "Active Features: 71.10%\n",
      "Decoder Weight Norm (Mean): 0.17191676795482635\n",
      "MSE Loss: 0.0792, L1 Loss: 0.01*2.2207\n",
      "Batch number:  60\n",
      "Active Features: 68.97%\n",
      "Decoder Weight Norm (Mean): 0.1719105839729309\n",
      "MSE Loss: 0.0792, L1 Loss: 0.01*2.3762\n",
      "Batch number:  61\n",
      "Active Features: 70.93%\n",
      "Decoder Weight Norm (Mean): 0.17190341651439667\n",
      "MSE Loss: 0.0797, L1 Loss: 0.01*1.9798\n",
      "Batch number:  62\n",
      "Active Features: 73.31%\n",
      "Decoder Weight Norm (Mean): 0.17189662158489227\n",
      "MSE Loss: 0.0795, L1 Loss: 0.01*1.9822\n",
      "Batch number:  63\n",
      "Active Features: 72.18%\n",
      "Decoder Weight Norm (Mean): 0.1718897968530655\n",
      "MSE Loss: 0.0799, L1 Loss: 0.01*1.6799\n",
      "Batch number:  64\n",
      "Active Features: 76.63%\n",
      "Decoder Weight Norm (Mean): 0.17188391089439392\n",
      "MSE Loss: 0.0792, L1 Loss: 0.01*1.9507\n",
      "Batch number:  65\n",
      "Active Features: 69.69%\n",
      "Decoder Weight Norm (Mean): 0.17187780141830444\n",
      "MSE Loss: 0.0796, L1 Loss: 0.01*1.7910\n",
      "Batch number:  66\n",
      "Active Features: 70.34%\n",
      "Decoder Weight Norm (Mean): 0.17187245190143585\n",
      "MSE Loss: 0.0792, L1 Loss: 0.01*2.1860\n",
      "Batch number:  67\n",
      "Active Features: 68.21%\n",
      "Decoder Weight Norm (Mean): 0.17186667025089264\n",
      "MSE Loss: 0.0796, L1 Loss: 0.01*1.7845\n",
      "Batch number:  68\n",
      "Active Features: 73.00%\n",
      "Decoder Weight Norm (Mean): 0.17186158895492554\n",
      "MSE Loss: 0.0788, L1 Loss: 0.01*1.5801\n",
      "Batch number:  69\n",
      "Active Features: 70.84%\n",
      "Decoder Weight Norm (Mean): 0.17185746133327484\n",
      "MSE Loss: 0.0794, L1 Loss: 0.01*1.9618\n",
      "Batch number:  70\n",
      "Active Features: 69.02%\n",
      "Decoder Weight Norm (Mean): 0.1718532294034958\n",
      "MSE Loss: 0.0789, L1 Loss: 0.01*1.6858\n",
      "Batch number:  71\n",
      "Active Features: 69.23%\n",
      "Decoder Weight Norm (Mean): 0.17184968292713165\n",
      "MSE Loss: 0.0794, L1 Loss: 0.01*2.0716\n",
      "Batch number:  72\n",
      "Active Features: 70.14%\n",
      "Decoder Weight Norm (Mean): 0.17184558510780334\n",
      "MSE Loss: 0.0791, L1 Loss: 0.01*2.0650\n",
      "Batch number:  73\n",
      "Active Features: 68.35%\n",
      "Decoder Weight Norm (Mean): 0.1718413233757019\n",
      "MSE Loss: 0.0793, L1 Loss: 0.01*1.7831\n",
      "Batch number:  74\n",
      "Active Features: 71.43%\n",
      "Decoder Weight Norm (Mean): 0.17183735966682434\n",
      "MSE Loss: 0.0784, L1 Loss: 0.01*1.8996\n",
      "Batch number:  75\n",
      "Active Features: 70.38%\n",
      "Decoder Weight Norm (Mean): 0.1718336045742035\n",
      "MSE Loss: 0.0789, L1 Loss: 0.01*1.8506\n",
      "Batch number:  76\n",
      "Active Features: 70.89%\n",
      "Decoder Weight Norm (Mean): 0.17182987928390503\n",
      "MSE Loss: 0.0784, L1 Loss: 0.01*1.9677\n",
      "Batch number:  77\n",
      "Active Features: 72.36%\n",
      "Decoder Weight Norm (Mean): 0.1718260496854782\n",
      "MSE Loss: 0.0799, L1 Loss: 0.01*1.8624\n",
      "Batch number:  78\n",
      "Active Features: 74.89%\n",
      "Decoder Weight Norm (Mean): 0.17182248830795288\n",
      "MSE Loss: 0.0800, L1 Loss: 0.01*1.7889\n",
      "Batch number:  79\n",
      "Active Features: 68.40%\n",
      "Decoder Weight Norm (Mean): 0.17181925475597382\n",
      "MSE Loss: 0.0791, L1 Loss: 0.01*2.0782\n",
      "Batch number:  80\n",
      "Active Features: 66.31%\n",
      "Decoder Weight Norm (Mean): 0.17181572318077087\n",
      "MSE Loss: 0.0785, L1 Loss: 0.01*1.8721\n",
      "Batch number:  81\n",
      "Active Features: 68.97%\n",
      "Decoder Weight Norm (Mean): 0.1718122810125351\n",
      "MSE Loss: 0.0788, L1 Loss: 0.01*1.8648\n",
      "Batch number:  82\n",
      "Active Features: 70.96%\n",
      "Decoder Weight Norm (Mean): 0.1718091368675232\n",
      "MSE Loss: 0.0789, L1 Loss: 0.01*1.8503\n",
      "Batch number:  83\n",
      "Active Features: 72.29%\n",
      "Decoder Weight Norm (Mean): 0.17180612683296204\n",
      "MSE Loss: 0.0805, L1 Loss: 0.01*2.2922\n",
      "Batch number:  84\n",
      "Active Features: 69.45%\n",
      "Decoder Weight Norm (Mean): 0.17180198431015015\n",
      "MSE Loss: 0.0779, L1 Loss: 0.01*1.8439\n",
      "Batch number:  85\n",
      "Active Features: 69.42%\n",
      "Decoder Weight Norm (Mean): 0.17179828882217407\n",
      "MSE Loss: 0.0782, L1 Loss: 0.01*2.1420\n",
      "Batch number:  86\n",
      "Active Features: 68.51%\n",
      "Decoder Weight Norm (Mean): 0.17179416120052338\n",
      "MSE Loss: 0.0779, L1 Loss: 0.01*2.4006\n",
      "Batch number:  87\n",
      "Active Features: 66.23%\n",
      "Decoder Weight Norm (Mean): 0.17178906500339508\n",
      "MSE Loss: 0.0781, L1 Loss: 0.01*1.8008\n",
      "Batch number:  88\n",
      "Active Features: 67.77%\n",
      "Decoder Weight Norm (Mean): 0.17178474366664886\n",
      "MSE Loss: 0.0790, L1 Loss: 0.01*2.1296\n",
      "Batch number:  89\n",
      "Active Features: 67.53%\n",
      "Decoder Weight Norm (Mean): 0.1717800796031952\n",
      "MSE Loss: 0.0788, L1 Loss: 0.01*1.8582\n",
      "Batch number:  90\n",
      "Active Features: 70.36%\n",
      "Decoder Weight Norm (Mean): 0.17177575826644897\n",
      "MSE Loss: 0.0786, L1 Loss: 0.01*1.8634\n",
      "Batch number:  91\n",
      "Active Features: 68.89%\n",
      "Decoder Weight Norm (Mean): 0.17177167534828186\n",
      "MSE Loss: 0.0782, L1 Loss: 0.01*2.0213\n",
      "Batch number:  92\n",
      "Active Features: 68.10%\n",
      "Decoder Weight Norm (Mean): 0.17176757752895355\n",
      "MSE Loss: 0.0792, L1 Loss: 0.01*2.2227\n",
      "Batch number:  93\n",
      "Active Features: 70.94%\n",
      "Decoder Weight Norm (Mean): 0.17176291346549988\n",
      "MSE Loss: 0.0783, L1 Loss: 0.01*1.9384\n",
      "Batch number:  94\n",
      "Active Features: 64.64%\n",
      "Decoder Weight Norm (Mean): 0.17175844311714172\n",
      "MSE Loss: 0.0787, L1 Loss: 0.01*1.7678\n",
      "Batch number:  95\n",
      "Active Features: 65.97%\n",
      "Decoder Weight Norm (Mean): 0.1717546284198761\n",
      "MSE Loss: 0.0795, L1 Loss: 0.01*1.9526\n",
      "Batch number:  96\n",
      "Active Features: 66.64%\n",
      "Decoder Weight Norm (Mean): 0.17175090312957764\n",
      "MSE Loss: 0.0793, L1 Loss: 0.01*1.8604\n",
      "Batch number:  97\n",
      "Active Features: 69.17%\n",
      "Decoder Weight Norm (Mean): 0.1717475801706314\n",
      "MSE Loss: 0.0781, L1 Loss: 0.01*1.7667\n",
      "Batch number:  98\n",
      "Active Features: 67.52%\n",
      "Decoder Weight Norm (Mean): 0.17174464464187622\n",
      "MSE Loss: 0.0785, L1 Loss: 0.01*2.0079\n",
      "Batch number:  99\n",
      "Active Features: 66.21%\n",
      "Decoder Weight Norm (Mean): 0.17174161970615387\n",
      "MSE Loss: 0.0785, L1 Loss: 0.01*2.3304\n",
      "Batch number:  100\n",
      "Active Features: 68.32%\n",
      "Decoder Weight Norm (Mean): 0.17173755168914795\n",
      "MSE Loss: 0.0801, L1 Loss: 0.01*2.1736\n",
      "Batch number:  101\n",
      "Active Features: 68.59%\n",
      "Decoder Weight Norm (Mean): 0.17173311114311218\n",
      "MSE Loss: 0.0789, L1 Loss: 0.01*1.9413\n",
      "Batch number:  102\n",
      "Active Features: 68.67%\n",
      "Decoder Weight Norm (Mean): 0.17172887921333313\n",
      "MSE Loss: 0.0778, L1 Loss: 0.01*1.8877\n",
      "Batch number:  103\n",
      "Active Features: 68.41%\n",
      "Decoder Weight Norm (Mean): 0.17172512412071228\n",
      "MSE Loss: 0.0789, L1 Loss: 0.01*2.2656\n",
      "Batch number:  104\n",
      "Active Features: 66.04%\n",
      "Decoder Weight Norm (Mean): 0.17172084748744965\n",
      "MSE Loss: 0.0791, L1 Loss: 0.01*1.9174\n",
      "Batch number:  105\n",
      "Active Features: 68.75%\n",
      "Decoder Weight Norm (Mean): 0.1717168092727661\n",
      "MSE Loss: 0.0783, L1 Loss: 0.01*2.0705\n",
      "Batch number:  106\n",
      "Active Features: 63.91%\n",
      "Decoder Weight Norm (Mean): 0.17171262204647064\n",
      "MSE Loss: 0.0775, L1 Loss: 0.01*2.0222\n",
      "Batch number:  107\n",
      "Active Features: 68.02%\n",
      "Decoder Weight Norm (Mean): 0.17170871794223785\n",
      "MSE Loss: 0.0784, L1 Loss: 0.01*2.1423\n",
      "Batch number:  108\n",
      "Active Features: 64.65%\n",
      "Decoder Weight Norm (Mean): 0.17170433700084686\n",
      "MSE Loss: 0.0771, L1 Loss: 0.01*1.9256\n",
      "Batch number:  109\n",
      "Active Features: 68.71%\n",
      "Decoder Weight Norm (Mean): 0.17170022428035736\n",
      "MSE Loss: 0.0787, L1 Loss: 0.01*1.9691\n",
      "Batch number:  110\n",
      "Active Features: 68.09%\n",
      "Decoder Weight Norm (Mean): 0.17169620096683502\n",
      "MSE Loss: 0.0801, L1 Loss: 0.01*1.9838\n",
      "Batch number:  111\n",
      "Active Features: 67.64%\n",
      "Decoder Weight Norm (Mean): 0.17169208824634552\n",
      "MSE Loss: 0.0790, L1 Loss: 0.01*1.8562\n",
      "Batch number:  112\n",
      "Active Features: 70.02%\n",
      "Decoder Weight Norm (Mean): 0.17168830335140228\n",
      "MSE Loss: 0.0780, L1 Loss: 0.01*2.1646\n",
      "Batch number:  113\n",
      "Active Features: 66.94%\n",
      "Decoder Weight Norm (Mean): 0.17168410122394562\n",
      "MSE Loss: 0.0803, L1 Loss: 0.01*1.8333\n",
      "Batch number:  114\n",
      "Active Features: 68.69%\n",
      "Decoder Weight Norm (Mean): 0.17168058454990387\n",
      "MSE Loss: 0.0797, L1 Loss: 0.01*1.7950\n",
      "Batch number:  115\n",
      "Active Features: 69.12%\n",
      "Decoder Weight Norm (Mean): 0.17167732119560242\n",
      "MSE Loss: 0.0780, L1 Loss: 0.01*1.8254\n",
      "Batch number:  116\n",
      "Active Features: 65.30%\n",
      "Decoder Weight Norm (Mean): 0.17167434096336365\n",
      "MSE Loss: 0.0771, L1 Loss: 0.01*1.5731\n",
      "Batch number:  117\n",
      "Active Features: 67.31%\n",
      "Decoder Weight Norm (Mean): 0.17167243361473083\n",
      "MSE Loss: 0.0786, L1 Loss: 0.01*1.7589\n",
      "Batch number:  118\n",
      "Active Features: 67.25%\n",
      "Decoder Weight Norm (Mean): 0.17167076468467712\n",
      "MSE Loss: 0.0787, L1 Loss: 0.01*1.5761\n",
      "Batch number:  119\n",
      "Active Features: 67.01%\n",
      "Decoder Weight Norm (Mean): 0.1716700792312622\n",
      "MSE Loss: 0.0782, L1 Loss: 0.01*1.9659\n",
      "Batch number:  120\n",
      "Active Features: 65.55%\n",
      "Decoder Weight Norm (Mean): 0.17166906595230103\n",
      "MSE Loss: 0.0787, L1 Loss: 0.01*1.8322\n",
      "Batch number:  121\n",
      "Active Features: 65.39%\n",
      "Decoder Weight Norm (Mean): 0.17166823148727417\n",
      "MSE Loss: 0.0789, L1 Loss: 0.01*1.7473\n",
      "Batch number:  122\n",
      "Active Features: 67.03%\n",
      "Decoder Weight Norm (Mean): 0.17166781425476074\n",
      "MSE Loss: 0.0784, L1 Loss: 0.01*1.8078\n",
      "Batch number:  123\n",
      "Active Features: 71.70%\n",
      "Decoder Weight Norm (Mean): 0.1716674268245697\n",
      "MSE Loss: 0.0782, L1 Loss: 0.01*1.8904\n",
      "Batch number:  124\n",
      "Active Features: 63.45%\n",
      "Decoder Weight Norm (Mean): 0.17166665196418762\n",
      "MSE Loss: 0.0786, L1 Loss: 0.01*1.8371\n",
      "Batch number:  125\n",
      "Active Features: 66.81%\n",
      "Decoder Weight Norm (Mean): 0.17166616022586823\n",
      "MSE Loss: 0.0774, L1 Loss: 0.01*1.9215\n",
      "Batch number:  126\n",
      "Active Features: 66.20%\n",
      "Decoder Weight Norm (Mean): 0.17166557908058167\n",
      "MSE Loss: 0.0773, L1 Loss: 0.01*2.2655\n",
      "Batch number:  127\n",
      "Active Features: 70.10%\n",
      "Decoder Weight Norm (Mean): 0.17166398465633392\n",
      "MSE Loss: 0.0783, L1 Loss: 0.01*1.8052\n",
      "Epoch [6/10], Loss: 12.6902 ----------------------------\n",
      "Batch number:  0\n",
      "Active Features: 70.08%\n",
      "Decoder Weight Norm (Mean): 0.1716625690460205\n",
      "MSE Loss: 0.0783, L1 Loss: 0.01*1.9851\n",
      "Batch number:  1\n",
      "Active Features: 65.88%\n",
      "Decoder Weight Norm (Mean): 0.17166094481945038\n",
      "MSE Loss: 0.0773, L1 Loss: 0.01*1.9914\n",
      "Batch number:  2\n",
      "Active Features: 65.55%\n",
      "Decoder Weight Norm (Mean): 0.17165930569171906\n",
      "MSE Loss: 0.0787, L1 Loss: 0.01*1.9888\n",
      "Batch number:  3\n",
      "Active Features: 63.93%\n",
      "Decoder Weight Norm (Mean): 0.17165783047676086\n",
      "MSE Loss: 0.0770, L1 Loss: 0.01*2.0627\n",
      "Batch number:  4\n",
      "Active Features: 62.21%\n",
      "Decoder Weight Norm (Mean): 0.1716562956571579\n",
      "MSE Loss: 0.0777, L1 Loss: 0.01*1.6689\n",
      "Batch number:  5\n",
      "Active Features: 64.42%\n",
      "Decoder Weight Norm (Mean): 0.17165546119213104\n",
      "MSE Loss: 0.0787, L1 Loss: 0.01*1.5313\n",
      "Batch number:  6\n",
      "Active Features: 67.05%\n",
      "Decoder Weight Norm (Mean): 0.17165562510490417\n",
      "MSE Loss: 0.0789, L1 Loss: 0.01*1.8012\n",
      "Batch number:  7\n",
      "Active Features: 66.95%\n",
      "Decoder Weight Norm (Mean): 0.17165592312812805\n",
      "MSE Loss: 0.0792, L1 Loss: 0.01*2.0069\n",
      "Batch number:  8\n",
      "Active Features: 68.99%\n",
      "Decoder Weight Norm (Mean): 0.17165595293045044\n",
      "MSE Loss: 0.0775, L1 Loss: 0.01*2.0236\n",
      "Batch number:  9\n",
      "Active Features: 66.64%\n",
      "Decoder Weight Norm (Mean): 0.17165564000606537\n",
      "MSE Loss: 0.0772, L1 Loss: 0.01*1.6003\n",
      "Batch number:  10\n",
      "Active Features: 67.35%\n",
      "Decoder Weight Norm (Mean): 0.17165622115135193\n",
      "MSE Loss: 0.0779, L1 Loss: 0.01*1.9581\n",
      "Batch number:  11\n",
      "Active Features: 64.92%\n",
      "Decoder Weight Norm (Mean): 0.1716565191745758\n",
      "MSE Loss: 0.0780, L1 Loss: 0.01*1.8182\n",
      "Batch number:  12\n",
      "Active Features: 64.69%\n",
      "Decoder Weight Norm (Mean): 0.1716568022966385\n",
      "MSE Loss: 0.0777, L1 Loss: 0.01*2.0313\n",
      "Batch number:  13\n",
      "Active Features: 67.19%\n",
      "Decoder Weight Norm (Mean): 0.17165672779083252\n",
      "MSE Loss: 0.0775, L1 Loss: 0.01*2.0121\n",
      "Batch number:  14\n",
      "Active Features: 65.32%\n",
      "Decoder Weight Norm (Mean): 0.17165623605251312\n",
      "MSE Loss: 0.0776, L1 Loss: 0.01*1.9326\n",
      "Batch number:  15\n",
      "Active Features: 66.05%\n",
      "Decoder Weight Norm (Mean): 0.17165564000606537\n",
      "MSE Loss: 0.0775, L1 Loss: 0.01*1.8073\n",
      "Batch number:  16\n",
      "Active Features: 66.01%\n",
      "Decoder Weight Norm (Mean): 0.17165523767471313\n",
      "MSE Loss: 0.0782, L1 Loss: 0.01*1.8885\n",
      "Batch number:  17\n",
      "Active Features: 63.87%\n",
      "Decoder Weight Norm (Mean): 0.1716548502445221\n",
      "MSE Loss: 0.0782, L1 Loss: 0.01*1.8390\n",
      "Batch number:  18\n",
      "Active Features: 66.25%\n",
      "Decoder Weight Norm (Mean): 0.17165477573871613\n",
      "MSE Loss: 0.0778, L1 Loss: 0.01*1.9320\n",
      "Batch number:  19\n",
      "Active Features: 65.52%\n",
      "Decoder Weight Norm (Mean): 0.17165468633174896\n",
      "MSE Loss: 0.0792, L1 Loss: 0.01*1.7978\n",
      "Batch number:  20\n",
      "Active Features: 64.72%\n",
      "Decoder Weight Norm (Mean): 0.1716548353433609\n",
      "MSE Loss: 0.0786, L1 Loss: 0.01*1.6107\n",
      "Batch number:  21\n",
      "Active Features: 63.73%\n",
      "Decoder Weight Norm (Mean): 0.17165565490722656\n",
      "MSE Loss: 0.0784, L1 Loss: 0.01*1.6718\n",
      "Batch number:  22\n",
      "Active Features: 63.47%\n",
      "Decoder Weight Norm (Mean): 0.17165721952915192\n",
      "MSE Loss: 0.0773, L1 Loss: 0.01*1.8722\n",
      "Batch number:  23\n",
      "Active Features: 65.40%\n",
      "Decoder Weight Norm (Mean): 0.17165878415107727\n",
      "MSE Loss: 0.0790, L1 Loss: 0.01*1.6044\n",
      "Batch number:  24\n",
      "Active Features: 65.15%\n",
      "Decoder Weight Norm (Mean): 0.17166073620319366\n",
      "MSE Loss: 0.0788, L1 Loss: 0.01*1.8186\n",
      "Batch number:  25\n",
      "Active Features: 65.16%\n",
      "Decoder Weight Norm (Mean): 0.17166276276111603\n",
      "MSE Loss: 0.0781, L1 Loss: 0.01*1.7383\n",
      "Batch number:  26\n",
      "Active Features: 64.80%\n",
      "Decoder Weight Norm (Mean): 0.17166514694690704\n",
      "MSE Loss: 0.0772, L1 Loss: 0.01*1.8830\n",
      "Batch number:  27\n",
      "Active Features: 62.52%\n",
      "Decoder Weight Norm (Mean): 0.17166739702224731\n",
      "MSE Loss: 0.0760, L1 Loss: 0.01*2.0103\n",
      "Batch number:  28\n",
      "Active Features: 64.24%\n",
      "Decoder Weight Norm (Mean): 0.1716693639755249\n",
      "MSE Loss: 0.0782, L1 Loss: 0.01*1.7992\n",
      "Batch number:  29\n",
      "Active Features: 70.64%\n",
      "Decoder Weight Norm (Mean): 0.17167158424854279\n",
      "MSE Loss: 0.0777, L1 Loss: 0.01*1.6942\n",
      "Batch number:  30\n",
      "Active Features: 62.22%\n",
      "Decoder Weight Norm (Mean): 0.17167380452156067\n",
      "MSE Loss: 0.0777, L1 Loss: 0.01*1.7349\n",
      "Batch number:  31\n",
      "Active Features: 66.57%\n",
      "Decoder Weight Norm (Mean): 0.17167621850967407\n",
      "MSE Loss: 0.0777, L1 Loss: 0.01*1.7915\n",
      "Batch number:  32\n",
      "Active Features: 63.18%\n",
      "Decoder Weight Norm (Mean): 0.17167846858501434\n",
      "MSE Loss: 0.0782, L1 Loss: 0.01*2.0861\n",
      "Batch number:  33\n",
      "Active Features: 64.92%\n",
      "Decoder Weight Norm (Mean): 0.17168019711971283\n",
      "MSE Loss: 0.0773, L1 Loss: 0.01*1.8016\n",
      "Batch number:  34\n",
      "Active Features: 59.99%\n",
      "Decoder Weight Norm (Mean): 0.1716819554567337\n",
      "MSE Loss: 0.0768, L1 Loss: 0.01*1.8212\n",
      "Batch number:  35\n",
      "Active Features: 67.30%\n",
      "Decoder Weight Norm (Mean): 0.17168404161930084\n",
      "MSE Loss: 0.0773, L1 Loss: 0.01*1.9128\n",
      "Batch number:  36\n",
      "Active Features: 64.77%\n",
      "Decoder Weight Norm (Mean): 0.17168596386909485\n",
      "MSE Loss: 0.0781, L1 Loss: 0.01*2.0203\n",
      "Batch number:  37\n",
      "Active Features: 64.11%\n",
      "Decoder Weight Norm (Mean): 0.17168757319450378\n",
      "MSE Loss: 0.0783, L1 Loss: 0.01*1.8693\n",
      "Batch number:  38\n",
      "Active Features: 62.05%\n",
      "Decoder Weight Norm (Mean): 0.17168912291526794\n",
      "MSE Loss: 0.0778, L1 Loss: 0.01*1.7995\n",
      "Batch number:  39\n",
      "Active Features: 63.95%\n",
      "Decoder Weight Norm (Mean): 0.17169080674648285\n",
      "MSE Loss: 0.0777, L1 Loss: 0.01*1.9264\n",
      "Batch number:  40\n",
      "Active Features: 63.29%\n",
      "Decoder Weight Norm (Mean): 0.1716926097869873\n",
      "MSE Loss: 0.0781, L1 Loss: 0.01*1.7232\n",
      "Batch number:  41\n",
      "Active Features: 62.47%\n",
      "Decoder Weight Norm (Mean): 0.17169466614723206\n",
      "MSE Loss: 0.0776, L1 Loss: 0.01*2.0701\n",
      "Batch number:  42\n",
      "Active Features: 64.66%\n",
      "Decoder Weight Norm (Mean): 0.17169636487960815\n",
      "MSE Loss: 0.0773, L1 Loss: 0.01*1.7273\n",
      "Batch number:  43\n",
      "Active Features: 63.83%\n",
      "Decoder Weight Norm (Mean): 0.17169836163520813\n",
      "MSE Loss: 0.0773, L1 Loss: 0.01*1.8017\n",
      "Batch number:  44\n",
      "Active Features: 67.44%\n",
      "Decoder Weight Norm (Mean): 0.1717003583908081\n",
      "MSE Loss: 0.0777, L1 Loss: 0.01*2.1108\n",
      "Batch number:  45\n",
      "Active Features: 65.17%\n",
      "Decoder Weight Norm (Mean): 0.17170169949531555\n",
      "MSE Loss: 0.0780, L1 Loss: 0.01*1.5480\n",
      "Batch number:  46\n",
      "Active Features: 65.31%\n",
      "Decoder Weight Norm (Mean): 0.17170356214046478\n",
      "MSE Loss: 0.0782, L1 Loss: 0.01*1.7723\n",
      "Batch number:  47\n",
      "Active Features: 62.75%\n",
      "Decoder Weight Norm (Mean): 0.17170536518096924\n",
      "MSE Loss: 0.0759, L1 Loss: 0.01*1.9986\n",
      "Batch number:  48\n",
      "Active Features: 63.90%\n",
      "Decoder Weight Norm (Mean): 0.17170709371566772\n",
      "MSE Loss: 0.0785, L1 Loss: 0.01*1.7853\n",
      "Batch number:  49\n",
      "Active Features: 63.06%\n",
      "Decoder Weight Norm (Mean): 0.17170901596546173\n",
      "MSE Loss: 0.0773, L1 Loss: 0.01*1.9069\n",
      "Batch number:  50\n",
      "Active Features: 62.42%\n",
      "Decoder Weight Norm (Mean): 0.17171071469783783\n",
      "MSE Loss: 0.0770, L1 Loss: 0.01*1.8427\n",
      "Batch number:  51\n",
      "Active Features: 66.19%\n",
      "Decoder Weight Norm (Mean): 0.17171239852905273\n",
      "MSE Loss: 0.0781, L1 Loss: 0.01*1.7588\n",
      "Batch number:  52\n",
      "Active Features: 61.30%\n",
      "Decoder Weight Norm (Mean): 0.17171411216259003\n",
      "MSE Loss: 0.0777, L1 Loss: 0.01*1.6764\n",
      "Batch number:  53\n",
      "Active Features: 64.24%\n",
      "Decoder Weight Norm (Mean): 0.17171624302864075\n",
      "MSE Loss: 0.0775, L1 Loss: 0.01*1.7213\n",
      "Batch number:  54\n",
      "Active Features: 62.80%\n",
      "Decoder Weight Norm (Mean): 0.171718567609787\n",
      "MSE Loss: 0.0763, L1 Loss: 0.01*1.9220\n",
      "Batch number:  55\n",
      "Active Features: 60.61%\n",
      "Decoder Weight Norm (Mean): 0.17172111570835114\n",
      "MSE Loss: 0.0777, L1 Loss: 0.01*1.5805\n",
      "Batch number:  56\n",
      "Active Features: 61.32%\n",
      "Decoder Weight Norm (Mean): 0.17172421514987946\n",
      "MSE Loss: 0.0769, L1 Loss: 0.01*1.7894\n",
      "Batch number:  57\n",
      "Active Features: 66.25%\n",
      "Decoder Weight Norm (Mean): 0.17172721028327942\n",
      "MSE Loss: 0.0787, L1 Loss: 0.01*1.7806\n",
      "Batch number:  58\n",
      "Active Features: 63.12%\n",
      "Decoder Weight Norm (Mean): 0.17173010110855103\n",
      "MSE Loss: 0.0770, L1 Loss: 0.01*1.7290\n",
      "Batch number:  59\n",
      "Active Features: 61.47%\n",
      "Decoder Weight Norm (Mean): 0.17173324525356293\n",
      "MSE Loss: 0.0772, L1 Loss: 0.01*1.7315\n",
      "Batch number:  60\n",
      "Active Features: 61.74%\n",
      "Decoder Weight Norm (Mean): 0.17173653841018677\n",
      "MSE Loss: 0.0779, L1 Loss: 0.01*1.9301\n",
      "Batch number:  61\n",
      "Active Features: 62.19%\n",
      "Decoder Weight Norm (Mean): 0.17173977196216583\n",
      "MSE Loss: 0.0776, L1 Loss: 0.01*1.7151\n",
      "Batch number:  62\n",
      "Active Features: 63.69%\n",
      "Decoder Weight Norm (Mean): 0.17174330353736877\n",
      "MSE Loss: 0.0769, L1 Loss: 0.01*1.7909\n",
      "Batch number:  63\n",
      "Active Features: 62.09%\n",
      "Decoder Weight Norm (Mean): 0.17174679040908813\n",
      "MSE Loss: 0.0780, L1 Loss: 0.01*1.8508\n",
      "Batch number:  64\n",
      "Active Features: 68.62%\n",
      "Decoder Weight Norm (Mean): 0.17175033688545227\n",
      "MSE Loss: 0.0775, L1 Loss: 0.01*2.0731\n",
      "Batch number:  65\n",
      "Active Features: 61.58%\n",
      "Decoder Weight Norm (Mean): 0.17175334692001343\n",
      "MSE Loss: 0.0781, L1 Loss: 0.01*1.7004\n",
      "Batch number:  66\n",
      "Active Features: 59.41%\n",
      "Decoder Weight Norm (Mean): 0.17175665497779846\n",
      "MSE Loss: 0.0772, L1 Loss: 0.01*1.7873\n",
      "Batch number:  67\n",
      "Active Features: 59.55%\n",
      "Decoder Weight Norm (Mean): 0.17176006734371185\n",
      "MSE Loss: 0.0759, L1 Loss: 0.01*1.9208\n",
      "Batch number:  68\n",
      "Active Features: 63.97%\n",
      "Decoder Weight Norm (Mean): 0.17176349461078644\n",
      "MSE Loss: 0.0771, L1 Loss: 0.01*1.8242\n",
      "Batch number:  69\n",
      "Active Features: 63.59%\n",
      "Decoder Weight Norm (Mean): 0.1717669814825058\n",
      "MSE Loss: 0.0768, L1 Loss: 0.01*1.5093\n",
      "Batch number:  70\n",
      "Active Features: 62.33%\n",
      "Decoder Weight Norm (Mean): 0.17177090048789978\n",
      "MSE Loss: 0.0771, L1 Loss: 0.01*1.7133\n",
      "Batch number:  71\n",
      "Active Features: 63.70%\n",
      "Decoder Weight Norm (Mean): 0.1717749536037445\n",
      "MSE Loss: 0.0776, L1 Loss: 0.01*1.6554\n",
      "Batch number:  72\n",
      "Active Features: 58.68%\n",
      "Decoder Weight Norm (Mean): 0.17177928984165192\n",
      "MSE Loss: 0.0777, L1 Loss: 0.01*1.6582\n",
      "Batch number:  73\n",
      "Active Features: 60.71%\n",
      "Decoder Weight Norm (Mean): 0.17178389430046082\n",
      "MSE Loss: 0.0769, L1 Loss: 0.01*1.7108\n",
      "Batch number:  74\n",
      "Active Features: 59.69%\n",
      "Decoder Weight Norm (Mean): 0.17178869247436523\n",
      "MSE Loss: 0.0760, L1 Loss: 0.01*1.8815\n",
      "Batch number:  75\n",
      "Active Features: 63.83%\n",
      "Decoder Weight Norm (Mean): 0.17179358005523682\n",
      "MSE Loss: 0.0774, L1 Loss: 0.01*1.6906\n",
      "Batch number:  76\n",
      "Active Features: 65.75%\n",
      "Decoder Weight Norm (Mean): 0.17179857194423676\n",
      "MSE Loss: 0.0764, L1 Loss: 0.01*2.0392\n",
      "Batch number:  77\n",
      "Active Features: 62.64%\n",
      "Decoder Weight Norm (Mean): 0.1718028485774994\n",
      "MSE Loss: 0.0764, L1 Loss: 0.01*2.0401\n",
      "Batch number:  78\n",
      "Active Features: 64.35%\n",
      "Decoder Weight Norm (Mean): 0.17180675268173218\n",
      "MSE Loss: 0.0768, L1 Loss: 0.01*1.8576\n",
      "Batch number:  79\n",
      "Active Features: 59.57%\n",
      "Decoder Weight Norm (Mean): 0.1718105673789978\n",
      "MSE Loss: 0.0769, L1 Loss: 0.01*1.6629\n",
      "Batch number:  80\n",
      "Active Features: 59.71%\n",
      "Decoder Weight Norm (Mean): 0.17181451618671417\n",
      "MSE Loss: 0.0765, L1 Loss: 0.01*1.9094\n",
      "Batch number:  81\n",
      "Active Features: 61.18%\n",
      "Decoder Weight Norm (Mean): 0.1718183010816574\n",
      "MSE Loss: 0.0765, L1 Loss: 0.01*1.6644\n",
      "Batch number:  82\n",
      "Active Features: 61.35%\n",
      "Decoder Weight Norm (Mean): 0.17182232439517975\n",
      "MSE Loss: 0.0767, L1 Loss: 0.01*1.9661\n",
      "Batch number:  83\n",
      "Active Features: 65.45%\n",
      "Decoder Weight Norm (Mean): 0.17182615399360657\n",
      "MSE Loss: 0.0781, L1 Loss: 0.01*1.8738\n",
      "Batch number:  84\n",
      "Active Features: 59.05%\n",
      "Decoder Weight Norm (Mean): 0.1718296855688095\n",
      "MSE Loss: 0.0754, L1 Loss: 0.01*2.0692\n",
      "Batch number:  85\n",
      "Active Features: 58.84%\n",
      "Decoder Weight Norm (Mean): 0.17183318734169006\n",
      "MSE Loss: 0.0766, L1 Loss: 0.01*1.7937\n",
      "Batch number:  86\n",
      "Active Features: 59.46%\n",
      "Decoder Weight Norm (Mean): 0.17183683812618256\n",
      "MSE Loss: 0.0764, L1 Loss: 0.01*1.7290\n",
      "Batch number:  87\n",
      "Active Features: 56.93%\n",
      "Decoder Weight Norm (Mean): 0.17184074223041534\n",
      "MSE Loss: 0.0759, L1 Loss: 0.01*1.8603\n",
      "Batch number:  88\n",
      "Active Features: 57.87%\n",
      "Decoder Weight Norm (Mean): 0.1718447506427765\n",
      "MSE Loss: 0.0769, L1 Loss: 0.01*1.9564\n",
      "Batch number:  89\n",
      "Active Features: 59.11%\n",
      "Decoder Weight Norm (Mean): 0.17184872925281525\n",
      "MSE Loss: 0.0763, L1 Loss: 0.01*1.8552\n",
      "Batch number:  90\n",
      "Active Features: 62.07%\n",
      "Decoder Weight Norm (Mean): 0.17185261845588684\n",
      "MSE Loss: 0.0767, L1 Loss: 0.01*1.7823\n",
      "Batch number:  91\n",
      "Active Features: 62.07%\n",
      "Decoder Weight Norm (Mean): 0.17185668647289276\n",
      "MSE Loss: 0.0763, L1 Loss: 0.01*2.0966\n",
      "Batch number:  92\n",
      "Active Features: 58.76%\n",
      "Decoder Weight Norm (Mean): 0.171860471367836\n",
      "MSE Loss: 0.0769, L1 Loss: 0.01*1.6841\n",
      "Batch number:  93\n",
      "Active Features: 62.09%\n",
      "Decoder Weight Norm (Mean): 0.1718645840883255\n",
      "MSE Loss: 0.0763, L1 Loss: 0.01*1.7922\n",
      "Batch number:  94\n",
      "Active Features: 57.97%\n",
      "Decoder Weight Norm (Mean): 0.17186865210533142\n",
      "MSE Loss: 0.0771, L1 Loss: 0.01*1.8369\n",
      "Batch number:  95\n",
      "Active Features: 55.19%\n",
      "Decoder Weight Norm (Mean): 0.17187324166297913\n",
      "MSE Loss: 0.0763, L1 Loss: 0.01*1.8671\n",
      "Batch number:  96\n",
      "Active Features: 59.98%\n",
      "Decoder Weight Norm (Mean): 0.171877920627594\n",
      "MSE Loss: 0.0762, L1 Loss: 0.01*2.1669\n",
      "Batch number:  97\n",
      "Active Features: 59.05%\n",
      "Decoder Weight Norm (Mean): 0.1718822419643402\n",
      "MSE Loss: 0.0764, L1 Loss: 0.01*1.5960\n",
      "Batch number:  98\n",
      "Active Features: 60.95%\n",
      "Decoder Weight Norm (Mean): 0.17188683152198792\n",
      "MSE Loss: 0.0770, L1 Loss: 0.01*1.9042\n",
      "Batch number:  99\n",
      "Active Features: 60.45%\n",
      "Decoder Weight Norm (Mean): 0.17189109325408936\n",
      "MSE Loss: 0.0762, L1 Loss: 0.01*1.7791\n",
      "Batch number:  100\n",
      "Active Features: 58.91%\n",
      "Decoder Weight Norm (Mean): 0.17189542949199677\n",
      "MSE Loss: 0.0778, L1 Loss: 0.01*1.8966\n",
      "Batch number:  101\n",
      "Active Features: 54.09%\n",
      "Decoder Weight Norm (Mean): 0.17189966142177582\n",
      "MSE Loss: 0.0760, L1 Loss: 0.01*1.7374\n",
      "Batch number:  102\n",
      "Active Features: 58.49%\n",
      "Decoder Weight Norm (Mean): 0.17190419137477875\n",
      "MSE Loss: 0.0756, L1 Loss: 0.01*1.8564\n",
      "Batch number:  103\n",
      "Active Features: 58.22%\n",
      "Decoder Weight Norm (Mean): 0.17190870642662048\n",
      "MSE Loss: 0.0765, L1 Loss: 0.01*1.7135\n",
      "Batch number:  104\n",
      "Active Features: 60.66%\n",
      "Decoder Weight Norm (Mean): 0.17191343009471893\n",
      "MSE Loss: 0.0765, L1 Loss: 0.01*1.9626\n",
      "Batch number:  105\n",
      "Active Features: 60.85%\n",
      "Decoder Weight Norm (Mean): 0.1719181388616562\n",
      "MSE Loss: 0.0756, L1 Loss: 0.01*1.8279\n",
      "Batch number:  106\n",
      "Active Features: 57.67%\n",
      "Decoder Weight Norm (Mean): 0.17192283272743225\n",
      "MSE Loss: 0.0765, L1 Loss: 0.01*1.8056\n",
      "Batch number:  107\n",
      "Active Features: 61.13%\n",
      "Decoder Weight Norm (Mean): 0.17192773520946503\n",
      "MSE Loss: 0.0763, L1 Loss: 0.01*1.7027\n",
      "Batch number:  108\n",
      "Active Features: 56.48%\n",
      "Decoder Weight Norm (Mean): 0.1719326376914978\n",
      "MSE Loss: 0.0758, L1 Loss: 0.01*1.8464\n",
      "Batch number:  109\n",
      "Active Features: 59.00%\n",
      "Decoder Weight Norm (Mean): 0.171937495470047\n",
      "MSE Loss: 0.0762, L1 Loss: 0.01*2.0122\n",
      "Batch number:  110\n",
      "Active Features: 61.09%\n",
      "Decoder Weight Norm (Mean): 0.17194214463233948\n",
      "MSE Loss: 0.0787, L1 Loss: 0.01*1.8778\n",
      "Batch number:  111\n",
      "Active Features: 60.04%\n",
      "Decoder Weight Norm (Mean): 0.1719466894865036\n",
      "MSE Loss: 0.0766, L1 Loss: 0.01*1.7262\n",
      "Batch number:  112\n",
      "Active Features: 60.89%\n",
      "Decoder Weight Norm (Mean): 0.17195136845111847\n",
      "MSE Loss: 0.0764, L1 Loss: 0.01*1.7246\n",
      "Batch number:  113\n",
      "Active Features: 57.21%\n",
      "Decoder Weight Norm (Mean): 0.17195628583431244\n",
      "MSE Loss: 0.0778, L1 Loss: 0.01*1.8292\n",
      "Batch number:  114\n",
      "Active Features: 60.58%\n",
      "Decoder Weight Norm (Mean): 0.1719612330198288\n",
      "MSE Loss: 0.0769, L1 Loss: 0.01*1.5965\n",
      "Batch number:  115\n",
      "Active Features: 60.78%\n",
      "Decoder Weight Norm (Mean): 0.17196647822856903\n",
      "MSE Loss: 0.0755, L1 Loss: 0.01*1.9124\n",
      "Batch number:  116\n",
      "Active Features: 60.10%\n",
      "Decoder Weight Norm (Mean): 0.1719716489315033\n",
      "MSE Loss: 0.0763, L1 Loss: 0.01*1.8222\n",
      "Batch number:  117\n",
      "Active Features: 58.33%\n",
      "Decoder Weight Norm (Mean): 0.17197702825069427\n",
      "MSE Loss: 0.0767, L1 Loss: 0.01*1.7202\n",
      "Batch number:  118\n",
      "Active Features: 58.47%\n",
      "Decoder Weight Norm (Mean): 0.1719825565814972\n",
      "MSE Loss: 0.0759, L1 Loss: 0.01*1.9935\n",
      "Batch number:  119\n",
      "Active Features: 59.36%\n",
      "Decoder Weight Norm (Mean): 0.17198780179023743\n",
      "MSE Loss: 0.0765, L1 Loss: 0.01*1.9014\n",
      "Batch number:  120\n",
      "Active Features: 57.35%\n",
      "Decoder Weight Norm (Mean): 0.1719929575920105\n",
      "MSE Loss: 0.0761, L1 Loss: 0.01*1.5930\n",
      "Batch number:  121\n",
      "Active Features: 57.11%\n",
      "Decoder Weight Norm (Mean): 0.17199832201004028\n",
      "MSE Loss: 0.0770, L1 Loss: 0.01*1.8907\n",
      "Batch number:  122\n",
      "Active Features: 60.43%\n",
      "Decoder Weight Norm (Mean): 0.17200346291065216\n",
      "MSE Loss: 0.0762, L1 Loss: 0.01*1.7631\n",
      "Batch number:  123\n",
      "Active Features: 61.11%\n",
      "Decoder Weight Norm (Mean): 0.17200878262519836\n",
      "MSE Loss: 0.0766, L1 Loss: 0.01*2.0831\n",
      "Batch number:  124\n",
      "Active Features: 59.98%\n",
      "Decoder Weight Norm (Mean): 0.17201374471187592\n",
      "MSE Loss: 0.0768, L1 Loss: 0.01*1.7723\n",
      "Batch number:  125\n",
      "Active Features: 57.55%\n",
      "Decoder Weight Norm (Mean): 0.17201882600784302\n",
      "MSE Loss: 0.0759, L1 Loss: 0.01*1.9047\n",
      "Batch number:  126\n",
      "Active Features: 55.35%\n",
      "Decoder Weight Norm (Mean): 0.17202386260032654\n",
      "MSE Loss: 0.0761, L1 Loss: 0.01*1.6520\n",
      "Batch number:  127\n",
      "Active Features: 62.31%\n",
      "Decoder Weight Norm (Mean): 0.1720292717218399\n",
      "MSE Loss: 0.0771, L1 Loss: 0.01*1.6572\n",
      "Epoch [7/10], Loss: 12.2217 ----------------------------\n",
      "Batch number:  0\n",
      "Active Features: 60.65%\n",
      "Decoder Weight Norm (Mean): 0.1720348298549652\n",
      "MSE Loss: 0.0768, L1 Loss: 0.01*1.8375\n",
      "Batch number:  1\n",
      "Active Features: 57.09%\n",
      "Decoder Weight Norm (Mean): 0.17204007506370544\n",
      "MSE Loss: 0.0748, L1 Loss: 0.01*1.6876\n",
      "Batch number:  2\n",
      "Active Features: 55.42%\n",
      "Decoder Weight Norm (Mean): 0.17204561829566956\n",
      "MSE Loss: 0.0762, L1 Loss: 0.01*1.8429\n",
      "Batch number:  3\n",
      "Active Features: 57.48%\n",
      "Decoder Weight Norm (Mean): 0.17205123603343964\n",
      "MSE Loss: 0.0753, L1 Loss: 0.01*2.0219\n",
      "Batch number:  4\n",
      "Active Features: 53.80%\n",
      "Decoder Weight Norm (Mean): 0.1720566302537918\n",
      "MSE Loss: 0.0759, L1 Loss: 0.01*1.9104\n",
      "Batch number:  5\n",
      "Active Features: 58.60%\n",
      "Decoder Weight Norm (Mean): 0.17206209897994995\n",
      "MSE Loss: 0.0768, L1 Loss: 0.01*2.1339\n",
      "Batch number:  6\n",
      "Active Features: 60.25%\n",
      "Decoder Weight Norm (Mean): 0.17206738889217377\n",
      "MSE Loss: 0.0766, L1 Loss: 0.01*1.5664\n",
      "Batch number:  7\n",
      "Active Features: 56.36%\n",
      "Decoder Weight Norm (Mean): 0.17207300662994385\n",
      "MSE Loss: 0.0767, L1 Loss: 0.01*2.0385\n",
      "Batch number:  8\n",
      "Active Features: 59.16%\n",
      "Decoder Weight Norm (Mean): 0.17207837104797363\n",
      "MSE Loss: 0.0760, L1 Loss: 0.01*1.8798\n",
      "Batch number:  9\n",
      "Active Features: 58.84%\n",
      "Decoder Weight Norm (Mean): 0.17208388447761536\n",
      "MSE Loss: 0.0753, L1 Loss: 0.01*1.6536\n",
      "Batch number:  10\n",
      "Active Features: 60.49%\n",
      "Decoder Weight Norm (Mean): 0.17208971083164215\n",
      "MSE Loss: 0.0765, L1 Loss: 0.01*1.8318\n",
      "Batch number:  11\n",
      "Active Features: 55.45%\n",
      "Decoder Weight Norm (Mean): 0.1720954030752182\n",
      "MSE Loss: 0.0761, L1 Loss: 0.01*1.7937\n",
      "Batch number:  12\n",
      "Active Features: 54.55%\n",
      "Decoder Weight Norm (Mean): 0.17210151255130768\n",
      "MSE Loss: 0.0756, L1 Loss: 0.01*1.8569\n",
      "Batch number:  13\n",
      "Active Features: 58.18%\n",
      "Decoder Weight Norm (Mean): 0.17210762202739716\n",
      "MSE Loss: 0.0757, L1 Loss: 0.01*1.8805\n",
      "Batch number:  14\n",
      "Active Features: 58.16%\n",
      "Decoder Weight Norm (Mean): 0.17211370170116425\n",
      "MSE Loss: 0.0754, L1 Loss: 0.01*1.6394\n",
      "Batch number:  15\n",
      "Active Features: 56.31%\n",
      "Decoder Weight Norm (Mean): 0.17212001979351044\n",
      "MSE Loss: 0.0755, L1 Loss: 0.01*1.8434\n",
      "Batch number:  16\n",
      "Active Features: 57.03%\n",
      "Decoder Weight Norm (Mean): 0.17212609946727753\n",
      "MSE Loss: 0.0769, L1 Loss: 0.01*1.6198\n",
      "Batch number:  17\n",
      "Active Features: 55.54%\n",
      "Decoder Weight Norm (Mean): 0.17213250696659088\n",
      "MSE Loss: 0.0762, L1 Loss: 0.01*1.6795\n",
      "Batch number:  18\n",
      "Active Features: 55.94%\n",
      "Decoder Weight Norm (Mean): 0.17213918268680573\n",
      "MSE Loss: 0.0762, L1 Loss: 0.01*1.8323\n",
      "Batch number:  19\n",
      "Active Features: 57.56%\n",
      "Decoder Weight Norm (Mean): 0.1721457690000534\n",
      "MSE Loss: 0.0770, L1 Loss: 0.01*1.6560\n",
      "Batch number:  20\n",
      "Active Features: 55.94%\n",
      "Decoder Weight Norm (Mean): 0.172152578830719\n",
      "MSE Loss: 0.0755, L1 Loss: 0.01*1.7897\n",
      "Batch number:  21\n",
      "Active Features: 56.84%\n",
      "Decoder Weight Norm (Mean): 0.172159343957901\n",
      "MSE Loss: 0.0763, L1 Loss: 0.01*1.8880\n",
      "Batch number:  22\n",
      "Active Features: 58.75%\n",
      "Decoder Weight Norm (Mean): 0.17216609418392181\n",
      "MSE Loss: 0.0759, L1 Loss: 0.01*1.7563\n",
      "Batch number:  23\n",
      "Active Features: 57.85%\n",
      "Decoder Weight Norm (Mean): 0.17217276990413666\n",
      "MSE Loss: 0.0772, L1 Loss: 0.01*1.9017\n",
      "Batch number:  24\n",
      "Active Features: 53.17%\n",
      "Decoder Weight Norm (Mean): 0.1721792370080948\n",
      "MSE Loss: 0.0767, L1 Loss: 0.01*2.0192\n",
      "Batch number:  25\n",
      "Active Features: 56.87%\n",
      "Decoder Weight Norm (Mean): 0.17218561470508575\n",
      "MSE Loss: 0.0765, L1 Loss: 0.01*1.6759\n",
      "Batch number:  26\n",
      "Active Features: 54.43%\n",
      "Decoder Weight Norm (Mean): 0.17219235002994537\n",
      "MSE Loss: 0.0756, L1 Loss: 0.01*1.5666\n",
      "Batch number:  27\n",
      "Active Features: 55.75%\n",
      "Decoder Weight Norm (Mean): 0.17219947278499603\n",
      "MSE Loss: 0.0750, L1 Loss: 0.01*1.7291\n",
      "Batch number:  28\n",
      "Active Features: 53.96%\n",
      "Decoder Weight Norm (Mean): 0.17220652103424072\n",
      "MSE Loss: 0.0764, L1 Loss: 0.01*1.8272\n",
      "Batch number:  29\n",
      "Active Features: 64.37%\n",
      "Decoder Weight Norm (Mean): 0.17221368849277496\n",
      "MSE Loss: 0.0763, L1 Loss: 0.01*1.5805\n",
      "Batch number:  30\n",
      "Active Features: 54.87%\n",
      "Decoder Weight Norm (Mean): 0.17222103476524353\n",
      "MSE Loss: 0.0753, L1 Loss: 0.01*1.7216\n",
      "Batch number:  31\n",
      "Active Features: 57.11%\n",
      "Decoder Weight Norm (Mean): 0.1722281575202942\n",
      "MSE Loss: 0.0759, L1 Loss: 0.01*1.8111\n",
      "Batch number:  32\n",
      "Active Features: 54.56%\n",
      "Decoder Weight Norm (Mean): 0.172234907746315\n",
      "MSE Loss: 0.0755, L1 Loss: 0.01*1.8497\n",
      "Batch number:  33\n",
      "Active Features: 58.80%\n",
      "Decoder Weight Norm (Mean): 0.1722419112920761\n",
      "MSE Loss: 0.0759, L1 Loss: 0.01*1.9999\n",
      "Batch number:  34\n",
      "Active Features: 54.58%\n",
      "Decoder Weight Norm (Mean): 0.17224836349487305\n",
      "MSE Loss: 0.0757, L1 Loss: 0.01*1.8944\n",
      "Batch number:  35\n",
      "Active Features: 54.01%\n",
      "Decoder Weight Norm (Mean): 0.17225486040115356\n",
      "MSE Loss: 0.0748, L1 Loss: 0.01*1.7124\n",
      "Batch number:  36\n",
      "Active Features: 56.58%\n",
      "Decoder Weight Norm (Mean): 0.17226152122020721\n",
      "MSE Loss: 0.0758, L1 Loss: 0.01*2.1383\n",
      "Batch number:  37\n",
      "Active Features: 54.85%\n",
      "Decoder Weight Norm (Mean): 0.17226772010326385\n",
      "MSE Loss: 0.0762, L1 Loss: 0.01*1.8808\n",
      "Batch number:  38\n",
      "Active Features: 55.88%\n",
      "Decoder Weight Norm (Mean): 0.17227426171302795\n",
      "MSE Loss: 0.0756, L1 Loss: 0.01*1.8308\n",
      "Batch number:  39\n",
      "Active Features: 56.38%\n",
      "Decoder Weight Norm (Mean): 0.17228052020072937\n",
      "MSE Loss: 0.0763, L1 Loss: 0.01*1.9286\n",
      "Batch number:  40\n",
      "Active Features: 56.38%\n",
      "Decoder Weight Norm (Mean): 0.17228643596172333\n",
      "MSE Loss: 0.0757, L1 Loss: 0.01*1.7262\n",
      "Batch number:  41\n",
      "Active Features: 56.02%\n",
      "Decoder Weight Norm (Mean): 0.17229284346103668\n",
      "MSE Loss: 0.0764, L1 Loss: 0.01*1.8290\n",
      "Batch number:  42\n",
      "Active Features: 55.66%\n",
      "Decoder Weight Norm (Mean): 0.17229904234409332\n",
      "MSE Loss: 0.0756, L1 Loss: 0.01*1.8296\n",
      "Batch number:  43\n",
      "Active Features: 54.19%\n",
      "Decoder Weight Norm (Mean): 0.1723054200410843\n",
      "MSE Loss: 0.0755, L1 Loss: 0.01*1.6838\n",
      "Batch number:  44\n",
      "Active Features: 59.48%\n",
      "Decoder Weight Norm (Mean): 0.17231202125549316\n",
      "MSE Loss: 0.0762, L1 Loss: 0.01*1.6931\n",
      "Batch number:  45\n",
      "Active Features: 58.26%\n",
      "Decoder Weight Norm (Mean): 0.17231835424900055\n",
      "MSE Loss: 0.0766, L1 Loss: 0.01*1.6217\n",
      "Batch number:  46\n",
      "Active Features: 56.78%\n",
      "Decoder Weight Norm (Mean): 0.17232471704483032\n",
      "MSE Loss: 0.0765, L1 Loss: 0.01*1.8088\n",
      "Batch number:  47\n",
      "Active Features: 53.33%\n",
      "Decoder Weight Norm (Mean): 0.17233087122440338\n",
      "MSE Loss: 0.0750, L1 Loss: 0.01*1.9389\n",
      "Batch number:  48\n",
      "Active Features: 56.26%\n",
      "Decoder Weight Norm (Mean): 0.17233720421791077\n",
      "MSE Loss: 0.0765, L1 Loss: 0.01*1.6745\n",
      "Batch number:  49\n",
      "Active Features: 54.08%\n",
      "Decoder Weight Norm (Mean): 0.17234382033348083\n",
      "MSE Loss: 0.0759, L1 Loss: 0.01*1.7726\n",
      "Batch number:  50\n",
      "Active Features: 55.76%\n",
      "Decoder Weight Norm (Mean): 0.17235006392002106\n",
      "MSE Loss: 0.0754, L1 Loss: 0.01*1.7009\n",
      "Batch number:  51\n",
      "Active Features: 58.70%\n",
      "Decoder Weight Norm (Mean): 0.1723564863204956\n",
      "MSE Loss: 0.0756, L1 Loss: 0.01*1.9864\n",
      "Batch number:  52\n",
      "Active Features: 54.01%\n",
      "Decoder Weight Norm (Mean): 0.1723627895116806\n",
      "MSE Loss: 0.0755, L1 Loss: 0.01*1.8463\n",
      "Batch number:  53\n",
      "Active Features: 59.62%\n",
      "Decoder Weight Norm (Mean): 0.17236916720867157\n",
      "MSE Loss: 0.0766, L1 Loss: 0.01*1.7970\n",
      "Batch number:  54\n",
      "Active Features: 56.46%\n",
      "Decoder Weight Norm (Mean): 0.17237520217895508\n",
      "MSE Loss: 0.0742, L1 Loss: 0.01*2.2457\n",
      "Batch number:  55\n",
      "Active Features: 59.04%\n",
      "Decoder Weight Norm (Mean): 0.17238090932369232\n",
      "MSE Loss: 0.0748, L1 Loss: 0.01*1.7882\n",
      "Batch number:  56\n",
      "Active Features: 53.49%\n",
      "Decoder Weight Norm (Mean): 0.1723869889974594\n",
      "MSE Loss: 0.0756, L1 Loss: 0.01*1.7961\n",
      "Batch number:  57\n",
      "Active Features: 54.83%\n",
      "Decoder Weight Norm (Mean): 0.17239278554916382\n",
      "MSE Loss: 0.0767, L1 Loss: 0.01*1.9096\n",
      "Batch number:  58\n",
      "Active Features: 55.16%\n",
      "Decoder Weight Norm (Mean): 0.1723984032869339\n",
      "MSE Loss: 0.0757, L1 Loss: 0.01*1.7878\n",
      "Batch number:  59\n",
      "Active Features: 56.05%\n",
      "Decoder Weight Norm (Mean): 0.17240466177463531\n",
      "MSE Loss: 0.0752, L1 Loss: 0.01*2.0588\n",
      "Batch number:  60\n",
      "Active Features: 55.33%\n",
      "Decoder Weight Norm (Mean): 0.1724105328321457\n",
      "MSE Loss: 0.0754, L1 Loss: 0.01*1.8963\n",
      "Batch number:  61\n",
      "Active Features: 53.44%\n",
      "Decoder Weight Norm (Mean): 0.17241635918617249\n",
      "MSE Loss: 0.0754, L1 Loss: 0.01*1.8354\n",
      "Batch number:  62\n",
      "Active Features: 53.86%\n",
      "Decoder Weight Norm (Mean): 0.17242223024368286\n",
      "MSE Loss: 0.0749, L1 Loss: 0.01*1.7885\n",
      "Batch number:  63\n",
      "Active Features: 54.99%\n",
      "Decoder Weight Norm (Mean): 0.1724282205104828\n",
      "MSE Loss: 0.0756, L1 Loss: 0.01*1.8907\n",
      "Batch number:  64\n",
      "Active Features: 61.10%\n",
      "Decoder Weight Norm (Mean): 0.172434002161026\n",
      "MSE Loss: 0.0750, L1 Loss: 0.01*1.6675\n",
      "Batch number:  65\n",
      "Active Features: 54.70%\n",
      "Decoder Weight Norm (Mean): 0.17243996262550354\n",
      "MSE Loss: 0.0759, L1 Loss: 0.01*1.7217\n",
      "Batch number:  66\n",
      "Active Features: 53.88%\n",
      "Decoder Weight Norm (Mean): 0.17244616150856018\n",
      "MSE Loss: 0.0740, L1 Loss: 0.01*1.6886\n",
      "Batch number:  67\n",
      "Active Features: 52.49%\n",
      "Decoder Weight Norm (Mean): 0.17245261371135712\n",
      "MSE Loss: 0.0749, L1 Loss: 0.01*1.8832\n",
      "Batch number:  68\n",
      "Active Features: 57.05%\n",
      "Decoder Weight Norm (Mean): 0.17245908081531525\n",
      "MSE Loss: 0.0748, L1 Loss: 0.01*1.9114\n",
      "Batch number:  69\n",
      "Active Features: 52.47%\n",
      "Decoder Weight Norm (Mean): 0.1724654734134674\n",
      "MSE Loss: 0.0744, L1 Loss: 0.01*1.6809\n",
      "Batch number:  70\n",
      "Active Features: 53.45%\n",
      "Decoder Weight Norm (Mean): 0.17247217893600464\n",
      "MSE Loss: 0.0751, L1 Loss: 0.01*1.7630\n",
      "Batch number:  71\n",
      "Active Features: 57.48%\n",
      "Decoder Weight Norm (Mean): 0.17247889935970306\n",
      "MSE Loss: 0.0750, L1 Loss: 0.01*2.0441\n",
      "Batch number:  72\n",
      "Active Features: 53.22%\n",
      "Decoder Weight Norm (Mean): 0.17248545587062836\n",
      "MSE Loss: 0.0752, L1 Loss: 0.01*1.8488\n",
      "Batch number:  73\n",
      "Active Features: 53.74%\n",
      "Decoder Weight Norm (Mean): 0.17249217629432678\n",
      "MSE Loss: 0.0755, L1 Loss: 0.01*1.6772\n",
      "Batch number:  74\n",
      "Active Features: 54.90%\n",
      "Decoder Weight Norm (Mean): 0.17249909043312073\n",
      "MSE Loss: 0.0743, L1 Loss: 0.01*1.8678\n",
      "Batch number:  75\n",
      "Active Features: 55.16%\n",
      "Decoder Weight Norm (Mean): 0.1725056767463684\n",
      "MSE Loss: 0.0751, L1 Loss: 0.01*1.7247\n",
      "Batch number:  76\n",
      "Active Features: 57.32%\n",
      "Decoder Weight Norm (Mean): 0.1725124716758728\n",
      "MSE Loss: 0.0755, L1 Loss: 0.01*1.8838\n",
      "Batch number:  77\n",
      "Active Features: 56.88%\n",
      "Decoder Weight Norm (Mean): 0.17251920700073242\n",
      "MSE Loss: 0.0758, L1 Loss: 0.01*1.7031\n",
      "Batch number:  78\n",
      "Active Features: 55.51%\n",
      "Decoder Weight Norm (Mean): 0.1725260466337204\n",
      "MSE Loss: 0.0748, L1 Loss: 0.01*1.9490\n",
      "Batch number:  79\n",
      "Active Features: 55.68%\n",
      "Decoder Weight Norm (Mean): 0.17253248393535614\n",
      "MSE Loss: 0.0758, L1 Loss: 0.01*1.8584\n",
      "Batch number:  80\n",
      "Active Features: 51.54%\n",
      "Decoder Weight Norm (Mean): 0.17253901064395905\n",
      "MSE Loss: 0.0746, L1 Loss: 0.01*1.7919\n",
      "Batch number:  81\n",
      "Active Features: 52.35%\n",
      "Decoder Weight Norm (Mean): 0.17254558205604553\n",
      "MSE Loss: 0.0751, L1 Loss: 0.01*1.9429\n",
      "Batch number:  82\n",
      "Active Features: 52.71%\n",
      "Decoder Weight Norm (Mean): 0.17255200445652008\n",
      "MSE Loss: 0.0750, L1 Loss: 0.01*1.8860\n",
      "Batch number:  83\n",
      "Active Features: 55.62%\n",
      "Decoder Weight Norm (Mean): 0.1725584715604782\n",
      "MSE Loss: 0.0765, L1 Loss: 0.01*1.6988\n",
      "Batch number:  84\n",
      "Active Features: 54.59%\n",
      "Decoder Weight Norm (Mean): 0.17256516218185425\n",
      "MSE Loss: 0.0735, L1 Loss: 0.01*1.6722\n",
      "Batch number:  85\n",
      "Active Features: 53.14%\n",
      "Decoder Weight Norm (Mean): 0.17257195711135864\n",
      "MSE Loss: 0.0743, L1 Loss: 0.01*1.7456\n",
      "Batch number:  86\n",
      "Active Features: 53.89%\n",
      "Decoder Weight Norm (Mean): 0.17257866263389587\n",
      "MSE Loss: 0.0743, L1 Loss: 0.01*1.9437\n",
      "Batch number:  87\n",
      "Active Features: 51.08%\n",
      "Decoder Weight Norm (Mean): 0.17258529365062714\n",
      "MSE Loss: 0.0740, L1 Loss: 0.01*1.9074\n",
      "Batch number:  88\n",
      "Active Features: 50.66%\n",
      "Decoder Weight Norm (Mean): 0.17259229719638824\n",
      "MSE Loss: 0.0752, L1 Loss: 0.01*1.8699\n",
      "Batch number:  89\n",
      "Active Features: 50.60%\n",
      "Decoder Weight Norm (Mean): 0.17259939014911652\n",
      "MSE Loss: 0.0750, L1 Loss: 0.01*1.7714\n",
      "Batch number:  90\n",
      "Active Features: 54.77%\n",
      "Decoder Weight Norm (Mean): 0.17260636389255524\n",
      "MSE Loss: 0.0756, L1 Loss: 0.01*1.8163\n",
      "Batch number:  91\n",
      "Active Features: 50.54%\n",
      "Decoder Weight Norm (Mean): 0.17261363565921783\n",
      "MSE Loss: 0.0743, L1 Loss: 0.01*1.7859\n",
      "Batch number:  92\n",
      "Active Features: 52.00%\n",
      "Decoder Weight Norm (Mean): 0.17262107133865356\n",
      "MSE Loss: 0.0753, L1 Loss: 0.01*1.8604\n",
      "Batch number:  93\n",
      "Active Features: 54.09%\n",
      "Decoder Weight Norm (Mean): 0.17262856662273407\n",
      "MSE Loss: 0.0734, L1 Loss: 0.01*2.1189\n",
      "Batch number:  94\n",
      "Active Features: 52.06%\n",
      "Decoder Weight Norm (Mean): 0.17263585329055786\n",
      "MSE Loss: 0.0753, L1 Loss: 0.01*2.0443\n",
      "Batch number:  95\n",
      "Active Features: 49.41%\n",
      "Decoder Weight Norm (Mean): 0.17264330387115479\n",
      "MSE Loss: 0.0749, L1 Loss: 0.01*1.8161\n",
      "Batch number:  96\n",
      "Active Features: 52.26%\n",
      "Decoder Weight Norm (Mean): 0.17265093326568604\n",
      "MSE Loss: 0.0744, L1 Loss: 0.01*1.8599\n",
      "Batch number:  97\n",
      "Active Features: 54.75%\n",
      "Decoder Weight Norm (Mean): 0.17265863716602325\n",
      "MSE Loss: 0.0743, L1 Loss: 0.01*1.7876\n",
      "Batch number:  98\n",
      "Active Features: 55.57%\n",
      "Decoder Weight Norm (Mean): 0.17266640067100525\n",
      "MSE Loss: 0.0751, L1 Loss: 0.01*1.9954\n",
      "Batch number:  99\n",
      "Active Features: 52.95%\n",
      "Decoder Weight Norm (Mean): 0.17267394065856934\n",
      "MSE Loss: 0.0746, L1 Loss: 0.01*1.8827\n",
      "Batch number:  100\n",
      "Active Features: 53.34%\n",
      "Decoder Weight Norm (Mean): 0.17268122732639313\n",
      "MSE Loss: 0.0756, L1 Loss: 0.01*2.0031\n",
      "Batch number:  101\n",
      "Active Features: 50.60%\n",
      "Decoder Weight Norm (Mean): 0.1726883202791214\n",
      "MSE Loss: 0.0745, L1 Loss: 0.01*1.9025\n",
      "Batch number:  102\n",
      "Active Features: 51.54%\n",
      "Decoder Weight Norm (Mean): 0.17269562184810638\n",
      "MSE Loss: 0.0731, L1 Loss: 0.01*1.8177\n",
      "Batch number:  103\n",
      "Active Features: 52.94%\n",
      "Decoder Weight Norm (Mean): 0.17270296812057495\n",
      "MSE Loss: 0.0743, L1 Loss: 0.01*1.6884\n",
      "Batch number:  104\n",
      "Active Features: 50.80%\n",
      "Decoder Weight Norm (Mean): 0.172710582613945\n",
      "MSE Loss: 0.0750, L1 Loss: 0.01*2.0470\n",
      "Batch number:  105\n",
      "Active Features: 51.59%\n",
      "Decoder Weight Norm (Mean): 0.17271798849105835\n",
      "MSE Loss: 0.0748, L1 Loss: 0.01*1.7810\n",
      "Batch number:  106\n",
      "Active Features: 50.27%\n",
      "Decoder Weight Norm (Mean): 0.17272554337978363\n",
      "MSE Loss: 0.0741, L1 Loss: 0.01*1.9085\n",
      "Batch number:  107\n",
      "Active Features: 53.74%\n",
      "Decoder Weight Norm (Mean): 0.1727331429719925\n",
      "MSE Loss: 0.0750, L1 Loss: 0.01*1.7741\n",
      "Batch number:  108\n",
      "Active Features: 48.70%\n",
      "Decoder Weight Norm (Mean): 0.17274071276187897\n",
      "MSE Loss: 0.0740, L1 Loss: 0.01*1.7099\n",
      "Batch number:  109\n",
      "Active Features: 54.13%\n",
      "Decoder Weight Norm (Mean): 0.17274850606918335\n",
      "MSE Loss: 0.0743, L1 Loss: 0.01*2.1900\n",
      "Batch number:  110\n",
      "Active Features: 55.24%\n",
      "Decoder Weight Norm (Mean): 0.1727561503648758\n",
      "MSE Loss: 0.0763, L1 Loss: 0.01*1.7151\n",
      "Batch number:  111\n",
      "Active Features: 51.53%\n",
      "Decoder Weight Norm (Mean): 0.17276376485824585\n",
      "MSE Loss: 0.0747, L1 Loss: 0.01*1.9547\n",
      "Batch number:  112\n",
      "Active Features: 51.67%\n",
      "Decoder Weight Norm (Mean): 0.172771155834198\n",
      "MSE Loss: 0.0741, L1 Loss: 0.01*1.6488\n",
      "Batch number:  113\n",
      "Active Features: 48.19%\n",
      "Decoder Weight Norm (Mean): 0.17277880012989044\n",
      "MSE Loss: 0.0755, L1 Loss: 0.01*1.8061\n",
      "Batch number:  114\n",
      "Active Features: 54.41%\n",
      "Decoder Weight Norm (Mean): 0.17278659343719482\n",
      "MSE Loss: 0.0749, L1 Loss: 0.01*1.8216\n",
      "Batch number:  115\n",
      "Active Features: 52.14%\n",
      "Decoder Weight Norm (Mean): 0.17279432713985443\n",
      "MSE Loss: 0.0749, L1 Loss: 0.01*1.6928\n",
      "Batch number:  116\n",
      "Active Features: 50.33%\n",
      "Decoder Weight Norm (Mean): 0.17280228435993195\n",
      "MSE Loss: 0.0743, L1 Loss: 0.01*1.7864\n",
      "Batch number:  117\n",
      "Active Features: 50.14%\n",
      "Decoder Weight Norm (Mean): 0.1728101223707199\n",
      "MSE Loss: 0.0754, L1 Loss: 0.01*1.6855\n",
      "Batch number:  118\n",
      "Active Features: 51.51%\n",
      "Decoder Weight Norm (Mean): 0.17281806468963623\n",
      "MSE Loss: 0.0749, L1 Loss: 0.01*1.5757\n",
      "Batch number:  119\n",
      "Active Features: 51.11%\n",
      "Decoder Weight Norm (Mean): 0.17282626032829285\n",
      "MSE Loss: 0.0750, L1 Loss: 0.01*1.7894\n",
      "Batch number:  120\n",
      "Active Features: 50.53%\n",
      "Decoder Weight Norm (Mean): 0.17283421754837036\n",
      "MSE Loss: 0.0743, L1 Loss: 0.01*1.7604\n",
      "Batch number:  121\n",
      "Active Features: 51.40%\n",
      "Decoder Weight Norm (Mean): 0.17284242808818817\n",
      "MSE Loss: 0.0750, L1 Loss: 0.01*1.7638\n",
      "Batch number:  122\n",
      "Active Features: 51.41%\n",
      "Decoder Weight Norm (Mean): 0.1728506088256836\n",
      "MSE Loss: 0.0744, L1 Loss: 0.01*1.7917\n",
      "Batch number:  123\n",
      "Active Features: 57.50%\n",
      "Decoder Weight Norm (Mean): 0.17285868525505066\n",
      "MSE Loss: 0.0749, L1 Loss: 0.01*1.8546\n",
      "Batch number:  124\n",
      "Active Features: 49.71%\n",
      "Decoder Weight Norm (Mean): 0.17286664247512817\n",
      "MSE Loss: 0.0746, L1 Loss: 0.01*1.8861\n",
      "Batch number:  125\n",
      "Active Features: 51.81%\n",
      "Decoder Weight Norm (Mean): 0.1728745996952057\n",
      "MSE Loss: 0.0739, L1 Loss: 0.01*1.9754\n",
      "Batch number:  126\n",
      "Active Features: 49.35%\n",
      "Decoder Weight Norm (Mean): 0.17288243770599365\n",
      "MSE Loss: 0.0739, L1 Loss: 0.01*1.7328\n",
      "Batch number:  127\n",
      "Active Features: 54.40%\n",
      "Decoder Weight Norm (Mean): 0.17289061844348907\n",
      "MSE Loss: 0.0746, L1 Loss: 0.01*1.7921\n",
      "Epoch [8/10], Loss: 11.9782 ----------------------------\n",
      "Batch number:  0\n",
      "Active Features: 58.38%\n",
      "Decoder Weight Norm (Mean): 0.1728985607624054\n",
      "MSE Loss: 0.0747, L1 Loss: 0.01*1.9033\n",
      "Batch number:  1\n",
      "Active Features: 51.52%\n",
      "Decoder Weight Norm (Mean): 0.17290626466274261\n",
      "MSE Loss: 0.0728, L1 Loss: 0.01*1.8549\n",
      "Batch number:  2\n",
      "Active Features: 53.05%\n",
      "Decoder Weight Norm (Mean): 0.17291410267353058\n",
      "MSE Loss: 0.0758, L1 Loss: 0.01*1.6601\n",
      "Batch number:  3\n",
      "Active Features: 51.22%\n",
      "Decoder Weight Norm (Mean): 0.17292222380638123\n",
      "MSE Loss: 0.0738, L1 Loss: 0.01*1.9183\n",
      "Batch number:  4\n",
      "Active Features: 48.40%\n",
      "Decoder Weight Norm (Mean): 0.17293035984039307\n",
      "MSE Loss: 0.0743, L1 Loss: 0.01*1.7559\n",
      "Batch number:  5\n",
      "Active Features: 50.93%\n",
      "Decoder Weight Norm (Mean): 0.17293880879878998\n",
      "MSE Loss: 0.0747, L1 Loss: 0.01*2.1200\n",
      "Batch number:  6\n",
      "Active Features: 52.29%\n",
      "Decoder Weight Norm (Mean): 0.17294703423976898\n",
      "MSE Loss: 0.0742, L1 Loss: 0.01*1.7678\n",
      "Batch number:  7\n",
      "Active Features: 51.70%\n",
      "Decoder Weight Norm (Mean): 0.17295536398887634\n",
      "MSE Loss: 0.0747, L1 Loss: 0.01*1.7854\n",
      "Batch number:  8\n",
      "Active Features: 53.49%\n",
      "Decoder Weight Norm (Mean): 0.1729637235403061\n",
      "MSE Loss: 0.0740, L1 Loss: 0.01*1.8292\n",
      "Batch number:  9\n",
      "Active Features: 49.00%\n",
      "Decoder Weight Norm (Mean): 0.17297202348709106\n",
      "MSE Loss: 0.0735, L1 Loss: 0.01*2.1488\n",
      "Batch number:  10\n",
      "Active Features: 55.16%\n",
      "Decoder Weight Norm (Mean): 0.17298029363155365\n",
      "MSE Loss: 0.0744, L1 Loss: 0.01*1.9978\n",
      "Batch number:  11\n",
      "Active Features: 49.11%\n",
      "Decoder Weight Norm (Mean): 0.1729883998632431\n",
      "MSE Loss: 0.0747, L1 Loss: 0.01*1.8564\n",
      "Batch number:  12\n",
      "Active Features: 50.46%\n",
      "Decoder Weight Norm (Mean): 0.17299659550189972\n",
      "MSE Loss: 0.0747, L1 Loss: 0.01*1.7699\n",
      "Batch number:  13\n",
      "Active Features: 53.90%\n",
      "Decoder Weight Norm (Mean): 0.1730048805475235\n",
      "MSE Loss: 0.0745, L1 Loss: 0.01*1.9401\n",
      "Batch number:  14\n",
      "Active Features: 52.31%\n",
      "Decoder Weight Norm (Mean): 0.17301291227340698\n",
      "MSE Loss: 0.0741, L1 Loss: 0.01*1.7370\n",
      "Batch number:  15\n",
      "Active Features: 52.33%\n",
      "Decoder Weight Norm (Mean): 0.1730208694934845\n",
      "MSE Loss: 0.0738, L1 Loss: 0.01*1.8559\n",
      "Batch number:  16\n",
      "Active Features: 49.11%\n",
      "Decoder Weight Norm (Mean): 0.1730288565158844\n",
      "MSE Loss: 0.0748, L1 Loss: 0.01*1.9652\n",
      "Batch number:  17\n",
      "Active Features: 50.52%\n",
      "Decoder Weight Norm (Mean): 0.17303675413131714\n",
      "MSE Loss: 0.0753, L1 Loss: 0.01*1.8686\n",
      "Batch number:  18\n",
      "Active Features: 52.85%\n",
      "Decoder Weight Norm (Mean): 0.17304474115371704\n",
      "MSE Loss: 0.0745, L1 Loss: 0.01*1.7188\n",
      "Batch number:  19\n",
      "Active Features: 50.26%\n",
      "Decoder Weight Norm (Mean): 0.1730526089668274\n",
      "MSE Loss: 0.0748, L1 Loss: 0.01*1.7279\n",
      "Batch number:  20\n",
      "Active Features: 49.79%\n",
      "Decoder Weight Norm (Mean): 0.17306046187877655\n",
      "MSE Loss: 0.0746, L1 Loss: 0.01*1.7883\n",
      "Batch number:  21\n",
      "Active Features: 50.62%\n",
      "Decoder Weight Norm (Mean): 0.17306850850582123\n",
      "MSE Loss: 0.0742, L1 Loss: 0.01*2.0547\n",
      "Batch number:  22\n",
      "Active Features: 50.53%\n",
      "Decoder Weight Norm (Mean): 0.1730765998363495\n",
      "MSE Loss: 0.0738, L1 Loss: 0.01*1.9811\n",
      "Batch number:  23\n",
      "Active Features: 50.54%\n",
      "Decoder Weight Norm (Mean): 0.17308469116687775\n",
      "MSE Loss: 0.0756, L1 Loss: 0.01*1.6052\n",
      "Batch number:  24\n",
      "Active Features: 51.67%\n",
      "Decoder Weight Norm (Mean): 0.17309284210205078\n",
      "MSE Loss: 0.0749, L1 Loss: 0.01*2.0737\n",
      "Batch number:  25\n",
      "Active Features: 46.22%\n",
      "Decoder Weight Norm (Mean): 0.17310091853141785\n",
      "MSE Loss: 0.0743, L1 Loss: 0.01*1.6963\n",
      "Batch number:  26\n",
      "Active Features: 48.69%\n",
      "Decoder Weight Norm (Mean): 0.17310930788516998\n",
      "MSE Loss: 0.0742, L1 Loss: 0.01*1.8985\n",
      "Batch number:  27\n",
      "Active Features: 50.92%\n",
      "Decoder Weight Norm (Mean): 0.17311763763427734\n",
      "MSE Loss: 0.0734, L1 Loss: 0.01*1.7982\n",
      "Batch number:  28\n",
      "Active Features: 48.49%\n",
      "Decoder Weight Norm (Mean): 0.17312602698802948\n",
      "MSE Loss: 0.0742, L1 Loss: 0.01*1.5801\n",
      "Batch number:  29\n",
      "Active Features: 57.05%\n",
      "Decoder Weight Norm (Mean): 0.17313474416732788\n",
      "MSE Loss: 0.0747, L1 Loss: 0.01*1.6698\n",
      "Batch number:  30\n",
      "Active Features: 48.02%\n",
      "Decoder Weight Norm (Mean): 0.17314310371875763\n",
      "MSE Loss: 0.0734, L1 Loss: 0.01*1.9994\n",
      "Batch number:  31\n",
      "Active Features: 52.03%\n",
      "Decoder Weight Norm (Mean): 0.17315161228179932\n",
      "MSE Loss: 0.0736, L1 Loss: 0.01*1.7764\n",
      "Batch number:  32\n",
      "Active Features: 48.68%\n",
      "Decoder Weight Norm (Mean): 0.17316026985645294\n",
      "MSE Loss: 0.0747, L1 Loss: 0.01*1.9544\n",
      "Batch number:  33\n",
      "Active Features: 50.78%\n",
      "Decoder Weight Norm (Mean): 0.17316848039627075\n",
      "MSE Loss: 0.0746, L1 Loss: 0.01*1.7979\n",
      "Batch number:  34\n",
      "Active Features: 48.03%\n",
      "Decoder Weight Norm (Mean): 0.17317697405815125\n",
      "MSE Loss: 0.0740, L1 Loss: 0.01*1.8781\n",
      "Batch number:  35\n",
      "Active Features: 48.35%\n",
      "Decoder Weight Norm (Mean): 0.17318494617938995\n",
      "MSE Loss: 0.0734, L1 Loss: 0.01*1.8534\n",
      "Batch number:  36\n",
      "Active Features: 51.15%\n",
      "Decoder Weight Norm (Mean): 0.1731930673122406\n",
      "MSE Loss: 0.0740, L1 Loss: 0.01*1.8206\n",
      "Batch number:  37\n",
      "Active Features: 48.91%\n",
      "Decoder Weight Norm (Mean): 0.1732012927532196\n",
      "MSE Loss: 0.0754, L1 Loss: 0.01*2.0019\n",
      "Batch number:  38\n",
      "Active Features: 47.52%\n",
      "Decoder Weight Norm (Mean): 0.1732092797756195\n",
      "MSE Loss: 0.0738, L1 Loss: 0.01*1.8521\n",
      "Batch number:  39\n",
      "Active Features: 50.18%\n",
      "Decoder Weight Norm (Mean): 0.17321759462356567\n",
      "MSE Loss: 0.0747, L1 Loss: 0.01*1.7322\n",
      "Batch number:  40\n",
      "Active Features: 49.66%\n",
      "Decoder Weight Norm (Mean): 0.17322567105293274\n",
      "MSE Loss: 0.0741, L1 Loss: 0.01*1.8909\n",
      "Batch number:  41\n",
      "Active Features: 47.34%\n",
      "Decoder Weight Norm (Mean): 0.17323347926139832\n",
      "MSE Loss: 0.0742, L1 Loss: 0.01*1.8232\n",
      "Batch number:  42\n",
      "Active Features: 50.85%\n",
      "Decoder Weight Norm (Mean): 0.17324168980121613\n",
      "MSE Loss: 0.0741, L1 Loss: 0.01*1.8597\n",
      "Batch number:  43\n",
      "Active Features: 50.62%\n",
      "Decoder Weight Norm (Mean): 0.17324943840503693\n",
      "MSE Loss: 0.0742, L1 Loss: 0.01*1.8593\n",
      "Batch number:  44\n",
      "Active Features: 54.12%\n",
      "Decoder Weight Norm (Mean): 0.17325717210769653\n",
      "MSE Loss: 0.0741, L1 Loss: 0.01*1.8934\n",
      "Batch number:  45\n",
      "Active Features: 51.41%\n",
      "Decoder Weight Norm (Mean): 0.1732649952173233\n",
      "MSE Loss: 0.0738, L1 Loss: 0.01*1.8738\n",
      "Batch number:  46\n",
      "Active Features: 51.25%\n",
      "Decoder Weight Norm (Mean): 0.173272505402565\n",
      "MSE Loss: 0.0746, L1 Loss: 0.01*1.7876\n",
      "Batch number:  47\n",
      "Active Features: 47.36%\n",
      "Decoder Weight Norm (Mean): 0.17328007519245148\n",
      "MSE Loss: 0.0724, L1 Loss: 0.01*2.0633\n",
      "Batch number:  48\n",
      "Active Features: 49.21%\n",
      "Decoder Weight Norm (Mean): 0.17328764498233795\n",
      "MSE Loss: 0.0746, L1 Loss: 0.01*1.7783\n",
      "Batch number:  49\n",
      "Active Features: 48.49%\n",
      "Decoder Weight Norm (Mean): 0.17329569160938263\n",
      "MSE Loss: 0.0739, L1 Loss: 0.01*1.8079\n",
      "Batch number:  50\n",
      "Active Features: 47.66%\n",
      "Decoder Weight Norm (Mean): 0.17330355942249298\n",
      "MSE Loss: 0.0739, L1 Loss: 0.01*1.7887\n",
      "Batch number:  51\n",
      "Active Features: 51.37%\n",
      "Decoder Weight Norm (Mean): 0.17331136763095856\n",
      "MSE Loss: 0.0738, L1 Loss: 0.01*1.6973\n",
      "Batch number:  52\n",
      "Active Features: 48.57%\n",
      "Decoder Weight Norm (Mean): 0.173319473862648\n",
      "MSE Loss: 0.0742, L1 Loss: 0.01*1.7009\n",
      "Batch number:  53\n",
      "Active Features: 48.94%\n",
      "Decoder Weight Norm (Mean): 0.17332769930362701\n",
      "MSE Loss: 0.0741, L1 Loss: 0.01*1.9741\n",
      "Batch number:  54\n",
      "Active Features: 50.86%\n",
      "Decoder Weight Norm (Mean): 0.17333564162254333\n",
      "MSE Loss: 0.0733, L1 Loss: 0.01*1.6824\n",
      "Batch number:  55\n",
      "Active Features: 48.30%\n",
      "Decoder Weight Norm (Mean): 0.173344224691391\n",
      "MSE Loss: 0.0747, L1 Loss: 0.01*1.9750\n",
      "Batch number:  56\n",
      "Active Features: 50.26%\n",
      "Decoder Weight Norm (Mean): 0.1733521670103073\n",
      "MSE Loss: 0.0738, L1 Loss: 0.01*1.6891\n",
      "Batch number:  57\n",
      "Active Features: 50.26%\n",
      "Decoder Weight Norm (Mean): 0.17336046695709229\n",
      "MSE Loss: 0.0756, L1 Loss: 0.01*1.9472\n",
      "Batch number:  58\n",
      "Active Features: 46.43%\n",
      "Decoder Weight Norm (Mean): 0.17336848378181458\n",
      "MSE Loss: 0.0737, L1 Loss: 0.01*1.7764\n",
      "Batch number:  59\n",
      "Active Features: 48.35%\n",
      "Decoder Weight Norm (Mean): 0.17337709665298462\n",
      "MSE Loss: 0.0742, L1 Loss: 0.01*1.9760\n",
      "Batch number:  60\n",
      "Active Features: 47.02%\n",
      "Decoder Weight Norm (Mean): 0.17338550090789795\n",
      "MSE Loss: 0.0725, L1 Loss: 0.01*1.7663\n",
      "Batch number:  61\n",
      "Active Features: 48.60%\n",
      "Decoder Weight Norm (Mean): 0.17339420318603516\n",
      "MSE Loss: 0.0737, L1 Loss: 0.01*1.8903\n",
      "Batch number:  62\n",
      "Active Features: 48.08%\n",
      "Decoder Weight Norm (Mean): 0.1734027862548828\n",
      "MSE Loss: 0.0735, L1 Loss: 0.01*1.7584\n",
      "Batch number:  63\n",
      "Active Features: 47.73%\n",
      "Decoder Weight Norm (Mean): 0.17341142892837524\n",
      "MSE Loss: 0.0740, L1 Loss: 0.01*1.7925\n",
      "Batch number:  64\n",
      "Active Features: 54.88%\n",
      "Decoder Weight Norm (Mean): 0.17341983318328857\n",
      "MSE Loss: 0.0734, L1 Loss: 0.01*1.8388\n",
      "Batch number:  65\n",
      "Active Features: 47.95%\n",
      "Decoder Weight Norm (Mean): 0.17342837154865265\n",
      "MSE Loss: 0.0740, L1 Loss: 0.01*1.8539\n",
      "Batch number:  66\n",
      "Active Features: 46.70%\n",
      "Decoder Weight Norm (Mean): 0.17343701422214508\n",
      "MSE Loss: 0.0730, L1 Loss: 0.01*1.8965\n",
      "Batch number:  67\n",
      "Active Features: 46.83%\n",
      "Decoder Weight Norm (Mean): 0.17344552278518677\n",
      "MSE Loss: 0.0736, L1 Loss: 0.01*1.8049\n",
      "Batch number:  68\n",
      "Active Features: 49.37%\n",
      "Decoder Weight Norm (Mean): 0.17345431447029114\n",
      "MSE Loss: 0.0730, L1 Loss: 0.01*1.8745\n",
      "Batch number:  69\n",
      "Active Features: 48.80%\n",
      "Decoder Weight Norm (Mean): 0.17346306145191193\n",
      "MSE Loss: 0.0734, L1 Loss: 0.01*1.6970\n",
      "Batch number:  70\n",
      "Active Features: 49.34%\n",
      "Decoder Weight Norm (Mean): 0.17347201704978943\n",
      "MSE Loss: 0.0736, L1 Loss: 0.01*1.7972\n",
      "Batch number:  71\n",
      "Active Features: 46.21%\n",
      "Decoder Weight Norm (Mean): 0.17348089814186096\n",
      "MSE Loss: 0.0732, L1 Loss: 0.01*1.8552\n",
      "Batch number:  72\n",
      "Active Features: 46.39%\n",
      "Decoder Weight Norm (Mean): 0.17348986864089966\n",
      "MSE Loss: 0.0736, L1 Loss: 0.01*1.9321\n",
      "Batch number:  73\n",
      "Active Features: 48.39%\n",
      "Decoder Weight Norm (Mean): 0.17349876463413239\n",
      "MSE Loss: 0.0740, L1 Loss: 0.01*1.7916\n",
      "Batch number:  74\n",
      "Active Features: 46.89%\n",
      "Decoder Weight Norm (Mean): 0.17350749671459198\n",
      "MSE Loss: 0.0729, L1 Loss: 0.01*1.7917\n",
      "Batch number:  75\n",
      "Active Features: 49.40%\n",
      "Decoder Weight Norm (Mean): 0.17351625859737396\n",
      "MSE Loss: 0.0740, L1 Loss: 0.01*1.9124\n",
      "Batch number:  76\n",
      "Active Features: 51.85%\n",
      "Decoder Weight Norm (Mean): 0.1735249161720276\n",
      "MSE Loss: 0.0731, L1 Loss: 0.01*1.8157\n",
      "Batch number:  77\n",
      "Active Features: 49.73%\n",
      "Decoder Weight Norm (Mean): 0.173533633351326\n",
      "MSE Loss: 0.0733, L1 Loss: 0.01*1.8560\n",
      "Batch number:  78\n",
      "Active Features: 49.99%\n",
      "Decoder Weight Norm (Mean): 0.1735420823097229\n",
      "MSE Loss: 0.0733, L1 Loss: 0.01*1.8704\n",
      "Batch number:  79\n",
      "Active Features: 47.02%\n",
      "Decoder Weight Norm (Mean): 0.1735505312681198\n",
      "MSE Loss: 0.0741, L1 Loss: 0.01*1.9171\n",
      "Batch number:  80\n",
      "Active Features: 46.17%\n",
      "Decoder Weight Norm (Mean): 0.17355895042419434\n",
      "MSE Loss: 0.0736, L1 Loss: 0.01*1.8611\n",
      "Batch number:  81\n",
      "Active Features: 47.19%\n",
      "Decoder Weight Norm (Mean): 0.17356738448143005\n",
      "MSE Loss: 0.0738, L1 Loss: 0.01*1.6458\n",
      "Batch number:  82\n",
      "Active Features: 48.54%\n",
      "Decoder Weight Norm (Mean): 0.17357610166072845\n",
      "MSE Loss: 0.0729, L1 Loss: 0.01*1.9016\n",
      "Batch number:  83\n",
      "Active Features: 48.09%\n",
      "Decoder Weight Norm (Mean): 0.1735846847295761\n",
      "MSE Loss: 0.0749, L1 Loss: 0.01*1.9861\n",
      "Batch number:  84\n",
      "Active Features: 46.63%\n",
      "Decoder Weight Norm (Mean): 0.17359331250190735\n",
      "MSE Loss: 0.0710, L1 Loss: 0.01*1.7912\n",
      "Batch number:  85\n",
      "Active Features: 46.39%\n",
      "Decoder Weight Norm (Mean): 0.17360210418701172\n",
      "MSE Loss: 0.0726, L1 Loss: 0.01*1.8111\n",
      "Batch number:  86\n",
      "Active Features: 46.02%\n",
      "Decoder Weight Norm (Mean): 0.1736108958721161\n",
      "MSE Loss: 0.0737, L1 Loss: 0.01*1.7189\n",
      "Batch number:  87\n",
      "Active Features: 42.30%\n",
      "Decoder Weight Norm (Mean): 0.1736198216676712\n",
      "MSE Loss: 0.0721, L1 Loss: 0.01*1.8855\n",
      "Batch number:  88\n",
      "Active Features: 46.39%\n",
      "Decoder Weight Norm (Mean): 0.1736288219690323\n",
      "MSE Loss: 0.0742, L1 Loss: 0.01*1.7984\n",
      "Batch number:  89\n",
      "Active Features: 45.70%\n",
      "Decoder Weight Norm (Mean): 0.17363794147968292\n",
      "MSE Loss: 0.0733, L1 Loss: 0.01*1.8000\n",
      "Batch number:  90\n",
      "Active Features: 47.91%\n",
      "Decoder Weight Norm (Mean): 0.1736472100019455\n",
      "MSE Loss: 0.0735, L1 Loss: 0.01*1.7974\n",
      "Batch number:  91\n",
      "Active Features: 46.57%\n",
      "Decoder Weight Norm (Mean): 0.17365631461143494\n",
      "MSE Loss: 0.0731, L1 Loss: 0.01*1.6647\n",
      "Batch number:  92\n",
      "Active Features: 47.22%\n",
      "Decoder Weight Norm (Mean): 0.17366552352905273\n",
      "MSE Loss: 0.0741, L1 Loss: 0.01*1.8970\n",
      "Batch number:  93\n",
      "Active Features: 48.91%\n",
      "Decoder Weight Norm (Mean): 0.17367465794086456\n",
      "MSE Loss: 0.0735, L1 Loss: 0.01*1.9257\n",
      "Batch number:  94\n",
      "Active Features: 45.28%\n",
      "Decoder Weight Norm (Mean): 0.17368385195732117\n",
      "MSE Loss: 0.0742, L1 Loss: 0.01*1.8970\n",
      "Batch number:  95\n",
      "Active Features: 43.88%\n",
      "Decoder Weight Norm (Mean): 0.17369328439235687\n",
      "MSE Loss: 0.0738, L1 Loss: 0.01*1.8709\n",
      "Batch number:  96\n",
      "Active Features: 46.71%\n",
      "Decoder Weight Norm (Mean): 0.17370277643203735\n",
      "MSE Loss: 0.0740, L1 Loss: 0.01*1.8209\n",
      "Batch number:  97\n",
      "Active Features: 46.59%\n",
      "Decoder Weight Norm (Mean): 0.1737125664949417\n",
      "MSE Loss: 0.0726, L1 Loss: 0.01*1.9088\n",
      "Batch number:  98\n",
      "Active Features: 49.38%\n",
      "Decoder Weight Norm (Mean): 0.17372219264507294\n",
      "MSE Loss: 0.0742, L1 Loss: 0.01*1.7116\n",
      "Batch number:  99\n",
      "Active Features: 45.01%\n",
      "Decoder Weight Norm (Mean): 0.17373180389404297\n",
      "MSE Loss: 0.0733, L1 Loss: 0.01*1.9117\n",
      "Batch number:  100\n",
      "Active Features: 47.49%\n",
      "Decoder Weight Norm (Mean): 0.17374128103256226\n",
      "MSE Loss: 0.0734, L1 Loss: 0.01*1.9052\n",
      "Batch number:  101\n",
      "Active Features: 45.98%\n",
      "Decoder Weight Norm (Mean): 0.1737508326768875\n",
      "MSE Loss: 0.0726, L1 Loss: 0.01*1.9634\n",
      "Batch number:  102\n",
      "Active Features: 47.32%\n",
      "Decoder Weight Norm (Mean): 0.1737602800130844\n",
      "MSE Loss: 0.0730, L1 Loss: 0.01*1.9768\n",
      "Batch number:  103\n",
      "Active Features: 49.81%\n",
      "Decoder Weight Norm (Mean): 0.17376989126205444\n",
      "MSE Loss: 0.0735, L1 Loss: 0.01*1.7807\n",
      "Batch number:  104\n",
      "Active Features: 43.90%\n",
      "Decoder Weight Norm (Mean): 0.17377959191799164\n",
      "MSE Loss: 0.0732, L1 Loss: 0.01*2.0103\n",
      "Batch number:  105\n",
      "Active Features: 47.34%\n",
      "Decoder Weight Norm (Mean): 0.17378924787044525\n",
      "MSE Loss: 0.0730, L1 Loss: 0.01*1.9425\n",
      "Batch number:  106\n",
      "Active Features: 43.66%\n",
      "Decoder Weight Norm (Mean): 0.17379869520664215\n",
      "MSE Loss: 0.0733, L1 Loss: 0.01*1.7392\n",
      "Batch number:  107\n",
      "Active Features: 47.20%\n",
      "Decoder Weight Norm (Mean): 0.17380817234516144\n",
      "MSE Loss: 0.0728, L1 Loss: 0.01*1.9075\n",
      "Batch number:  108\n",
      "Active Features: 44.40%\n",
      "Decoder Weight Norm (Mean): 0.1738176792860031\n",
      "MSE Loss: 0.0729, L1 Loss: 0.01*1.8321\n",
      "Batch number:  109\n",
      "Active Features: 47.52%\n",
      "Decoder Weight Norm (Mean): 0.17382705211639404\n",
      "MSE Loss: 0.0727, L1 Loss: 0.01*2.0090\n",
      "Batch number:  110\n",
      "Active Features: 46.66%\n",
      "Decoder Weight Norm (Mean): 0.17383627593517303\n",
      "MSE Loss: 0.0749, L1 Loss: 0.01*2.0981\n",
      "Batch number:  111\n",
      "Active Features: 47.66%\n",
      "Decoder Weight Norm (Mean): 0.17384538054466248\n",
      "MSE Loss: 0.0737, L1 Loss: 0.01*1.8257\n",
      "Batch number:  112\n",
      "Active Features: 48.84%\n",
      "Decoder Weight Norm (Mean): 0.17385460436344147\n",
      "MSE Loss: 0.0736, L1 Loss: 0.01*1.8072\n",
      "Batch number:  113\n",
      "Active Features: 43.75%\n",
      "Decoder Weight Norm (Mean): 0.17386381328105927\n",
      "MSE Loss: 0.0736, L1 Loss: 0.01*1.8100\n",
      "Batch number:  114\n",
      "Active Features: 47.47%\n",
      "Decoder Weight Norm (Mean): 0.1738729476928711\n",
      "MSE Loss: 0.0732, L1 Loss: 0.01*1.8414\n",
      "Batch number:  115\n",
      "Active Features: 46.27%\n",
      "Decoder Weight Norm (Mean): 0.1738819181919098\n",
      "MSE Loss: 0.0734, L1 Loss: 0.01*1.7687\n",
      "Batch number:  116\n",
      "Active Features: 44.12%\n",
      "Decoder Weight Norm (Mean): 0.17389070987701416\n",
      "MSE Loss: 0.0732, L1 Loss: 0.01*2.0658\n",
      "Batch number:  117\n",
      "Active Features: 46.42%\n",
      "Decoder Weight Norm (Mean): 0.17389966547489166\n",
      "MSE Loss: 0.0729, L1 Loss: 0.01*2.0190\n",
      "Batch number:  118\n",
      "Active Features: 46.22%\n",
      "Decoder Weight Norm (Mean): 0.17390857636928558\n",
      "MSE Loss: 0.0736, L1 Loss: 0.01*1.8126\n",
      "Batch number:  119\n",
      "Active Features: 44.45%\n",
      "Decoder Weight Norm (Mean): 0.17391760647296906\n",
      "MSE Loss: 0.0733, L1 Loss: 0.01*1.9224\n",
      "Batch number:  120\n",
      "Active Features: 43.61%\n",
      "Decoder Weight Norm (Mean): 0.17392684519290924\n",
      "MSE Loss: 0.0726, L1 Loss: 0.01*1.8758\n",
      "Batch number:  121\n",
      "Active Features: 45.59%\n",
      "Decoder Weight Norm (Mean): 0.17393605411052704\n",
      "MSE Loss: 0.0740, L1 Loss: 0.01*1.8765\n",
      "Batch number:  122\n",
      "Active Features: 45.15%\n",
      "Decoder Weight Norm (Mean): 0.1739453375339508\n",
      "MSE Loss: 0.0731, L1 Loss: 0.01*1.7724\n",
      "Batch number:  123\n",
      "Active Features: 52.18%\n",
      "Decoder Weight Norm (Mean): 0.17395466566085815\n",
      "MSE Loss: 0.0733, L1 Loss: 0.01*1.9306\n",
      "Batch number:  124\n",
      "Active Features: 41.98%\n",
      "Decoder Weight Norm (Mean): 0.17396385967731476\n",
      "MSE Loss: 0.0728, L1 Loss: 0.01*1.9416\n",
      "Batch number:  125\n",
      "Active Features: 42.90%\n",
      "Decoder Weight Norm (Mean): 0.17397303879261017\n",
      "MSE Loss: 0.0722, L1 Loss: 0.01*1.9936\n",
      "Batch number:  126\n",
      "Active Features: 42.12%\n",
      "Decoder Weight Norm (Mean): 0.17398232221603394\n",
      "MSE Loss: 0.0722, L1 Loss: 0.01*1.8607\n",
      "Batch number:  127\n",
      "Active Features: 47.14%\n",
      "Decoder Weight Norm (Mean): 0.1739916205406189\n",
      "MSE Loss: 0.0732, L1 Loss: 0.01*1.7075\n",
      "Epoch [9/10], Loss: 11.8131 ----------------------------\n",
      "Batch number:  0\n",
      "Active Features: 47.21%\n",
      "Decoder Weight Norm (Mean): 0.1740008145570755\n",
      "MSE Loss: 0.0731, L1 Loss: 0.01*1.6671\n",
      "Batch number:  1\n",
      "Active Features: 46.01%\n",
      "Decoder Weight Norm (Mean): 0.1740100085735321\n",
      "MSE Loss: 0.0720, L1 Loss: 0.01*1.8476\n",
      "Batch number:  2\n",
      "Active Features: 46.51%\n",
      "Decoder Weight Norm (Mean): 0.17401915788650513\n",
      "MSE Loss: 0.0732, L1 Loss: 0.01*1.7678\n",
      "Batch number:  3\n",
      "Active Features: 41.44%\n",
      "Decoder Weight Norm (Mean): 0.17402826249599457\n",
      "MSE Loss: 0.0720, L1 Loss: 0.01*1.7072\n",
      "Batch number:  4\n",
      "Active Features: 42.96%\n",
      "Decoder Weight Norm (Mean): 0.1740376502275467\n",
      "MSE Loss: 0.0721, L1 Loss: 0.01*1.8656\n",
      "Batch number:  5\n",
      "Active Features: 42.74%\n",
      "Decoder Weight Norm (Mean): 0.1740470975637436\n",
      "MSE Loss: 0.0734, L1 Loss: 0.01*1.9759\n",
      "Batch number:  6\n",
      "Active Features: 45.77%\n",
      "Decoder Weight Norm (Mean): 0.17405657470226288\n",
      "MSE Loss: 0.0733, L1 Loss: 0.01*1.7445\n",
      "Batch number:  7\n",
      "Active Features: 48.50%\n",
      "Decoder Weight Norm (Mean): 0.17406615614891052\n",
      "MSE Loss: 0.0737, L1 Loss: 0.01*1.9456\n",
      "Batch number:  8\n",
      "Active Features: 48.92%\n",
      "Decoder Weight Norm (Mean): 0.17407560348510742\n",
      "MSE Loss: 0.0735, L1 Loss: 0.01*1.9059\n",
      "Batch number:  9\n",
      "Active Features: 46.98%\n",
      "Decoder Weight Norm (Mean): 0.17408490180969238\n",
      "MSE Loss: 0.0724, L1 Loss: 0.01*1.9177\n",
      "Batch number:  10\n",
      "Active Features: 51.95%\n",
      "Decoder Weight Norm (Mean): 0.17409445345401764\n",
      "MSE Loss: 0.0738, L1 Loss: 0.01*1.8672\n",
      "Batch number:  11\n",
      "Active Features: 43.78%\n",
      "Decoder Weight Norm (Mean): 0.17410364747047424\n",
      "MSE Loss: 0.0733, L1 Loss: 0.01*2.0378\n",
      "Batch number:  12\n",
      "Active Features: 45.30%\n",
      "Decoder Weight Norm (Mean): 0.17411290109157562\n",
      "MSE Loss: 0.0730, L1 Loss: 0.01*1.6787\n",
      "Batch number:  13\n",
      "Active Features: 47.06%\n",
      "Decoder Weight Norm (Mean): 0.17412212491035461\n",
      "MSE Loss: 0.0734, L1 Loss: 0.01*1.8620\n",
      "Batch number:  14\n",
      "Active Features: 45.78%\n",
      "Decoder Weight Norm (Mean): 0.17413131892681122\n",
      "MSE Loss: 0.0726, L1 Loss: 0.01*1.9598\n",
      "Batch number:  15\n",
      "Active Features: 45.73%\n",
      "Decoder Weight Norm (Mean): 0.17414052784442902\n",
      "MSE Loss: 0.0724, L1 Loss: 0.01*2.0679\n",
      "Batch number:  16\n",
      "Active Features: 43.98%\n",
      "Decoder Weight Norm (Mean): 0.17414988577365875\n",
      "MSE Loss: 0.0741, L1 Loss: 0.01*1.8707\n",
      "Batch number:  17\n",
      "Active Features: 41.50%\n",
      "Decoder Weight Norm (Mean): 0.17415928840637207\n",
      "MSE Loss: 0.0733, L1 Loss: 0.01*1.7969\n",
      "Batch number:  18\n",
      "Active Features: 46.06%\n",
      "Decoder Weight Norm (Mean): 0.17416895925998688\n",
      "MSE Loss: 0.0736, L1 Loss: 0.01*1.8068\n",
      "Batch number:  19\n",
      "Active Features: 43.30%\n",
      "Decoder Weight Norm (Mean): 0.17417855560779572\n",
      "MSE Loss: 0.0730, L1 Loss: 0.01*1.8834\n",
      "Batch number:  20\n",
      "Active Features: 40.26%\n",
      "Decoder Weight Norm (Mean): 0.1741882562637329\n",
      "MSE Loss: 0.0735, L1 Loss: 0.01*1.8752\n",
      "Batch number:  21\n",
      "Active Features: 44.18%\n",
      "Decoder Weight Norm (Mean): 0.17419804632663727\n",
      "MSE Loss: 0.0733, L1 Loss: 0.01*1.8103\n",
      "Batch number:  22\n",
      "Active Features: 45.77%\n",
      "Decoder Weight Norm (Mean): 0.17420771718025208\n",
      "MSE Loss: 0.0729, L1 Loss: 0.01*1.8509\n",
      "Batch number:  23\n",
      "Active Features: 46.20%\n",
      "Decoder Weight Norm (Mean): 0.17421743273735046\n",
      "MSE Loss: 0.0737, L1 Loss: 0.01*1.8981\n",
      "Batch number:  24\n",
      "Active Features: 41.96%\n",
      "Decoder Weight Norm (Mean): 0.17422707378864288\n",
      "MSE Loss: 0.0731, L1 Loss: 0.01*1.7951\n",
      "Batch number:  25\n",
      "Active Features: 39.53%\n",
      "Decoder Weight Norm (Mean): 0.17423683404922485\n",
      "MSE Loss: 0.0727, L1 Loss: 0.01*1.8339\n",
      "Batch number:  26\n",
      "Active Features: 43.23%\n",
      "Decoder Weight Norm (Mean): 0.17424684762954712\n",
      "MSE Loss: 0.0729, L1 Loss: 0.01*1.9283\n",
      "Batch number:  27\n",
      "Active Features: 45.16%\n",
      "Decoder Weight Norm (Mean): 0.1742568016052246\n",
      "MSE Loss: 0.0719, L1 Loss: 0.01*1.8863\n",
      "Batch number:  28\n",
      "Active Features: 43.14%\n",
      "Decoder Weight Norm (Mean): 0.17426665127277374\n",
      "MSE Loss: 0.0728, L1 Loss: 0.01*1.8047\n",
      "Batch number:  29\n",
      "Active Features: 48.68%\n",
      "Decoder Weight Norm (Mean): 0.17427648603916168\n",
      "MSE Loss: 0.0728, L1 Loss: 0.01*1.7610\n",
      "Batch number:  30\n",
      "Active Features: 40.18%\n",
      "Decoder Weight Norm (Mean): 0.17428644001483917\n",
      "MSE Loss: 0.0722, L1 Loss: 0.01*2.0206\n",
      "Batch number:  31\n",
      "Active Features: 45.67%\n",
      "Decoder Weight Norm (Mean): 0.17429609596729279\n",
      "MSE Loss: 0.0726, L1 Loss: 0.01*1.8883\n",
      "Batch number:  32\n",
      "Active Features: 42.77%\n",
      "Decoder Weight Norm (Mean): 0.17430570721626282\n",
      "MSE Loss: 0.0730, L1 Loss: 0.01*1.9481\n",
      "Batch number:  33\n",
      "Active Features: 44.80%\n",
      "Decoder Weight Norm (Mean): 0.17431513965129852\n",
      "MSE Loss: 0.0729, L1 Loss: 0.01*1.8202\n",
      "Batch number:  34\n",
      "Active Features: 43.92%\n",
      "Decoder Weight Norm (Mean): 0.17432446777820587\n",
      "MSE Loss: 0.0728, L1 Loss: 0.01*1.7816\n",
      "Batch number:  35\n",
      "Active Features: 42.53%\n",
      "Decoder Weight Norm (Mean): 0.1743338257074356\n",
      "MSE Loss: 0.0721, L1 Loss: 0.01*1.8201\n",
      "Batch number:  36\n",
      "Active Features: 42.95%\n",
      "Decoder Weight Norm (Mean): 0.17434342205524445\n",
      "MSE Loss: 0.0728, L1 Loss: 0.01*1.7901\n",
      "Batch number:  37\n",
      "Active Features: 42.92%\n",
      "Decoder Weight Norm (Mean): 0.17435340583324432\n",
      "MSE Loss: 0.0728, L1 Loss: 0.01*2.2205\n",
      "Batch number:  38\n",
      "Active Features: 40.90%\n",
      "Decoder Weight Norm (Mean): 0.17436334490776062\n",
      "MSE Loss: 0.0728, L1 Loss: 0.01*1.7157\n",
      "Batch number:  39\n",
      "Active Features: 45.56%\n",
      "Decoder Weight Norm (Mean): 0.17437335848808289\n",
      "MSE Loss: 0.0733, L1 Loss: 0.01*1.7398\n",
      "Batch number:  40\n",
      "Active Features: 44.30%\n",
      "Decoder Weight Norm (Mean): 0.17438335716724396\n",
      "MSE Loss: 0.0730, L1 Loss: 0.01*2.0239\n",
      "Batch number:  41\n",
      "Active Features: 43.61%\n",
      "Decoder Weight Norm (Mean): 0.17439331114292145\n",
      "MSE Loss: 0.0723, L1 Loss: 0.01*1.9363\n",
      "Batch number:  42\n",
      "Active Features: 44.06%\n",
      "Decoder Weight Norm (Mean): 0.17440323531627655\n",
      "MSE Loss: 0.0723, L1 Loss: 0.01*1.8781\n",
      "Batch number:  43\n",
      "Active Features: 42.13%\n",
      "Decoder Weight Norm (Mean): 0.17441320419311523\n",
      "MSE Loss: 0.0725, L1 Loss: 0.01*2.0787\n",
      "Batch number:  44\n",
      "Active Features: 49.30%\n",
      "Decoder Weight Norm (Mean): 0.17442315816879272\n",
      "MSE Loss: 0.0732, L1 Loss: 0.01*1.9661\n",
      "Batch number:  45\n",
      "Active Features: 44.09%\n",
      "Decoder Weight Norm (Mean): 0.17443296313285828\n",
      "MSE Loss: 0.0727, L1 Loss: 0.01*1.9344\n",
      "Batch number:  46\n",
      "Active Features: 45.97%\n",
      "Decoder Weight Norm (Mean): 0.17444266378879547\n",
      "MSE Loss: 0.0736, L1 Loss: 0.01*1.9050\n",
      "Batch number:  47\n",
      "Active Features: 40.59%\n",
      "Decoder Weight Norm (Mean): 0.17445246875286102\n",
      "MSE Loss: 0.0716, L1 Loss: 0.01*1.8451\n",
      "Batch number:  48\n",
      "Active Features: 44.95%\n",
      "Decoder Weight Norm (Mean): 0.17446252703666687\n",
      "MSE Loss: 0.0739, L1 Loss: 0.01*1.8735\n",
      "Batch number:  49\n",
      "Active Features: 41.91%\n",
      "Decoder Weight Norm (Mean): 0.17447258532047272\n",
      "MSE Loss: 0.0728, L1 Loss: 0.01*1.7887\n",
      "Batch number:  50\n",
      "Active Features: 41.68%\n",
      "Decoder Weight Norm (Mean): 0.17448261380195618\n",
      "MSE Loss: 0.0726, L1 Loss: 0.01*1.7867\n",
      "Batch number:  51\n",
      "Active Features: 44.89%\n",
      "Decoder Weight Norm (Mean): 0.17449255287647247\n",
      "MSE Loss: 0.0728, L1 Loss: 0.01*1.8028\n",
      "Batch number:  52\n",
      "Active Features: 44.53%\n",
      "Decoder Weight Norm (Mean): 0.17450228333473206\n",
      "MSE Loss: 0.0723, L1 Loss: 0.01*1.9430\n",
      "Batch number:  53\n",
      "Active Features: 45.62%\n",
      "Decoder Weight Norm (Mean): 0.17451196908950806\n",
      "MSE Loss: 0.0732, L1 Loss: 0.01*1.8343\n",
      "Batch number:  54\n",
      "Active Features: 43.06%\n",
      "Decoder Weight Norm (Mean): 0.17452186346054077\n",
      "MSE Loss: 0.0715, L1 Loss: 0.01*1.8130\n",
      "Batch number:  55\n",
      "Active Features: 43.36%\n",
      "Decoder Weight Norm (Mean): 0.17453190684318542\n",
      "MSE Loss: 0.0723, L1 Loss: 0.01*1.8236\n",
      "Batch number:  56\n",
      "Active Features: 44.59%\n",
      "Decoder Weight Norm (Mean): 0.17454178631305695\n",
      "MSE Loss: 0.0725, L1 Loss: 0.01*1.6362\n",
      "Batch number:  57\n",
      "Active Features: 44.94%\n",
      "Decoder Weight Norm (Mean): 0.1745515763759613\n",
      "MSE Loss: 0.0736, L1 Loss: 0.01*1.8003\n",
      "Batch number:  58\n",
      "Active Features: 40.22%\n",
      "Decoder Weight Norm (Mean): 0.1745612919330597\n",
      "MSE Loss: 0.0720, L1 Loss: 0.01*1.7348\n",
      "Batch number:  59\n",
      "Active Features: 44.83%\n",
      "Decoder Weight Norm (Mean): 0.1745714396238327\n",
      "MSE Loss: 0.0733, L1 Loss: 0.01*1.9416\n",
      "Batch number:  60\n",
      "Active Features: 43.33%\n",
      "Decoder Weight Norm (Mean): 0.1745813935995102\n",
      "MSE Loss: 0.0727, L1 Loss: 0.01*1.8396\n",
      "Batch number:  61\n",
      "Active Features: 43.64%\n",
      "Decoder Weight Norm (Mean): 0.1745913177728653\n",
      "MSE Loss: 0.0730, L1 Loss: 0.01*1.7388\n",
      "Batch number:  62\n",
      "Active Features: 47.21%\n",
      "Decoder Weight Norm (Mean): 0.174601212143898\n",
      "MSE Loss: 0.0724, L1 Loss: 0.01*1.8832\n",
      "Batch number:  63\n",
      "Active Features: 41.03%\n",
      "Decoder Weight Norm (Mean): 0.17461085319519043\n",
      "MSE Loss: 0.0727, L1 Loss: 0.01*1.8502\n",
      "Batch number:  64\n",
      "Active Features: 48.12%\n",
      "Decoder Weight Norm (Mean): 0.1746203750371933\n",
      "MSE Loss: 0.0732, L1 Loss: 0.01*1.9232\n",
      "Batch number:  65\n",
      "Active Features: 40.28%\n",
      "Decoder Weight Norm (Mean): 0.17463010549545288\n",
      "MSE Loss: 0.0724, L1 Loss: 0.01*1.7960\n",
      "Batch number:  66\n",
      "Active Features: 42.49%\n",
      "Decoder Weight Norm (Mean): 0.17463985085487366\n",
      "MSE Loss: 0.0718, L1 Loss: 0.01*2.0364\n",
      "Batch number:  67\n",
      "Active Features: 40.02%\n",
      "Decoder Weight Norm (Mean): 0.17464962601661682\n",
      "MSE Loss: 0.0724, L1 Loss: 0.01*1.8737\n",
      "Batch number:  68\n",
      "Active Features: 45.45%\n",
      "Decoder Weight Norm (Mean): 0.17465956509113312\n",
      "MSE Loss: 0.0718, L1 Loss: 0.01*1.9120\n",
      "Batch number:  69\n",
      "Active Features: 40.26%\n",
      "Decoder Weight Norm (Mean): 0.17466939985752106\n",
      "MSE Loss: 0.0717, L1 Loss: 0.01*2.1222\n",
      "Batch number:  70\n",
      "Active Features: 40.99%\n",
      "Decoder Weight Norm (Mean): 0.17467929422855377\n",
      "MSE Loss: 0.0719, L1 Loss: 0.01*1.7607\n",
      "Batch number:  71\n",
      "Active Features: 40.76%\n",
      "Decoder Weight Norm (Mean): 0.17468920350074768\n",
      "MSE Loss: 0.0720, L1 Loss: 0.01*1.8783\n",
      "Batch number:  72\n",
      "Active Features: 41.70%\n",
      "Decoder Weight Norm (Mean): 0.174699068069458\n",
      "MSE Loss: 0.0732, L1 Loss: 0.01*1.7642\n",
      "Batch number:  73\n",
      "Active Features: 39.41%\n",
      "Decoder Weight Norm (Mean): 0.17470890283584595\n",
      "MSE Loss: 0.0724, L1 Loss: 0.01*1.9183\n",
      "Batch number:  74\n",
      "Active Features: 41.31%\n",
      "Decoder Weight Norm (Mean): 0.1747189462184906\n",
      "MSE Loss: 0.0712, L1 Loss: 0.01*1.8592\n",
      "Batch number:  75\n",
      "Active Features: 43.86%\n",
      "Decoder Weight Norm (Mean): 0.17472927272319794\n",
      "MSE Loss: 0.0727, L1 Loss: 0.01*1.7725\n",
      "Batch number:  76\n",
      "Active Features: 46.47%\n",
      "Decoder Weight Norm (Mean): 0.17473948001861572\n",
      "MSE Loss: 0.0721, L1 Loss: 0.01*1.7410\n",
      "Batch number:  77\n",
      "Active Features: 42.87%\n",
      "Decoder Weight Norm (Mean): 0.17474956810474396\n",
      "MSE Loss: 0.0714, L1 Loss: 0.01*2.0729\n",
      "Batch number:  78\n",
      "Active Features: 46.27%\n",
      "Decoder Weight Norm (Mean): 0.1747596114873886\n",
      "MSE Loss: 0.0732, L1 Loss: 0.01*1.9257\n",
      "Batch number:  79\n",
      "Active Features: 43.48%\n",
      "Decoder Weight Norm (Mean): 0.17476941645145416\n",
      "MSE Loss: 0.0727, L1 Loss: 0.01*1.9102\n",
      "Batch number:  80\n",
      "Active Features: 41.69%\n",
      "Decoder Weight Norm (Mean): 0.17477929592132568\n",
      "MSE Loss: 0.0723, L1 Loss: 0.01*1.8960\n",
      "Batch number:  81\n",
      "Active Features: 40.78%\n",
      "Decoder Weight Norm (Mean): 0.17478927969932556\n",
      "MSE Loss: 0.0721, L1 Loss: 0.01*1.6873\n",
      "Batch number:  82\n",
      "Active Features: 41.45%\n",
      "Decoder Weight Norm (Mean): 0.17479926347732544\n",
      "MSE Loss: 0.0714, L1 Loss: 0.01*1.8271\n",
      "Batch number:  83\n",
      "Active Features: 44.16%\n",
      "Decoder Weight Norm (Mean): 0.1748093217611313\n",
      "MSE Loss: 0.0729, L1 Loss: 0.01*1.9563\n",
      "Batch number:  84\n",
      "Active Features: 40.91%\n",
      "Decoder Weight Norm (Mean): 0.17481940984725952\n",
      "MSE Loss: 0.0707, L1 Loss: 0.01*1.9332\n",
      "Batch number:  85\n",
      "Active Features: 41.32%\n",
      "Decoder Weight Norm (Mean): 0.17482951283454895\n",
      "MSE Loss: 0.0720, L1 Loss: 0.01*1.8034\n",
      "Batch number:  86\n",
      "Active Features: 39.40%\n",
      "Decoder Weight Norm (Mean): 0.17483961582183838\n",
      "MSE Loss: 0.0714, L1 Loss: 0.01*1.8232\n",
      "Batch number:  87\n",
      "Active Features: 41.77%\n",
      "Decoder Weight Norm (Mean): 0.17485009133815765\n",
      "MSE Loss: 0.0721, L1 Loss: 0.01*1.8013\n",
      "Batch number:  88\n",
      "Active Features: 40.98%\n",
      "Decoder Weight Norm (Mean): 0.17486052215099335\n",
      "MSE Loss: 0.0718, L1 Loss: 0.01*1.9728\n",
      "Batch number:  89\n",
      "Active Features: 42.95%\n",
      "Decoder Weight Norm (Mean): 0.1748710423707962\n",
      "MSE Loss: 0.0721, L1 Loss: 0.01*1.8822\n",
      "Batch number:  90\n",
      "Active Features: 41.92%\n",
      "Decoder Weight Norm (Mean): 0.17488139867782593\n",
      "MSE Loss: 0.0719, L1 Loss: 0.01*1.8666\n",
      "Batch number:  91\n",
      "Active Features: 40.74%\n",
      "Decoder Weight Norm (Mean): 0.17489205300807953\n",
      "MSE Loss: 0.0724, L1 Loss: 0.01*1.7802\n",
      "Batch number:  92\n",
      "Active Features: 39.90%\n",
      "Decoder Weight Norm (Mean): 0.17490266263484955\n",
      "MSE Loss: 0.0717, L1 Loss: 0.01*1.8264\n",
      "Batch number:  93\n",
      "Active Features: 43.15%\n",
      "Decoder Weight Norm (Mean): 0.1749131679534912\n",
      "MSE Loss: 0.0716, L1 Loss: 0.01*1.7620\n",
      "Batch number:  94\n",
      "Active Features: 37.74%\n",
      "Decoder Weight Norm (Mean): 0.1749238520860672\n",
      "MSE Loss: 0.0723, L1 Loss: 0.01*1.8433\n",
      "Batch number:  95\n",
      "Active Features: 38.10%\n",
      "Decoder Weight Norm (Mean): 0.1749345362186432\n",
      "MSE Loss: 0.0724, L1 Loss: 0.01*2.0389\n",
      "Batch number:  96\n",
      "Active Features: 40.29%\n",
      "Decoder Weight Norm (Mean): 0.1749451607465744\n",
      "MSE Loss: 0.0734, L1 Loss: 0.01*1.8440\n",
      "Batch number:  97\n",
      "Active Features: 39.90%\n",
      "Decoder Weight Norm (Mean): 0.17495585978031158\n",
      "MSE Loss: 0.0715, L1 Loss: 0.01*1.8698\n",
      "Batch number:  98\n",
      "Active Features: 43.37%\n",
      "Decoder Weight Norm (Mean): 0.1749662607908249\n",
      "MSE Loss: 0.0729, L1 Loss: 0.01*1.8275\n",
      "Batch number:  99\n",
      "Active Features: 39.88%\n",
      "Decoder Weight Norm (Mean): 0.1749766319990158\n",
      "MSE Loss: 0.0722, L1 Loss: 0.01*1.8837\n",
      "Batch number:  100\n",
      "Active Features: 40.52%\n",
      "Decoder Weight Norm (Mean): 0.1749870628118515\n",
      "MSE Loss: 0.0735, L1 Loss: 0.01*1.8762\n",
      "Batch number:  101\n",
      "Active Features: 38.09%\n",
      "Decoder Weight Norm (Mean): 0.17499756813049316\n",
      "MSE Loss: 0.0719, L1 Loss: 0.01*1.7015\n",
      "Batch number:  102\n",
      "Active Features: 41.64%\n",
      "Decoder Weight Norm (Mean): 0.1750081330537796\n",
      "MSE Loss: 0.0708, L1 Loss: 0.01*1.7549\n",
      "Batch number:  103\n",
      "Active Features: 42.04%\n",
      "Decoder Weight Norm (Mean): 0.175018772482872\n",
      "MSE Loss: 0.0716, L1 Loss: 0.01*1.9495\n",
      "Batch number:  104\n",
      "Active Features: 39.71%\n",
      "Decoder Weight Norm (Mean): 0.17502951622009277\n",
      "MSE Loss: 0.0721, L1 Loss: 0.01*1.8587\n",
      "Batch number:  105\n",
      "Active Features: 43.82%\n",
      "Decoder Weight Norm (Mean): 0.17504024505615234\n",
      "MSE Loss: 0.0726, L1 Loss: 0.01*2.0244\n",
      "Batch number:  106\n",
      "Active Features: 38.31%\n",
      "Decoder Weight Norm (Mean): 0.1750510036945343\n",
      "MSE Loss: 0.0720, L1 Loss: 0.01*2.0007\n",
      "Batch number:  107\n",
      "Active Features: 41.62%\n",
      "Decoder Weight Norm (Mean): 0.17506195604801178\n",
      "MSE Loss: 0.0715, L1 Loss: 0.01*2.0309\n",
      "Batch number:  108\n",
      "Active Features: 38.39%\n",
      "Decoder Weight Norm (Mean): 0.1750728040933609\n",
      "MSE Loss: 0.0717, L1 Loss: 0.01*1.7439\n",
      "Batch number:  109\n",
      "Active Features: 39.71%\n",
      "Decoder Weight Norm (Mean): 0.17508356273174286\n",
      "MSE Loss: 0.0720, L1 Loss: 0.01*1.7717\n",
      "Batch number:  110\n",
      "Active Features: 41.60%\n",
      "Decoder Weight Norm (Mean): 0.1750941127538681\n",
      "MSE Loss: 0.0736, L1 Loss: 0.01*1.8829\n",
      "Batch number:  111\n",
      "Active Features: 40.67%\n",
      "Decoder Weight Norm (Mean): 0.17510470747947693\n",
      "MSE Loss: 0.0723, L1 Loss: 0.01*1.8568\n",
      "Batch number:  112\n",
      "Active Features: 41.98%\n",
      "Decoder Weight Norm (Mean): 0.17511533200740814\n",
      "MSE Loss: 0.0712, L1 Loss: 0.01*1.8047\n",
      "Batch number:  113\n",
      "Active Features: 39.34%\n",
      "Decoder Weight Norm (Mean): 0.17512597143650055\n",
      "MSE Loss: 0.0731, L1 Loss: 0.01*1.6569\n",
      "Batch number:  114\n",
      "Active Features: 43.39%\n",
      "Decoder Weight Norm (Mean): 0.1751365214586258\n",
      "MSE Loss: 0.0732, L1 Loss: 0.01*1.7048\n",
      "Batch number:  115\n",
      "Active Features: 41.79%\n",
      "Decoder Weight Norm (Mean): 0.1751469075679779\n",
      "MSE Loss: 0.0726, L1 Loss: 0.01*1.9866\n",
      "Batch number:  116\n",
      "Active Features: 39.47%\n",
      "Decoder Weight Norm (Mean): 0.17515721917152405\n",
      "MSE Loss: 0.0718, L1 Loss: 0.01*1.8838\n",
      "Batch number:  117\n",
      "Active Features: 39.88%\n",
      "Decoder Weight Norm (Mean): 0.17516745626926422\n",
      "MSE Loss: 0.0724, L1 Loss: 0.01*2.0487\n",
      "Batch number:  118\n",
      "Active Features: 41.05%\n",
      "Decoder Weight Norm (Mean): 0.1751774698495865\n",
      "MSE Loss: 0.0725, L1 Loss: 0.01*1.9314\n",
      "Batch number:  119\n",
      "Active Features: 40.01%\n",
      "Decoder Weight Norm (Mean): 0.17518754303455353\n",
      "MSE Loss: 0.0719, L1 Loss: 0.01*1.8825\n",
      "Batch number:  120\n",
      "Active Features: 38.64%\n",
      "Decoder Weight Norm (Mean): 0.17519772052764893\n",
      "MSE Loss: 0.0714, L1 Loss: 0.01*1.8921\n",
      "Batch number:  121\n",
      "Active Features: 39.86%\n",
      "Decoder Weight Norm (Mean): 0.17520791292190552\n",
      "MSE Loss: 0.0725, L1 Loss: 0.01*1.8564\n",
      "Batch number:  122\n",
      "Active Features: 41.34%\n",
      "Decoder Weight Norm (Mean): 0.17521806061267853\n",
      "MSE Loss: 0.0719, L1 Loss: 0.01*1.8045\n",
      "Batch number:  123\n",
      "Active Features: 48.63%\n",
      "Decoder Weight Norm (Mean): 0.1752282828092575\n",
      "MSE Loss: 0.0723, L1 Loss: 0.01*1.8190\n",
      "Batch number:  124\n",
      "Active Features: 39.72%\n",
      "Decoder Weight Norm (Mean): 0.1752384752035141\n",
      "MSE Loss: 0.0729, L1 Loss: 0.01*1.8489\n",
      "Batch number:  125\n",
      "Active Features: 39.98%\n",
      "Decoder Weight Norm (Mean): 0.17524857819080353\n",
      "MSE Loss: 0.0713, L1 Loss: 0.01*1.8697\n",
      "Batch number:  126\n",
      "Active Features: 39.24%\n",
      "Decoder Weight Norm (Mean): 0.17525868117809296\n",
      "MSE Loss: 0.0710, L1 Loss: 0.01*1.7766\n",
      "Batch number:  127\n",
      "Active Features: 44.47%\n",
      "Decoder Weight Norm (Mean): 0.17526856064796448\n",
      "MSE Loss: 0.0723, L1 Loss: 0.01*1.9908\n",
      "Epoch [10/10], Loss: 11.6661 ----------------------------\n",
      "Time taken: 1931.64 seconds\n"
     ]
    }
   ],
   "source": [
    "# Create dataset and dataloader\n",
    "data_dir = \"activations_data\"\n",
    "dataset = ActivationDataset(\n",
    "    data_dir, \n",
    "    batch_size=2048, # 8192 examples per batch\n",
    "    f_type=\"train\", \n",
    "    test_fraction=0.01, \n",
    "    scale_factor=scale_factor, \n",
    "    seed=42\n",
    ")\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=False) # take 1 batch at a time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize model parameters\n",
    "input_dim = 3072  \n",
    "hidden_dim = 20000 # = 4096  # Adjust based on your requirements\n",
    "\n",
    "# Initialize the model\n",
    "model = SparseAutoencoder(input_dim, hidden_dim).to(device)\n",
    "\n",
    "# Set up loss function and optimizer\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "l1_lambda = 0.01  # Regularization strength for sparsity\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10 # should be 1 \n",
    "\n",
    "print(f\"Training Parameters:\\n\"\n",
    "      f\"Data Directory: {data_dir}\\n\"\n",
    "      f\"Batch Size: {dataset.batch_size}\\n\"\n",
    "      f\"Test Fraction: {dataset.test_fraction}\\n\"\n",
    "      f\"Scale Factor: {scale_factor}\\n\"\n",
    "      f\"Seed: {dataset.seed}\\n\"\n",
    "      f\"Optimizer: {optimizer.__class__.__name__}\\n\"\n",
    "      f\"Learning Rate: {optimizer.param_groups[0]['lr']}\\n\"\n",
    "      f\"L1 Lambda: {l1_lambda}\\n\"\n",
    "      f\"Number of Epochs: {num_epochs}\\n\"\n",
    "      f\"SAE Input Dimension: {input_dim}\\n\"\n",
    "      f\"SAE Hidden Dimension: {hidden_dim}\\n\"\n",
    "      f\"-----------------------------------\"\n",
    ")\n",
    "time_start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        print(\"Batch number: \", i)\n",
    "        # batch is already of shape (1, 8192, 3072)        \n",
    "        # Forward pass\n",
    "        batch = batch.to(device)\n",
    "        outputs, encoded = model(batch) # remove extra dimension from DataLoader\n",
    "        mse_loss = criterion(outputs, batch)\n",
    "\n",
    "        # Active features calculation\n",
    "        # Count how many latent features (columns) are active (nonzero for any token in the batch)\n",
    "        active_features = torch.any(encoded > 0, dim=1).sum().item()\n",
    "        total_features = encoded.shape[2]  # Number of latent features (4096 in this case)\n",
    "        active_percentage = active_features / total_features * 100\n",
    "        # print(f\"Encoded Activation Range: {torch.min(encoded).item()}, {torch.max(encoded).item()}\")\n",
    "        print(f\"Active Features: {active_percentage:.2f}%\")\n",
    "\n",
    "        # Add L1 regularization for sparsity\n",
    "        # decoder_weight_norms = torch.norm(model.decoder.weight, 2, dim=0)\n",
    "        # l1_loss = torch.norm(encoded, 1) * decoder_weight_norms.sum()\n",
    "        # l1_loss = torch.sum(torch.norm(model.decoder.weight, 2, dim=0) * torch.norm(encoded, 1, dim=(0, 1)))\n",
    "        decoder_weight_norms = torch.norm(model.decoder.weight, p=2, dim=0)  # Shape: [num_features]\n",
    "        l1_terms = encoded * decoder_weight_norms.unsqueeze(0)  # Shape: [batch_size, num_features]\n",
    "        l1_loss_per_sample = torch.sum(l1_terms, dim=1)  # Shape: [batch_size]\n",
    "        l1_loss = torch.mean(l1_loss_per_sample)\n",
    "\n",
    "        loss = mse_loss + l1_lambda * l1_loss\n",
    "\n",
    "        print(f\"Decoder Weight Norm (Mean): {torch.norm(model.decoder.weight, dim=0).mean().item()}\")\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        print(f\"MSE Loss: {mse_loss.item():.4f}, L1 Loss: {l1_lambda}*{l1_loss.item():.4f}\")\n",
    "        # if mse_loss.item() < 0.1:\n",
    "        #     optimizer.lr = 0.001\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f} ----------------------------\")\n",
    "time_end = time.time()\n",
    "print(f\"Time taken: {time_end - time_start:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(39824.7422, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.norm(model.decoder.weight, 2, dim=0) * torch.norm(encoded, 1, dim=(0, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.9043e+08, device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_weight_norms = torch.norm(model.decoder.weight, 2, dim=0)\n",
    "torch.norm(encoded, 1) * decoder_weight_norms.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9912, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_weight_norms = torch.norm(model.decoder.weight, p=2, dim=0)  # Shape: [num_features]\n",
    "\n",
    "# Compute L1 loss\n",
    "l1_terms = encoded * decoder_weight_norms.unsqueeze(0)  # Shape: [batch_size, num_features]\n",
    "l1_loss_per_sample = torch.sum(l1_terms, dim=1)  # Shape: [batch_size]\n",
    "l1_loss = torch.mean(l1_loss_per_sample)\n",
    "l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0723, device='cuda:0', grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.functional.mse_loss(outputs, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4397, -0.2815, -0.1868,  ...,  0.0095, -0.0852,  0.0508],\n",
       "         [ 0.2226, -0.3894,  0.2339,  ..., -0.0876, -0.2494,  0.0829],\n",
       "         [-0.1384,  0.2993,  0.6459,  ...,  0.5884,  0.3007, -0.2561],\n",
       "         ...,\n",
       "         [-0.0704, -0.2355, -0.1776,  ...,  0.0759, -0.7137, -0.0335],\n",
       "         [-0.3434,  0.1598,  0.3406,  ...,  0.1457,  0.1715,  0.2137],\n",
       "         [-0.4306,  0.0175, -0.2405,  ..., -0.2906, -0.0880,  0.0143]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324 2988 5464 6790 7295 7617 8209 8866 9559 10517 10949 13383 13697 14053 14992 15418 16107 16334 16470 16934 17012 17403 17483 17806 18309 19488 19532 "
     ]
    }
   ],
   "source": [
    "for i, e in enumerate(encoded[0][0].tolist()):\n",
    "    if e > 0:\n",
    "        print(i, end=\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1171, -0.1708, -0.4158,  ...,  0.0361, -0.1209,  0.0342],\n",
       "         [ 0.1363, -0.0606, -0.5527,  ..., -0.0951, -0.1431, -0.1023],\n",
       "         [ 0.0615, -0.0455,  0.6307,  ..., -0.0054, -0.1843, -0.0567],\n",
       "         ...,\n",
       "         [-0.1891, -0.1686, -0.0410,  ..., -0.1083, -0.6818, -0.0256],\n",
       "         [ 0.0737,  0.1446,  0.0732,  ...,  0.0307, -0.2643, -0.1383],\n",
       "         [-0.0130, -0.1322,  0.1182,  ...,  0.0025, -0.2267, -0.0295]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0723, device='cuda:0', grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(outputs,batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), f\"models/sparse_autoencoder_tmp.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model deleted and GPU memory freed\n"
     ]
    }
   ],
   "source": [
    "# clear gpu\n",
    "del model\n",
    "del data_loader\n",
    "del dataset\n",
    "del criterion\n",
    "del optimizer\n",
    "del batch\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Model deleted and GPU memory freed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"activations_data\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_dim = 3072  \n",
    "hidden_dim = 20000 # = 4096  # Adjust based on your requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SparseAutoencoder(input_dim, hidden_dim).to(device)\n",
    "model.load_state_dict(torch.load(\"models/sparse_autoencoder_tmp.pth\"))\n",
    "\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "l1_lambda = 0.01  # Regularization strength for sparsity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 130 batches for train set\n",
      "Percent Active Features: 62.90%\n",
      "MSE Loss: 0.0718, L1 Loss: 3.9542, Explained Var: 0.9430\n",
      "Percent Active Features: 58.57%\n",
      "MSE Loss: 0.0707, L1 Loss: 3.7889, Explained Var: 0.9261\n",
      "Percent Active Features: 60.22%\n",
      "MSE Loss: 0.0719, L1 Loss: 3.8433, Explained Var: 0.9351\n",
      "Percent Active Features: 59.35%\n",
      "MSE Loss: 0.0710, L1 Loss: 4.0171, Explained Var: 0.9468\n",
      "Percent Active Features: 55.31%\n",
      "MSE Loss: 0.0714, L1 Loss: 3.7788, Explained Var: 0.9308\n",
      "Percent Active Features: 57.20%\n",
      "MSE Loss: 0.0727, L1 Loss: 3.8381, Explained Var: 0.9297\n",
      "Percent Active Features: 57.39%\n",
      "MSE Loss: 0.0717, L1 Loss: 3.8512, Explained Var: 0.9394\n",
      "Percent Active Features: 57.26%\n",
      "MSE Loss: 0.0725, L1 Loss: 3.7483, Explained Var: 0.9178\n",
      "Percent Active Features: 60.99%\n",
      "MSE Loss: 0.0719, L1 Loss: 3.8362, Explained Var: 0.9352\n",
      "Percent Active Features: 55.90%\n",
      "MSE Loss: 0.0709, L1 Loss: 3.6664, Explained Var: 0.9118\n",
      "Percent Active Features: 63.76%\n",
      "MSE Loss: 0.0720, L1 Loss: 3.8200, Explained Var: 0.9351\n",
      "Percent Active Features: 54.10%\n",
      "MSE Loss: 0.0716, L1 Loss: 3.9722, Explained Var: 0.9432\n",
      "Percent Active Features: 55.06%\n",
      "MSE Loss: 0.0723, L1 Loss: 3.7287, Explained Var: 0.9180\n",
      "Percent Active Features: 60.17%\n",
      "MSE Loss: 0.0716, L1 Loss: 3.8278, Explained Var: 0.9354\n",
      "Percent Active Features: 58.03%\n",
      "MSE Loss: 0.0719, L1 Loss: 3.4655, Explained Var: 0.8752\n",
      "Percent Active Features: 57.02%\n",
      "MSE Loss: 0.0713, L1 Loss: 3.7596, Explained Var: 0.9255\n",
      "Percent Active Features: 59.81%\n",
      "MSE Loss: 0.0728, L1 Loss: 3.6285, Explained Var: 0.9004\n",
      "Percent Active Features: 55.03%\n",
      "MSE Loss: 0.0716, L1 Loss: 3.9613, Explained Var: 0.9432\n",
      "Percent Active Features: 60.19%\n",
      "MSE Loss: 0.0720, L1 Loss: 3.7782, Explained Var: 0.9303\n",
      "Percent Active Features: 59.55%\n",
      "MSE Loss: 0.0724, L1 Loss: 3.7475, Explained Var: 0.9298\n",
      "Percent Active Features: 58.05%\n",
      "MSE Loss: 0.0716, L1 Loss: 3.7161, Explained Var: 0.9187\n",
      "Percent Active Features: 59.11%\n",
      "MSE Loss: 0.0722, L1 Loss: 4.0203, Explained Var: 0.9489\n",
      "Percent Active Features: 58.75%\n",
      "MSE Loss: 0.0717, L1 Loss: 3.8529, Explained Var: 0.9395\n",
      "Percent Active Features: 61.06%\n",
      "MSE Loss: 0.0732, L1 Loss: 3.8505, Explained Var: 0.9341\n",
      "Percent Active Features: 56.06%\n",
      "MSE Loss: 0.0724, L1 Loss: 3.6090, Explained Var: 0.9008\n",
      "Percent Active Features: 54.62%\n",
      "MSE Loss: 0.0719, L1 Loss: 3.6655, Explained Var: 0.9108\n",
      "Percent Active Features: 56.27%\n",
      "MSE Loss: 0.0711, L1 Loss: 3.6738, Explained Var: 0.9191\n",
      "Percent Active Features: 59.10%\n",
      "MSE Loss: 0.0714, L1 Loss: 3.6853, Explained Var: 0.9189\n",
      "Percent Active Features: 57.93%\n",
      "MSE Loss: 0.0719, L1 Loss: 3.6112, Explained Var: 0.9106\n",
      "Percent Active Features: 62.46%\n",
      "MSE Loss: 0.0725, L1 Loss: 4.0489, Explained Var: 0.9513\n",
      "Percent Active Features: 56.35%\n",
      "MSE Loss: 0.0718, L1 Loss: 3.4994, Explained Var: 0.8756\n",
      "Percent Active Features: 57.69%\n",
      "MSE Loss: 0.0721, L1 Loss: 3.7932, Explained Var: 0.9349\n",
      "Percent Active Features: 56.88%\n",
      "MSE Loss: 0.0719, L1 Loss: 3.5479, Explained Var: 0.8756\n",
      "Percent Active Features: 58.94%\n",
      "MSE Loss: 0.0721, L1 Loss: 4.0585, Explained Var: 0.9516\n",
      "Percent Active Features: 54.14%\n",
      "MSE Loss: 0.0718, L1 Loss: 3.7405, Explained Var: 0.9186\n",
      "Percent Active Features: 56.68%\n",
      "MSE Loss: 0.0713, L1 Loss: 3.7082, Explained Var: 0.9253\n",
      "Percent Active Features: 57.34%\n",
      "MSE Loss: 0.0717, L1 Loss: 3.9304, Explained Var: 0.9395\n",
      "Percent Active Features: 56.08%\n",
      "MSE Loss: 0.0722, L1 Loss: 3.6337, Explained Var: 0.9104\n",
      "Percent Active Features: 56.62%\n",
      "MSE Loss: 0.0720, L1 Loss: 3.7805, Explained Var: 0.9303\n",
      "Percent Active Features: 57.84%\n",
      "MSE Loss: 0.0723, L1 Loss: 3.4645, Explained Var: 0.8746\n",
      "Percent Active Features: 56.73%\n",
      "MSE Loss: 0.0721, L1 Loss: 3.6409, Explained Var: 0.9104\n",
      "Percent Active Features: 54.60%\n",
      "MSE Loss: 0.0720, L1 Loss: 3.5195, Explained Var: 0.8753\n",
      "Percent Active Features: 57.71%\n",
      "MSE Loss: 0.0717, L1 Loss: 3.6013, Explained Var: 0.9016\n",
      "Percent Active Features: 57.55%\n",
      "MSE Loss: 0.0719, L1 Loss: 3.9012, Explained Var: 0.9393\n",
      "Percent Active Features: 64.66%\n",
      "MSE Loss: 0.0719, L1 Loss: 3.6916, Explained Var: 0.9248\n",
      "Percent Active Features: 61.97%\n",
      "MSE Loss: 0.0721, L1 Loss: 4.0451, Explained Var: 0.9489\n",
      "Percent Active Features: 59.16%\n",
      "MSE Loss: 0.0725, L1 Loss: 3.8605, Explained Var: 0.9388\n",
      "Percent Active Features: 56.95%\n",
      "MSE Loss: 0.0708, L1 Loss: 3.7885, Explained Var: 0.9313\n",
      "Percent Active Features: 57.42%\n",
      "MSE Loss: 0.0720, L1 Loss: 3.4125, Explained Var: 0.8309\n",
      "Percent Active Features: 57.90%\n",
      "MSE Loss: 0.0718, L1 Loss: 3.6886, Explained Var: 0.9185\n",
      "Percent Active Features: 57.46%\n",
      "MSE Loss: 0.0721, L1 Loss: 3.8729, Explained Var: 0.9391\n",
      "Percent Active Features: 61.98%\n",
      "MSE Loss: 0.0724, L1 Loss: 3.5707, Explained Var: 0.9007\n",
      "Percent Active Features: 59.14%\n",
      "MSE Loss: 0.0719, L1 Loss: 3.7653, Explained Var: 0.9249\n",
      "Percent Active Features: 61.01%\n",
      "MSE Loss: 0.0721, L1 Loss: 3.6670, Explained Var: 0.9104\n",
      "Percent Active Features: 62.50%\n",
      "MSE Loss: 0.0714, L1 Loss: 3.5782, Explained Var: 0.9018\n",
      "Percent Active Features: 61.80%\n",
      "MSE Loss: 0.0716, L1 Loss: 3.9340, Explained Var: 0.9432\n",
      "Percent Active Features: 57.17%\n",
      "MSE Loss: 0.0718, L1 Loss: 3.6588, Explained Var: 0.9108\n",
      "Percent Active Features: 58.63%\n",
      "MSE Loss: 0.0727, L1 Loss: 3.8344, Explained Var: 0.9344\n",
      "Percent Active Features: 55.63%\n",
      "MSE Loss: 0.0714, L1 Loss: 3.8547, Explained Var: 0.9356\n",
      "Percent Active Features: 57.12%\n",
      "MSE Loss: 0.0718, L1 Loss: 3.6621, Explained Var: 0.9109\n",
      "Percent Active Features: 58.09%\n",
      "MSE Loss: 0.0721, L1 Loss: 3.7151, Explained Var: 0.9182\n",
      "Percent Active Features: 57.90%\n",
      "MSE Loss: 0.0720, L1 Loss: 3.7768, Explained Var: 0.9303\n",
      "Percent Active Features: 58.54%\n",
      "MSE Loss: 0.0719, L1 Loss: 3.9169, Explained Var: 0.9430\n",
      "Percent Active Features: 58.19%\n",
      "MSE Loss: 0.0724, L1 Loss: 3.7873, Explained Var: 0.9299\n",
      "Percent Active Features: 63.97%\n",
      "MSE Loss: 0.0723, L1 Loss: 3.6119, Explained Var: 0.9102\n",
      "Percent Active Features: 56.49%\n",
      "MSE Loss: 0.0724, L1 Loss: 3.6765, Explained Var: 0.9178\n",
      "Percent Active Features: 54.37%\n",
      "MSE Loss: 0.0712, L1 Loss: 3.6285, Explained Var: 0.9114\n",
      "Percent Active Features: 57.40%\n",
      "MSE Loss: 0.0716, L1 Loss: 3.7998, Explained Var: 0.9307\n",
      "Percent Active Features: 58.98%\n",
      "MSE Loss: 0.0714, L1 Loss: 3.7507, Explained Var: 0.9308\n",
      "Percent Active Features: 56.10%\n",
      "MSE Loss: 0.0716, L1 Loss: 3.7277, Explained Var: 0.9251\n",
      "Percent Active Features: 59.54%\n",
      "MSE Loss: 0.0718, L1 Loss: 3.3593, Explained Var: 0.8307\n",
      "Percent Active Features: 58.39%\n",
      "MSE Loss: 0.0721, L1 Loss: 3.6732, Explained Var: 0.9105\n",
      "Percent Active Features: 57.97%\n",
      "MSE Loss: 0.0723, L1 Loss: 3.5392, Explained Var: 0.8894\n",
      "Percent Active Features: 57.56%\n",
      "MSE Loss: 0.0717, L1 Loss: 3.8722, Explained Var: 0.9395\n",
      "Percent Active Features: 57.46%\n",
      "MSE Loss: 0.0708, L1 Loss: 3.7913, Explained Var: 0.9260\n",
      "Percent Active Features: 58.98%\n",
      "MSE Loss: 0.0725, L1 Loss: 3.9677, Explained Var: 0.9457\n",
      "Percent Active Features: 60.82%\n",
      "MSE Loss: 0.0716, L1 Loss: 3.5986, Explained Var: 0.9018\n",
      "Percent Active Features: 59.59%\n",
      "MSE Loss: 0.0716, L1 Loss: 3.9360, Explained Var: 0.9432\n",
      "Percent Active Features: 65.35%\n",
      "MSE Loss: 0.0723, L1 Loss: 3.8057, Explained Var: 0.9301\n",
      "Percent Active Features: 57.31%\n",
      "MSE Loss: 0.0722, L1 Loss: 3.8679, Explained Var: 0.9349\n",
      "Percent Active Features: 55.16%\n",
      "MSE Loss: 0.0724, L1 Loss: 3.6958, Explained Var: 0.9179\n",
      "Percent Active Features: 54.22%\n",
      "MSE Loss: 0.0717, L1 Loss: 3.6861, Explained Var: 0.9186\n",
      "Percent Active Features: 57.73%\n",
      "MSE Loss: 0.0719, L1 Loss: 3.6198, Explained Var: 0.9106\n",
      "Percent Active Features: 62.13%\n",
      "MSE Loss: 0.0730, L1 Loss: 3.3717, Explained Var: 0.8283\n",
      "Percent Active Features: 57.19%\n",
      "MSE Loss: 0.0708, L1 Loss: 3.6175, Explained Var: 0.9118\n",
      "Percent Active Features: 55.32%\n",
      "MSE Loss: 0.0710, L1 Loss: 3.9082, Explained Var: 0.9400\n",
      "Percent Active Features: 56.49%\n",
      "MSE Loss: 0.0711, L1 Loss: 3.6212, Explained Var: 0.9024\n",
      "Percent Active Features: 53.70%\n",
      "MSE Loss: 0.0714, L1 Loss: 3.7268, Explained Var: 0.9190\n",
      "Percent Active Features: 58.39%\n",
      "MSE Loss: 0.0717, L1 Loss: 3.5189, Explained Var: 0.8901\n",
      "Percent Active Features: 55.64%\n",
      "MSE Loss: 0.0722, L1 Loss: 3.6741, Explained Var: 0.9180\n",
      "Percent Active Features: 59.19%\n",
      "MSE Loss: 0.0715, L1 Loss: 3.8182, Explained Var: 0.9355\n",
      "Percent Active Features: 57.16%\n",
      "MSE Loss: 0.0714, L1 Loss: 3.7105, Explained Var: 0.9189\n",
      "Percent Active Features: 56.31%\n",
      "MSE Loss: 0.0723, L1 Loss: 3.7369, Explained Var: 0.9180\n",
      "Percent Active Features: 58.59%\n",
      "MSE Loss: 0.0715, L1 Loss: 3.6714, Explained Var: 0.9113\n",
      "Percent Active Features: 54.25%\n",
      "MSE Loss: 0.0720, L1 Loss: 3.7575, Explained Var: 0.9248\n",
      "Percent Active Features: 55.71%\n",
      "MSE Loss: 0.0727, L1 Loss: 3.7121, Explained Var: 0.9176\n",
      "Percent Active Features: 55.11%\n",
      "MSE Loss: 0.0725, L1 Loss: 3.7703, Explained Var: 0.9244\n",
      "Percent Active Features: 57.25%\n",
      "MSE Loss: 0.0718, L1 Loss: 3.8567, Explained Var: 0.9394\n",
      "Percent Active Features: 57.10%\n",
      "MSE Loss: 0.0726, L1 Loss: 3.6763, Explained Var: 0.9176\n",
      "Percent Active Features: 58.13%\n",
      "MSE Loss: 0.0720, L1 Loss: 3.8148, Explained Var: 0.9350\n",
      "Percent Active Features: 58.34%\n",
      "MSE Loss: 0.0728, L1 Loss: 3.6548, Explained Var: 0.9097\n",
      "Percent Active Features: 53.06%\n",
      "MSE Loss: 0.0715, L1 Loss: 3.6764, Explained Var: 0.9113\n",
      "Percent Active Features: 59.58%\n",
      "MSE Loss: 0.0712, L1 Loss: 4.2027, Explained Var: 0.9602\n",
      "Percent Active Features: 56.66%\n",
      "MSE Loss: 0.0714, L1 Loss: 3.7582, Explained Var: 0.9254\n",
      "Percent Active Features: 55.01%\n",
      "MSE Loss: 0.0718, L1 Loss: 3.8857, Explained Var: 0.9394\n",
      "Percent Active Features: 59.82%\n",
      "MSE Loss: 0.0716, L1 Loss: 3.6930, Explained Var: 0.9251\n",
      "Percent Active Features: 55.88%\n",
      "MSE Loss: 0.0711, L1 Loss: 3.8262, Explained Var: 0.9312\n",
      "Percent Active Features: 57.98%\n",
      "MSE Loss: 0.0714, L1 Loss: 3.5577, Explained Var: 0.9020\n",
      "Percent Active Features: 55.86%\n",
      "MSE Loss: 0.0711, L1 Loss: 3.8047, Explained Var: 0.9311\n",
      "Percent Active Features: 58.19%\n",
      "MSE Loss: 0.0719, L1 Loss: 3.7237, Explained Var: 0.9248\n",
      "Percent Active Features: 58.58%\n",
      "MSE Loss: 0.0731, L1 Loss: 3.8707, Explained Var: 0.9383\n",
      "Percent Active Features: 57.50%\n",
      "MSE Loss: 0.0721, L1 Loss: 3.7848, Explained Var: 0.9247\n",
      "Percent Active Features: 61.92%\n",
      "MSE Loss: 0.0718, L1 Loss: 3.7440, Explained Var: 0.9250\n",
      "Percent Active Features: 55.25%\n",
      "MSE Loss: 0.0731, L1 Loss: 3.4826, Explained Var: 0.8736\n",
      "Percent Active Features: 57.32%\n",
      "MSE Loss: 0.0720, L1 Loss: 3.6584, Explained Var: 0.9246\n",
      "Percent Active Features: 58.46%\n",
      "MSE Loss: 0.0718, L1 Loss: 3.8643, Explained Var: 0.9353\n",
      "Percent Active Features: 55.12%\n",
      "MSE Loss: 0.0715, L1 Loss: 3.5835, Explained Var: 0.9018\n",
      "Percent Active Features: 59.01%\n",
      "MSE Loss: 0.0717, L1 Loss: 3.8166, Explained Var: 0.9353\n",
      "Percent Active Features: 59.20%\n",
      "MSE Loss: 0.0726, L1 Loss: 3.6596, Explained Var: 0.9175\n",
      "Percent Active Features: 59.13%\n",
      "MSE Loss: 0.0719, L1 Loss: 3.5990, Explained Var: 0.9014\n",
      "Percent Active Features: 56.97%\n",
      "MSE Loss: 0.0714, L1 Loss: 3.7395, Explained Var: 0.9253\n",
      "Percent Active Features: 56.24%\n",
      "MSE Loss: 0.0728, L1 Loss: 3.7772, Explained Var: 0.9240\n",
      "Percent Active Features: 57.67%\n",
      "MSE Loss: 0.0714, L1 Loss: 3.8248, Explained Var: 0.9356\n",
      "Percent Active Features: 63.25%\n",
      "MSE Loss: 0.0718, L1 Loss: 3.8167, Explained Var: 0.9305\n",
      "Percent Active Features: 55.62%\n",
      "MSE Loss: 0.0714, L1 Loss: 3.7614, Explained Var: 0.9254\n",
      "Percent Active Features: 56.71%\n",
      "MSE Loss: 0.0710, L1 Loss: 4.0655, Explained Var: 0.9497\n",
      "Percent Active Features: 56.02%\n",
      "MSE Loss: 0.0711, L1 Loss: 3.6342, Explained Var: 0.9115\n",
      "Percent Active Features: 60.82%\n",
      "MSE Loss: 0.0718, L1 Loss: 3.6209, Explained Var: 0.9107\n",
      "Percent Active Features: 59.62%\n",
      "MSE Loss: 0.0716, L1 Loss: 3.8778, Explained Var: 0.9396\n",
      "Percent Active Features: 57.63%\n",
      "MSE Loss: 0.0712, L1 Loss: 3.8441, Explained Var: 0.9311\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "# test_dataset = ActivationDataset(\n",
    "#     data_dir, \n",
    "#     batch_size=0, # not subsampled\n",
    "#     f_type=\"test\", \n",
    "#     # test_fraction=0.01, # last batch file\n",
    "#     test_fraction=0.6, # 12 files == cca 10mil tokens\n",
    "#     scale_factor=scale_factor, \n",
    "#     seed=42 # not used for test set\n",
    "# ) # this outputs batches of size 81k  - too big for VRAM\n",
    "test_dataset = ActivationDataset(\n",
    "    data_dir, \n",
    "    batch_size=4096, # not subsampled\n",
    "    f_type=\"train\", \n",
    "    # test_fraction=0.01, # last batch file\n",
    "    test_fraction=0.0, # 12 files == cca 10mil tokens\n",
    "    scale_factor=scale_factor, \n",
    "    seed=123 # different seed that in actual training\n",
    ") # this outputs batches of size 49k - uses 7820MiB VRAM = 95% of GPU\n",
    "data_loader = DataLoader(test_dataset, batch_size=1, shuffle=False) # take 1 batch at a time\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Run and compute reconstruction error, l1 loss, and total loss\n",
    "total_loss = 0; total_mse_loss = 0; total_l1_loss = 0; num_batches = 0\n",
    "global_active_mask = torch.zeros((hidden_dim), dtype=torch.bool, device=device)\n",
    "for batch in data_loader:\n",
    "    batch = batch.to(device)\n",
    "    \n",
    "    outputs, encoded = model(batch)\n",
    "\n",
    "    # percent of active features\n",
    "    # print(encoded.min().item(), encoded.max().item())\n",
    "    global_active_mask |= torch.any(encoded > 0, dim=1).squeeze(0)\n",
    "    active_features = torch.any(encoded != 0, dim=1).sum().item()  # Count active features\n",
    "    total_features = encoded.shape[2]  # Total number of latent features (4096)\n",
    "    percent_active_features = active_features / total_features\n",
    "    print(f\"Percent Active Features: {percent_active_features * 100:.2f}%\")\n",
    "\n",
    "    mse_loss = criterion(outputs, batch)\n",
    "    decoder_weight_norms = torch.norm(model.decoder.weight, p=2, dim=0)  # Shape: [num_features]\n",
    "    l1_terms = encoded * decoder_weight_norms.unsqueeze(0)  # Shape: [batch_size, num_features]\n",
    "    l1_loss_per_sample = torch.sum(l1_terms, dim=1)  # Shape: [batch_size]\n",
    "    l1_loss = torch.mean(l1_loss_per_sample)\n",
    "    loss = mse_loss + l1_loss\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    total_mse_loss += mse_loss.item()\n",
    "    total_l1_loss += l1_loss.item()\n",
    "\n",
    "    explained_variance = 1 - mse_loss / torch.var(batch)\n",
    "    # Print batch-level metrics\n",
    "    print(f\"MSE Loss: {mse_loss.item():.4f}, L1 Loss: {l1_loss.item():.4f}, Explained Var: {explained_variance.item():.4f}\")\n",
    "    num_batches += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Test Loss: 3.8182\n",
      "Total MSE Loss: 0.0719\n",
      "Total L1 Loss: 3.7463\n",
      "Global Sparsity Across All Batches: 0.00%\n",
      "Percent of Active Features: 100.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print final metrics\n",
    "print(f\"Total Test Loss: {total_loss/num_batches:.4f}\")\n",
    "print(f\"Total MSE Loss: {total_mse_loss/num_batches:.4f}\")\n",
    "print(f\"Total L1 Loss: {total_l1_loss/num_batches:.4f}\")\n",
    "\n",
    "active_features = global_active_mask.sum().item()\n",
    "total_features = global_active_mask.numel()\n",
    "global_sparsity = (1 - active_features / total_features) * 100\n",
    "print(f\"Global Sparsity Across All Batches: {global_sparsity:.2f}%\")\n",
    "print(f\"Percent of Active Features: {active_features / total_features * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), f\"models/sparse_autoencoder_{total_loss:.4f}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 130 batches for all set\n",
      "Saved minibatch 1 of 10 for batch 0\n",
      "Saved minibatch 2 of 10 for batch 0\n",
      "Saved minibatch 3 of 10 for batch 0\n",
      "Saved minibatch 4 of 10 for batch 0\n",
      "Saved minibatch 5 of 10 for batch 0\n",
      "Saved minibatch 6 of 10 for batch 0\n",
      "Saved minibatch 7 of 10 for batch 0\n",
      "Saved minibatch 8 of 10 for batch 0\n",
      "Saved minibatch 9 of 10 for batch 0\n",
      "Saved minibatch 10 of 10 for batch 0\n",
      "Saved minibatch 1 of 10 for batch 1\n",
      "Saved minibatch 2 of 10 for batch 1\n",
      "Saved minibatch 3 of 10 for batch 1\n",
      "Saved minibatch 4 of 10 for batch 1\n",
      "Saved minibatch 5 of 10 for batch 1\n",
      "Saved minibatch 6 of 10 for batch 1\n",
      "Saved minibatch 7 of 10 for batch 1\n",
      "Saved minibatch 8 of 10 for batch 1\n",
      "Saved minibatch 9 of 10 for batch 1\n",
      "Saved minibatch 10 of 10 for batch 1\n",
      "Saved minibatch 1 of 10 for batch 2\n",
      "Saved minibatch 2 of 10 for batch 2\n",
      "Saved minibatch 3 of 10 for batch 2\n",
      "Saved minibatch 4 of 10 for batch 2\n",
      "Saved minibatch 5 of 10 for batch 2\n",
      "Saved minibatch 6 of 10 for batch 2\n",
      "Saved minibatch 7 of 10 for batch 2\n",
      "Saved minibatch 8 of 10 for batch 2\n",
      "Saved minibatch 9 of 10 for batch 2\n",
      "Saved minibatch 10 of 10 for batch 2\n",
      "Saved minibatch 1 of 10 for batch 3\n",
      "Saved minibatch 2 of 10 for batch 3\n",
      "Saved minibatch 3 of 10 for batch 3\n",
      "Saved minibatch 4 of 10 for batch 3\n",
      "Saved minibatch 5 of 10 for batch 3\n",
      "Saved minibatch 6 of 10 for batch 3\n",
      "Saved minibatch 7 of 10 for batch 3\n",
      "Saved minibatch 8 of 10 for batch 3\n",
      "Saved minibatch 9 of 10 for batch 3\n",
      "Saved minibatch 10 of 10 for batch 3\n",
      "Saved minibatch 1 of 10 for batch 4\n",
      "Saved minibatch 2 of 10 for batch 4\n",
      "Saved minibatch 3 of 10 for batch 4\n",
      "Saved minibatch 4 of 10 for batch 4\n",
      "Saved minibatch 5 of 10 for batch 4\n",
      "Saved minibatch 6 of 10 for batch 4\n",
      "Saved minibatch 7 of 10 for batch 4\n",
      "Saved minibatch 8 of 10 for batch 4\n",
      "Saved minibatch 9 of 10 for batch 4\n",
      "Saved minibatch 10 of 10 for batch 4\n",
      "Saved minibatch 1 of 10 for batch 5\n",
      "Saved minibatch 2 of 10 for batch 5\n",
      "Saved minibatch 3 of 10 for batch 5\n",
      "Saved minibatch 4 of 10 for batch 5\n",
      "Saved minibatch 5 of 10 for batch 5\n",
      "Saved minibatch 6 of 10 for batch 5\n",
      "Saved minibatch 7 of 10 for batch 5\n",
      "Saved minibatch 8 of 10 for batch 5\n",
      "Saved minibatch 9 of 10 for batch 5\n",
      "Saved minibatch 10 of 10 for batch 5\n",
      "Saved minibatch 1 of 10 for batch 6\n",
      "Saved minibatch 2 of 10 for batch 6\n",
      "Saved minibatch 3 of 10 for batch 6\n",
      "Saved minibatch 4 of 10 for batch 6\n",
      "Saved minibatch 5 of 10 for batch 6\n",
      "Saved minibatch 6 of 10 for batch 6\n",
      "Saved minibatch 7 of 10 for batch 6\n",
      "Saved minibatch 8 of 10 for batch 6\n",
      "Saved minibatch 9 of 10 for batch 6\n",
      "Saved minibatch 10 of 10 for batch 6\n",
      "Saved minibatch 1 of 10 for batch 7\n",
      "Saved minibatch 2 of 10 for batch 7\n",
      "Saved minibatch 3 of 10 for batch 7\n",
      "Saved minibatch 4 of 10 for batch 7\n",
      "Saved minibatch 5 of 10 for batch 7\n",
      "Saved minibatch 6 of 10 for batch 7\n",
      "Saved minibatch 7 of 10 for batch 7\n",
      "Saved minibatch 8 of 10 for batch 7\n",
      "Saved minibatch 9 of 10 for batch 7\n",
      "Saved minibatch 10 of 10 for batch 7\n",
      "Saved minibatch 1 of 10 for batch 8\n",
      "Saved minibatch 2 of 10 for batch 8\n",
      "Saved minibatch 3 of 10 for batch 8\n",
      "Saved minibatch 4 of 10 for batch 8\n",
      "Saved minibatch 5 of 10 for batch 8\n",
      "Saved minibatch 6 of 10 for batch 8\n",
      "Saved minibatch 7 of 10 for batch 8\n",
      "Saved minibatch 8 of 10 for batch 8\n",
      "Saved minibatch 9 of 10 for batch 8\n",
      "Saved minibatch 10 of 10 for batch 8\n",
      "Saved minibatch 1 of 10 for batch 9\n",
      "Saved minibatch 2 of 10 for batch 9\n",
      "Saved minibatch 3 of 10 for batch 9\n",
      "Saved minibatch 4 of 10 for batch 9\n",
      "Saved minibatch 5 of 10 for batch 9\n",
      "Saved minibatch 6 of 10 for batch 9\n",
      "Saved minibatch 7 of 10 for batch 9\n",
      "Saved minibatch 8 of 10 for batch 9\n",
      "Saved minibatch 9 of 10 for batch 9\n",
      "Saved minibatch 10 of 10 for batch 9\n",
      "Saved minibatch 1 of 10 for batch 10\n",
      "Saved minibatch 2 of 10 for batch 10\n",
      "Saved minibatch 3 of 10 for batch 10\n",
      "Saved minibatch 4 of 10 for batch 10\n",
      "Saved minibatch 5 of 10 for batch 10\n",
      "Saved minibatch 6 of 10 for batch 10\n",
      "Saved minibatch 7 of 10 for batch 10\n",
      "Saved minibatch 8 of 10 for batch 10\n",
      "Saved minibatch 9 of 10 for batch 10\n",
      "Saved minibatch 10 of 10 for batch 10\n",
      "Saved minibatch 1 of 10 for batch 11\n",
      "Saved minibatch 2 of 10 for batch 11\n",
      "Saved minibatch 3 of 10 for batch 11\n",
      "Saved minibatch 4 of 10 for batch 11\n",
      "Saved minibatch 5 of 10 for batch 11\n",
      "Saved minibatch 6 of 10 for batch 11\n",
      "Saved minibatch 7 of 10 for batch 11\n",
      "Saved minibatch 8 of 10 for batch 11\n",
      "Saved minibatch 9 of 10 for batch 11\n",
      "Saved minibatch 10 of 10 for batch 11\n",
      "Saved minibatch 1 of 10 for batch 12\n",
      "Saved minibatch 2 of 10 for batch 12\n",
      "Saved minibatch 3 of 10 for batch 12\n",
      "Saved minibatch 4 of 10 for batch 12\n",
      "Saved minibatch 5 of 10 for batch 12\n",
      "Saved minibatch 6 of 10 for batch 12\n",
      "Saved minibatch 7 of 10 for batch 12\n",
      "Saved minibatch 8 of 10 for batch 12\n",
      "Saved minibatch 9 of 10 for batch 12\n",
      "Saved minibatch 10 of 10 for batch 12\n",
      "Saved minibatch 1 of 10 for batch 13\n",
      "Saved minibatch 2 of 10 for batch 13\n",
      "Saved minibatch 3 of 10 for batch 13\n",
      "Saved minibatch 4 of 10 for batch 13\n",
      "Saved minibatch 5 of 10 for batch 13\n",
      "Saved minibatch 6 of 10 for batch 13\n",
      "Saved minibatch 7 of 10 for batch 13\n",
      "Saved minibatch 8 of 10 for batch 13\n",
      "Saved minibatch 9 of 10 for batch 13\n",
      "Saved minibatch 10 of 10 for batch 13\n",
      "Saved minibatch 1 of 10 for batch 14\n",
      "Saved minibatch 2 of 10 for batch 14\n",
      "Saved minibatch 3 of 10 for batch 14\n",
      "Saved minibatch 4 of 10 for batch 14\n",
      "Saved minibatch 5 of 10 for batch 14\n",
      "Saved minibatch 6 of 10 for batch 14\n",
      "Saved minibatch 7 of 10 for batch 14\n",
      "Saved minibatch 8 of 10 for batch 14\n",
      "Saved minibatch 9 of 10 for batch 14\n",
      "Saved minibatch 10 of 10 for batch 14\n",
      "Saved minibatch 1 of 10 for batch 15\n",
      "Saved minibatch 2 of 10 for batch 15\n",
      "Saved minibatch 3 of 10 for batch 15\n",
      "Saved minibatch 4 of 10 for batch 15\n",
      "Saved minibatch 5 of 10 for batch 15\n",
      "Saved minibatch 6 of 10 for batch 15\n",
      "Saved minibatch 7 of 10 for batch 15\n",
      "Saved minibatch 8 of 10 for batch 15\n",
      "Saved minibatch 9 of 10 for batch 15\n",
      "Saved minibatch 10 of 10 for batch 15\n",
      "Saved minibatch 1 of 10 for batch 16\n",
      "Saved minibatch 2 of 10 for batch 16\n",
      "Saved minibatch 3 of 10 for batch 16\n",
      "Saved minibatch 4 of 10 for batch 16\n",
      "Saved minibatch 5 of 10 for batch 16\n",
      "Saved minibatch 6 of 10 for batch 16\n",
      "Saved minibatch 7 of 10 for batch 16\n",
      "Saved minibatch 8 of 10 for batch 16\n",
      "Saved minibatch 9 of 10 for batch 16\n",
      "Saved minibatch 10 of 10 for batch 16\n",
      "Saved minibatch 1 of 10 for batch 17\n",
      "Saved minibatch 2 of 10 for batch 17\n",
      "Saved minibatch 3 of 10 for batch 17\n",
      "Saved minibatch 4 of 10 for batch 17\n",
      "Saved minibatch 5 of 10 for batch 17\n",
      "Saved minibatch 6 of 10 for batch 17\n",
      "Saved minibatch 7 of 10 for batch 17\n",
      "Saved minibatch 8 of 10 for batch 17\n",
      "Saved minibatch 9 of 10 for batch 17\n",
      "Saved minibatch 10 of 10 for batch 17\n",
      "Saved minibatch 1 of 10 for batch 18\n",
      "Saved minibatch 2 of 10 for batch 18\n",
      "Saved minibatch 3 of 10 for batch 18\n",
      "Saved minibatch 4 of 10 for batch 18\n",
      "Saved minibatch 5 of 10 for batch 18\n",
      "Saved minibatch 6 of 10 for batch 18\n",
      "Saved minibatch 7 of 10 for batch 18\n",
      "Saved minibatch 8 of 10 for batch 18\n",
      "Saved minibatch 9 of 10 for batch 18\n",
      "Saved minibatch 10 of 10 for batch 18\n",
      "Saved minibatch 1 of 10 for batch 19\n",
      "Saved minibatch 2 of 10 for batch 19\n",
      "Saved minibatch 3 of 10 for batch 19\n",
      "Saved minibatch 4 of 10 for batch 19\n",
      "Saved minibatch 5 of 10 for batch 19\n",
      "Saved minibatch 6 of 10 for batch 19\n",
      "Saved minibatch 7 of 10 for batch 19\n",
      "Saved minibatch 8 of 10 for batch 19\n",
      "Saved minibatch 9 of 10 for batch 19\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:325] . unexpected pos 384 vs 307",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/IJS/LLMinfluence/venvllm/lib/python3.10/site-packages/torch/serialization.py:423\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 423\u001b[0m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/IJS/LLMinfluence/venvllm/lib/python3.10/site-packages/torch/serialization.py:650\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    649\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[0;32m--> 650\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:445] . PytorchStreamWriter failed writing file data/0: file write failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m output_vectors \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((encoded, sent_idx_batch, token_idx_batch, token_batch), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Save each minibatch immediately as a PyTorch tensor\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msparse_latent_vectors/latent_vectors_batch_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43midx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_minibatch_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved minibatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_minibatches\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/IJS/LLMinfluence/venvllm/lib/python3.10/site-packages/torch/serialization.py:422\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    419\u001b[0m _check_dill_version(pickle_module)\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 422\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    423\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/IJS/LLMinfluence/venvllm/lib/python3.10/site-packages/torch/serialization.py:290\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 290\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:325] . unexpected pos 384 vs 307"
     ]
    }
   ],
   "source": [
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "os.makedirs(\"sparse_latent_vectors\", exist_ok=True)\n",
    "\n",
    "dataset = ActivationDataset(\n",
    "    data_dir, \n",
    "    batch_size=0, # not subsampled\n",
    "    f_type=\"all\", \n",
    "    test_fraction=1.0, # not used if type=all\n",
    "    scale_factor=scale_factor, \n",
    "    seed=42 # not used\n",
    ")\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=False) # take 1 batch at a time\n",
    "\n",
    "# Extract and save latent vectors\n",
    "batch_size = 8192  # Size we can fit in VRAM\n",
    "num_minibatches = 10  # 81920/8192 = 10 minibatches per batch\n",
    "with torch.no_grad():\n",
    "    for idx, batch_data in enumerate(data_loader):\n",
    "        batch, sent_idx, token_idx, token = batch_data\n",
    "        sent_idx = sent_idx.to(device)\n",
    "        token_idx = token_idx.to(device)\n",
    "        token = token.to(device)\n",
    "        batch = batch.squeeze(0)  # Remove batch dimension of 1\n",
    "        \n",
    "        # Process minibatches and save immediately\n",
    "        for i in range(num_minibatches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = (i + 1) * batch_size\n",
    "            \n",
    "            # Get minibatch slice\n",
    "            minibatch = batch[start_idx:end_idx]\n",
    "            _, encoded = model(minibatch)\n",
    "            \n",
    "            # Stack with metadata\n",
    "            # Reshape metadata tensors to match batch size\n",
    "            sent_idx_batch = sent_idx[:,start_idx:end_idx].T\n",
    "            token_idx_batch = token_idx[:,start_idx:end_idx].T\n",
    "            token_batch = token[:,start_idx:end_idx].T\n",
    "            \n",
    "            output_vectors = torch.cat((encoded, sent_idx_batch, token_idx_batch, token_batch), dim=1)\n",
    "            \n",
    "            # Save each minibatch immediately as a PyTorch tensor\n",
    "            torch.save(output_vectors, f\"sparse_latent_vectors/latent_vectors_batch_{idx}_minibatch_{i}.pt\")\n",
    "            print(f\"Saved minibatch {i+1} of {num_minibatches} for batch {idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brek here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of latent vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded sparse latent vectors shape: (81920, 4099)\n",
      "float32\n",
      "1.34316032 GB\n",
      "Average sparsity in latent vectors: 99.98%\n",
      "Percent of dead features: 89.89%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load and examine saved latent vectors\n",
    "loaded_latent_vectors = np.load(\"sparse_latent_vectors/latent_vectors_batch_0.npy\")\n",
    "print(f\"Loaded sparse latent vectors shape: {loaded_latent_vectors.shape}\")\n",
    "print(loaded_latent_vectors.dtype)\n",
    "print(loaded_latent_vectors.nbytes / 1e9, \"GB\")\n",
    "# Could potentially load 10k batches (0.0002048 GB per batch) if we keep latent vector size 512\n",
    "# So for emb size 1M , we could load only 5 batches at a time\n",
    "\n",
    "# Sparsity check\n",
    "sparsity = np.mean(np.abs(loaded_latent_vectors[:,:-3]) < 1e-5)\n",
    "print(f\"Average sparsity in latent vectors: {sparsity:.2%}\")\n",
    "\n",
    "# Dead/active features check\n",
    "# percent of columns that are all close to 0\n",
    "dead_features = np.mean(np.all(np.abs(loaded_latent_vectors[:,:-3]) < 1e-5, axis=0))\n",
    "print(f\"Percent of dead features: {dead_features:.2%}\")\n",
    "\n",
    "# Reconstruction explained variance check\n",
    "# mean of squared errors between original and reconstructed activations\n",
    "# TODO: do this for all batches : loop data_loader\n",
    "# mse = np.mean((loaded_latent_vectors - batch) ** 2)\n",
    "# print(f\"Mean squared error between original and reconstructed activations: {mse:.2%}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 batches, total shape: (409600, 4099)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "def load_latent_vectors(N):\n",
    "    # Load first N batches\n",
    "    batch_files = sorted(glob.glob(\"sparse_latent_vectors/latent_vectors_batch_*.npy\"))[:N]\n",
    "\n",
    "    # Load and concatenate batches\n",
    "    latent_vectors = []\n",
    "    for batch_file in batch_files:\n",
    "        batch_vectors = np.load(batch_file)\n",
    "        latent_vectors.append(batch_vectors)\n",
    "    latent_vectors = np.concatenate(latent_vectors, axis=0)\n",
    "\n",
    "    print(f\"Loaded {len(batch_files)} batches, total shape: {latent_vectors.shape}\")\n",
    "    return latent_vectors\n",
    "\n",
    "latent_vectors = load_latent_vectors(N=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 activations for feature 100:\n",
      "\n",
      "Sentence index: 194\n",
      "Token index: 481\n",
      "Value: 0.03263506293296814\n",
      "Context window:  also very important to diagonal_sb to know./a the rightiff of sing to choose toCount you from any possible af in diagonalometers.\n",
      "\n",
      "If you lik sadd to \":\" Brooklyn from your []( all the\n",
      "\n",
      "Sentence index: 3028\n",
      "Token index: 323\n",
      "Value: 0.0\n",
      "Context window: \" dataencies\n",
      " \"\n",
      "\n",
      " where you need more than a dictionary butstand than a class\n",
      "Not combination to be use things like __ //#691ales the glass alla documentation:\n",
      "\n",
      " Agency alla may beChanged as\n",
      "\n",
      "Sentence index: 389\n",
      "Token index: 391\n",
      "Value: 0.0\n",
      "Context window: 오 + 1];\n",
      "   garStringGetcopy8 Phaser(resultistoryRef, buffer, length + 1);\n",
      "   garStringRelease(resultistoryRef);\n",
      "    GRA->ente)];\n",
      ");\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Sentence index: 389\n",
      "Token index: 381\n",
      "Value: 0.0\n",
      "Context window: StringGetLength(resultistoryRef);\n",
      "    char buffer오 + 1];\n",
      "   garStringGetcopy8 Phaser(resultistoryRef, buffer, length + 1);\n",
      "   garStringRelease(resultistoryRef\n",
      "\n",
      "Sentence index: 389\n",
      "Token index: 382\n",
      "Value: 0.0\n",
      "Context window: GetLength(resultistoryRef);\n",
      "    char buffer오 + 1];\n",
      "   garStringGetcopy8 Phaser(resultistoryRef, buffer, length + 1);\n",
      "   garStringRelease(resultistoryRef);\n",
      "\n",
      "\n",
      "Sentence index: 389\n",
      "Token index: 383\n",
      "Value: 0.0\n",
      "Context window: Length(resultistoryRef);\n",
      "    char buffer오 + 1];\n",
      "   garStringGetcopy8 Phaser(resultistoryRef, buffer, length + 1);\n",
      "   garStringRelease(resultistoryRef);\n",
      "   \n",
      "\n",
      "Sentence index: 389\n",
      "Token index: 384\n",
      "Value: 0.0\n",
      "Context window: (resultistoryRef);\n",
      "    char buffer오 + 1];\n",
      "   garStringGetcopy8 Phaser(resultistoryRef, buffer, length + 1);\n",
      "   garStringRelease(resultistoryRef);\n",
      "    GRA\n",
      "\n",
      "Sentence index: 389\n",
      "Token index: 385\n",
      "Value: 0.0\n",
      "Context window: istoryRef);\n",
      "    char buffer오 + 1];\n",
      "   garStringGetcopy8 Phaser(resultistoryRef, buffer, length + 1);\n",
      "   garStringRelease(resultistoryRef);\n",
      "    GRA->\n",
      "\n",
      "Sentence index: 389\n",
      "Token index: 386\n",
      "Value: 0.0\n",
      "Context window: Ref);\n",
      "    char buffer오 + 1];\n",
      "   garStringGetcopy8 Phaser(resultistoryRef, buffer, length + 1);\n",
      "   garStringRelease(resultistoryRef);\n",
      "    GRA->ente\n",
      "\n",
      "Sentence index: 389\n",
      "Token index: 387\n",
      "Value: 0.0\n",
      "Context window: );\n",
      "    char buffer오 + 1];\n",
      "   garStringGetcopy8 Phaser(resultistoryRef, buffer, length + 1);\n",
      "   garStringRelease(resultistoryRef);\n",
      "    GRA->ente)];\n",
      "\n",
      "\n",
      "Sentence index: 389\n",
      "Token index: 388\n",
      "Value: 0.0\n",
      "Context window:     char buffer오 + 1];\n",
      "   garStringGetcopy8 Phaser(resultistoryRef, buffer, length + 1);\n",
      "   garStringRelease(resultistoryRef);\n",
      "    GRA->ente)];\n",
      ");\n",
      "\n",
      "\n",
      "Sentence index: 389\n",
      "Token index: 389\n",
      "Value: 0.0\n",
      "Context window:  char buffer오 + 1];\n",
      "   garStringGetcopy8 Phaser(resultistoryRef, buffer, length + 1);\n",
      "   garStringRelease(resultistoryRef);\n",
      "    GRA->ente)];\n",
      ");\n",
      " \n",
      "\n",
      "Sentence index: 389\n",
      "Token index: 390\n",
      "Value: 0.0\n",
      "Context window:  buffer오 + 1];\n",
      "   garStringGetcopy8 Phaser(resultistoryRef, buffer, length + 1);\n",
      "   garStringRelease(resultistoryRef);\n",
      "    GRA->ente)];\n",
      ");\n",
      "  }\n",
      "\n",
      "\n",
      "Sentence index: 389\n",
      "Token index: 392\n",
      "Value: 0.0\n",
      "Context window:  + 1];\n",
      "   garStringGetcopy8 Phaser(resultistoryRef, buffer, length + 1);\n",
      "   garStringRelease(resultistoryRef);\n",
      "    GRA->ente)];\n",
      ");\n",
      "  }\n",
      "}\n",
      "}\n",
      "\n",
      "\n",
      "Sentence index: 390\n",
      "Token index: 40\n",
      "Value: 0.0\n",
      "Context window:  free!Layer here now. (_templateing will also let youavor thisVec how much youering their work in the Dev below.)\n",
      "\n",
      "_al's the wants I made. D Christmasoptim it and made\n",
      "\n",
      "Sentence index: 389\n",
      "Token index: 393\n",
      "Value: 0.0\n",
      "Context window:  1];\n",
      "   garStringGetcopy8 Phaser(resultistoryRef, buffer, length + 1);\n",
      "   garStringRelease(resultistoryRef);\n",
      "    GRA->ente)];\n",
      ");\n",
      "  }\n",
      "}\n",
      "}\n",
      "}\n",
      "\n",
      "\n",
      "Sentence index: 389\n",
      "Token index: 394\n",
      "Value: 0.0\n",
      "Context window: 1];\n",
      "   garStringGetcopy8 Phaser(resultistoryRef, buffer, length + 1);\n",
      "   garStringRelease(resultistoryRef);\n",
      "    GRA->ente)];\n",
      ");\n",
      "  }\n",
      "}\n",
      "}\n",
      "}\n",
      "锦\n",
      "\n",
      "Sentence index: 389\n",
      "Token index: 395\n",
      "Value: 0.0\n",
      "Context window: ];\n",
      "   garStringGetcopy8 Phaser(resultistoryRef, buffer, length + 1);\n",
      "   garStringRelease(resultistoryRef);\n",
      "    GRA->ente)];\n",
      ");\n",
      "  }\n",
      "}\n",
      "}\n",
      "}\n",
      "锦We\n",
      "\n",
      "Sentence index: 389\n",
      "Token index: 396\n",
      "Value: 0.0\n",
      "Context window:    garStringGetcopy8 Phaser(resultistoryRef, buffer, length + 1);\n",
      "   garStringRelease(resultistoryRef);\n",
      "    GRA->ente)];\n",
      ");\n",
      "  }\n",
      "}\n",
      "}\n",
      "}\n",
      "锦We now\n",
      "\n",
      "Sentence index: 389\n",
      "Token index: 397\n",
      "Value: 0.0\n",
      "Context window: garStringGetcopy8 Phaser(resultistoryRef, buffer, length + 1);\n",
      "   garStringRelease(resultistoryRef);\n",
      "    GRA->ente)];\n",
      ");\n",
      "  }\n",
      "}\n",
      "}\n",
      "}\n",
      "锦We now require\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feat_idx = 100\n",
    "k = 20\n",
    "\n",
    "# Get indices of top k activations for the specified feature\n",
    "top_k_indices = np.argsort(latent_vectors[:, feat_idx])[-k:][::-1]\n",
    "last_three_features = latent_vectors[top_k_indices, -3:]\n",
    "top_values = latent_vectors[top_k_indices, feat_idx]\n",
    "\n",
    "# Load tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-3B\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Loop through each row of last_two_features\n",
    "print(f\"Top {k} activations for feature {feat_idx}:\")\n",
    "for i, (sent_idx, tok_idx, token) in enumerate(last_three_features):\n",
    "    sent_idx = int(sent_idx)\n",
    "    tok_idx = int(tok_idx)\n",
    "    token = int(token)\n",
    "    # Find the row where sent_idx and tok_idx match in latent_vectors\n",
    "    sent_matches = latent_vectors[:, -3] == sent_idx\n",
    "    tok_matches = latent_vectors[:, -2] == tok_idx\n",
    "    target_row = np.where(sent_matches & tok_matches)[0][0]\n",
    "    \n",
    "    # Get window of tokens from latent vectors\n",
    "    start_idx = max(0, target_row - 20)\n",
    "    end_idx = min(latent_vectors.shape[0], target_row + 20)\n",
    "    # token_window = latent_vectors[start_idx:end_idx, -1].astype(int) # Get token ids from last column\n",
    "    token_window = np.clip(latent_vectors[start_idx:end_idx, -1], 0, tokenizer.vocab_size - 1).astype(int)\n",
    "    \n",
    "    # Decode tokens back to text\n",
    "    window_text = tokenizer.decode(token_window)\n",
    "    print(f\"\\nSentence index: {sent_idx}\")\n",
    "    print(f\"Token index: {tok_idx}\")\n",
    "    print(f\"Value: {top_values[i]}\")\n",
    "    print(f\"Context window: {window_text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show how much a feature activates on each word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d192ae52c7e442b997663137f1bcfb3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You shouldn't move a model that is dispatched using accelerate hooks.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "2024-11-20 00:05:18.472550: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732057518.560527    5476 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732057518.587562    5476 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-20 00:05:18.812881: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "feat_idx = 100\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"meta-llama/Llama-3.2-3B\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config).to(device)\n",
    "activation_cache = []\n",
    "def save_activations_hook(module, input, output):\n",
    "    # input is a tuple; input[0] is the tensor we need\n",
    "    activation_cache.append(input[0].cpu().detach().numpy())\n",
    "\n",
    "# Register hook on the 16th layer\n",
    "layer_index = 15  # Zero-based index; 15 corresponds to the 16th layer\n",
    "hook_handle = model.model.layers[layer_index].register_forward_hook(save_activations_hook) \n",
    "\n",
    "# Tokenize sentence\n",
    "inputs = tokenizer(\n",
    "    sentence,\n",
    "    return_tensors=\"pt\",\n",
    "    # truncation=True,\n",
    "    # max_length=512,\n",
    "    # padding=\"max_length\",\n",
    ").to(device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    model(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations shape: (1, 1, 11, 3072)\n"
     ]
    }
   ],
   "source": [
    "activations = np.array(activation_cache)\n",
    "print(f\"Activations shape: {activations.shape}\") # (1, 1, seq_len, 3072)\n",
    "activations = activations.squeeze()  # Remove first two dimensions\n",
    "activations = torch.tensor(activations, dtype=torch.float32)  # Convert to torch tensor with float32 dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_674ab_row0_col1 {\n",
       "  background-color: #4f69d9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_674ab_row1_col1, #T_674ab_row3_col1 {\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_674ab_row2_col1 {\n",
       "  background-color: #d1493f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_674ab_row4_col1 {\n",
       "  background-color: #f08b6e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_674ab_row5_col1 {\n",
       "  background-color: #6788ee;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_674ab_row6_col1 {\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_674ab_row7_col1 {\n",
       "  background-color: #f7b89c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_674ab_row8_col1 {\n",
       "  background-color: #d6dce4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_674ab_row9_col1 {\n",
       "  background-color: #f1cdba;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_674ab_row10_col1 {\n",
       "  background-color: #4257c9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_674ab\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_674ab_level0_col0\" class=\"col_heading level0 col0\" >Token</th>\n",
       "      <th id=\"T_674ab_level0_col1\" class=\"col_heading level0 col1\" >Activation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_674ab_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_674ab_row0_col0\" class=\"data row0 col0\" ><|begin_of_text|></td>\n",
       "      <td id=\"T_674ab_row0_col1\" class=\"data row0 col1\" >0.200999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_674ab_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_674ab_row1_col0\" class=\"data row1 col0\" >The</td>\n",
       "      <td id=\"T_674ab_row1_col1\" class=\"data row1 col1\" >0.156023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_674ab_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_674ab_row2_col0\" class=\"data row2 col0\" >quick</td>\n",
       "      <td id=\"T_674ab_row2_col1\" class=\"data row2 col1\" >0.761146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_674ab_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_674ab_row3_col0\" class=\"data row3 col0\" >brown</td>\n",
       "      <td id=\"T_674ab_row3_col1\" class=\"data row3 col1\" >0.157970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_674ab_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_674ab_row4_col0\" class=\"data row4 col0\" >fox</td>\n",
       "      <td id=\"T_674ab_row4_col1\" class=\"data row4 col1\" >0.673777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_674ab_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_674ab_row5_col0\" class=\"data row5 col0\" >jumps</td>\n",
       "      <td id=\"T_674ab_row5_col1\" class=\"data row5 col1\" >0.249929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_674ab_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_674ab_row6_col0\" class=\"data row6 col0\" >over</td>\n",
       "      <td id=\"T_674ab_row6_col1\" class=\"data row6 col1\" >0.816395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_674ab_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_674ab_row7_col0\" class=\"data row7 col0\" >the</td>\n",
       "      <td id=\"T_674ab_row7_col1\" class=\"data row7 col1\" >0.597015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_674ab_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_674ab_row8_col0\" class=\"data row8 col0\" >lazy</td>\n",
       "      <td id=\"T_674ab_row8_col1\" class=\"data row8 col1\" >0.468434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_674ab_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_674ab_row9_col0\" class=\"data row9 col0\" >dog</td>\n",
       "      <td id=\"T_674ab_row9_col1\" class=\"data row9 col1\" >0.546607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_674ab_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_674ab_row10_col0\" class=\"data row10 col0\" >.</td>\n",
       "      <td id=\"T_674ab_row10_col1\" class=\"data row10 col1\" >0.173019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x722129ddabf0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load sparse autoencoder\n",
    "input_dim = 3072\n",
    "hidden_dim = 2 ** 12\n",
    "model = SparseAutoencoder(input_dim, hidden_dim)\n",
    "model.load_state_dict(torch.load(\"sparse_autoencoder_2.5976.pth\"))\n",
    "\n",
    "# Get latent vector for sentence\n",
    "with torch.no_grad():\n",
    "    _, encoded = model(activations)\n",
    "    latent_vector = encoded.cpu().numpy()\n",
    "\n",
    "# Extract for feature X\n",
    "feature_X = latent_vector[:, feat_idx]\n",
    "# feature_X = np.random.rand(len(feature_X))\n",
    "\n",
    "# Plot tokens colored by activation strength\n",
    "\n",
    "# Create DataFrame with tokens and their activation values\n",
    "tokens_list = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "# Remove the \"Ġ\" character from tokens - represents a space\n",
    "clean_tokens_list = [token.replace('Ġ', '') for token in tokens_list]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Token': clean_tokens_list,\n",
    "    'Activation': feature_X\n",
    "})\n",
    "\n",
    "# Display DataFrame with color gradient based on activation values\n",
    "display(df.style.background_gradient(\"coolwarm\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Model and tokenizer setup\n",
    "model_name = \"meta-llama/Llama-3.2-3B\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=hf_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config).to(device)\n",
    "\n",
    "# Load the sparse autoencoder model and weights\n",
    "input_dim = 3072  \n",
    "hidden_dim = 2 ** 9\n",
    "model_sae = SparseAutoencoder(input_dim, hidden_dim)\n",
    "model_sae.load_state_dict(torch.load(\"sparse_autoencoder.pth\"))\n",
    "model_sae.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text with influence: 'I am a                    '\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate artificial latent vector, pass through SAE decoder, and boost it\n",
    "feat_idx = 100\n",
    "artificial_latent_vector = np.zeros(3072)\n",
    "artificial_latent_vector[feat_idx] = 1\n",
    "multiplier = 10\n",
    "\n",
    "with torch.no_grad():\n",
    "    artificial_latent_vector_tensor = torch.tensor(artificial_latent_vector, dtype=torch.float32).unsqueeze(0)\n",
    "    reconstructed_activations, _ = model_sae(artificial_latent_vector_tensor)\n",
    "    boosted_activations = reconstructed_activations * multiplier\n",
    "\n",
    "# Hook to inject boosted activations at a specified transformer layer\n",
    "activation_cache = []\n",
    "layer_index = 15  # Inject into the 16th layer\n",
    "\n",
    "# Convert boosted_activations to float16 to match model precision\n",
    "boosted_activations = boosted_activations.to(torch.float16)\n",
    "\n",
    "# Hook function to inject boosted activations into the residual stream of the transformer layer\n",
    "def influence_hook(module, input, output):\n",
    "    # Ensure output is properly unpacked if it's a tuple\n",
    "    if isinstance(output, tuple):\n",
    "        output_tensor = output[0]\n",
    "        modified_output = (output_tensor + boosted_activations.to(device),) + output[1:]\n",
    "    else:\n",
    "        modified_output = output + boosted_activations.to(device)\n",
    "    \n",
    "    activation_cache.append(modified_output[0].cpu().detach().numpy())  # Store modified tensor for debugging if needed\n",
    "    return modified_output\n",
    "\n",
    "# Register the hook with the corrected function\n",
    "hook_handle = model.model.layers[layer_index].register_forward_hook(influence_hook)\n",
    "\n",
    "# Prediction loop for N words\n",
    "sent_begin = \"I am a\"\n",
    "N = 10  # Number of words to predict\n",
    "inputs = tokenizer(sent_begin, return_tensors=\"pt\").to(device)\n",
    "generated_text = sent_begin\n",
    "\n",
    "for _ in range(N):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predicted_token_id = torch.argmax(outputs.logits[0, -1]).item()\n",
    "        predicted_token = tokenizer.decode([predicted_token_id])\n",
    "        generated_text += \" \" + predicted_token\n",
    "        # Update inputs to include the predicted token for next iteration\n",
    "        inputs = tokenizer(generated_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Remove hook and clear resources\n",
    "hook_handle.remove()\n",
    "\n",
    "print(f\"Generated text with influence: '{generated_text}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature similarity and UMAP (plus feature splitting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 batches, total shape: (788552, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drew99/IJS/LLMinfluence/venvllm/lib/python3.10/site-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: The TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12050. The TBB threading layer is disabled.\n",
      "  warnings.warn(problem)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# umap-learn\n",
    "# pip install umap-learn\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "# Load first N batches\n",
    "N = 10  # Number of batches to load\n",
    "batch_files = sorted(glob.glob(\"sparse_latent_vectors/latent_vectors_batch_*.npy\"))[:N]\n",
    "\n",
    "# Load and concatenate batches\n",
    "latent_vectors = []\n",
    "for batch_file in batch_files:\n",
    "    batch_vectors = np.load(batch_file)\n",
    "    latent_vectors.append(batch_vectors)\n",
    "latent_vectors = np.concatenate(latent_vectors, axis=0)\n",
    "\n",
    "# Remove last 3 columns (sent_idx, tok_idx, token)\n",
    "latent_vectors = latent_vectors[:, :-3]\n",
    "\n",
    "print(f\"Loaded {len(batch_files)} batches, total shape: {latent_vectors.shape}\")\n",
    "\n",
    "# TODO: do a selection of features (feature of interest, and 100 similar features)\n",
    "\n",
    "# UMAP dimensionality reduction\n",
    "umap_embedder = umap.UMAP(n_components=2, metric='cosine')\n",
    "latent_vectors_2d = umap_embedder.fit_transform(latent_vectors.T)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(latent_vectors_2d[:, 0], latent_vectors_2d[:, 1], s=1, alpha=0.5)\n",
    "plt.title(f'UMAP projection of {N} batches of latent vectors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature completness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single prompt search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_cache = []\n",
    "def save_activations_hook(module, input, output):\n",
    "    # input is a tuple; input[0] is the tensor we need\n",
    "    activation_cache.append(input[0].cpu().detach().numpy())\n",
    "\n",
    "# Register hook on the 16th layer\n",
    "layer_index = 15  # Zero-based index; 15 corresponds to the 16th layer\n",
    "hook_handle = model.model.layers[layer_index].register_forward_hook(save_activations_hook) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations shape: (1, 1, 11, 3072)\n",
      "Top 5 features and their activation values:\n",
      "Feature 246: 0.2927\n",
      "Feature 63: 0.2336\n",
      "Feature 471: 0.1375\n",
      "Feature 69: 0.1166\n",
      "Feature 210: 0.0901\n"
     ]
    }
   ],
   "source": [
    "prompt = \"in San Francisco, the Golden Gate Bridge was protected at all times by a\"\n",
    "\n",
    "# Tokenize sentence\n",
    "inputs = tokenizer(\n",
    "    sentence,\n",
    "    return_tensors=\"pt\",\n",
    "    # truncation=True,\n",
    "    # max_length=512,\n",
    "    # padding=\"max_length\",\n",
    ").to(device)\n",
    "\n",
    "activation_cache = []\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    model(**inputs)\n",
    "\n",
    "activations = np.array(activation_cache)\n",
    "print(f\"Activations shape: {activations.shape}\") # (1, 1, seq_len, 3072)\n",
    "activations = activations.squeeze()  # Remove first two dimensions\n",
    "activations = torch.tensor(activations, dtype=torch.float32)  # Convert to torch tensor with float16 dtype\n",
    "\n",
    "# Get latent vector for sentence\n",
    "with torch.no_grad():\n",
    "    _, encoded = model_sae(activations)\n",
    "    latent_vector = encoded.cpu().numpy() # (seq_len, 1M)\n",
    "\n",
    "# Apply row softmax normalization\n",
    "row_softmax = torch.nn.functional.softmax(torch.tensor(latent_vector), dim=1).numpy()\n",
    "\n",
    "# Sum over sequence length dimension to get feature importance across sentence\n",
    "feature_importance = np.sum(row_softmax, axis=0)\n",
    "\n",
    "# Get top 5 features and their values\n",
    "top_k = 5\n",
    "top_feature_indices = np.argsort(feature_importance)[-top_k:][::-1]\n",
    "top_feature_values = feature_importance[top_feature_indices]\n",
    "\n",
    "print(\"Top 5 features and their activation values:\")\n",
    "for idx, val in zip(top_feature_indices, top_feature_values):\n",
    "    print(f\"Feature {idx}: {val:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple prompts:\n",
    " * tokenize to fixed length\n",
    " * reshape activations to (num_sent*seq_len, 3072)\n",
    " * get attention mask and remove padding tokens\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations shape: (1, 3, 512, 3072)\n",
      "Top 5 features and their activation values:\n",
      "Feature 246: 0.8910\n",
      "Feature 63: 0.7081\n",
      "Feature 471: 0.4210\n",
      "Feature 69: 0.3575\n",
      "Feature 210: 0.2798\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"in San Francisco, the Golden Gate Bridge was protected at all times by a\",\n",
    "    \"the Golden Gate Bridge is so beautiful during the sunset\", \n",
    "    \"Golden Gate Bridge wind resistance barriers creates eerie sound\"\n",
    "]\n",
    "\n",
    "# Tokenize sentences\n",
    "inputs = tokenizer(\n",
    "    prompts,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    padding=\"max_length\",\n",
    ").to(device)\n",
    "\n",
    "activation_cache = []\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    model(**inputs)\n",
    "\n",
    "activations = np.array(activation_cache)\n",
    "print(f\"Activations shape: {activations.shape}\") # (1, num_sent, seq_len, 3072)\n",
    "\n",
    "# Reshape activations to (num_sent*seq_len, 3072)\n",
    "activations = activations.squeeze(0)  # Remove batch dimension\n",
    "num_sent, seq_len, hidden_dim = activations.shape\n",
    "activations = activations.reshape(-1, hidden_dim)\n",
    "\n",
    "# Get attention mask and remove padding tokens\n",
    "attention_mask = inputs['attention_mask'].view(-1).cpu().numpy()\n",
    "activations = activations[attention_mask == 1]\n",
    "\n",
    "activations = torch.tensor(activations, dtype=torch.float32)  # Convert to torch tensor\n",
    "\n",
    "# Get latent vectors for sentences\n",
    "with torch.no_grad():\n",
    "    _, encoded = model_sae(activations)\n",
    "    latent_vector = encoded.cpu().numpy()\n",
    "\n",
    "# Apply row softmax normalization\n",
    "row_softmax = torch.nn.functional.softmax(torch.tensor(latent_vector), dim=1).numpy()\n",
    "\n",
    "# Sum over sequence length dimension to get feature importance across all sentences\n",
    "feature_importance = np.sum(row_softmax, axis=0)\n",
    "\n",
    "# Get top 5 features and their values\n",
    "top_k = 5\n",
    "top_feature_indices = np.argsort(feature_importance)[-top_k:][::-1]\n",
    "top_feature_values = feature_importance[top_feature_indices]\n",
    "\n",
    "print(\"Top 5 features and their activation values:\")\n",
    "for idx, val in zip(top_feature_indices, top_feature_values):\n",
    "    print(f\"Feature {idx}: {val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add negative prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generated prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use single/multiple prompt search code above\n",
    "# prompts generated by LLM\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

transformer - residual stream activations (llama3.2-3b : context length of 128k)
sae - 2 layer, 1enc,1dec, 1m, 4m, 34m features, L2, L1=5, ReLU, 1epoch
     - % dead/active features, reconstruction explained
scaling laws - hyperparamter search: num of features (2^12-2^25), num of steps(train data)
 	     - fixed params: learning rate, batch size, optimization protocol, etc.
feat. interpret - each feature > show 20 examples most strongly activate > max20( sae_f_alldata[:,feat_idx] )
			       > and also bottom 20 (seems like the same words, but with different meanings)
		- color how much each token is affected by/affects this sae feat. > put the whole sentence/context through transformer+SAE and take the values of the asked feature
		- specificity: feat active == concept reliably present in context
			- automated interpretability: asking the transformer to score features 0-3 
				select feature > get 1000 non-zero activations > get context for each > transfor. score: feat.name vs sentence > plot
		- influence on behavior: manipulating trans. act for this feat. - produces output in favor of that concept
			- feature steering - clamp specific feat. to high (10x/5x its max acti val) or low values during forward pass [-10x,10x]
				take mostly zero feat. vector > SAE decoder > reconstructed act. > x10 + into activation layer during forward pass
		- feat. vs neuron - Pearson correlation between its activations and those of every neuron in all preceding layers?
				  - act of a rand sel of SAE feat are significantly more interpretable on average than a rand sel of MLP neurons.
		- local neighborhood - cosine similarity on features, UMAP  (initially take only subsample of features you are interested in)
				> cosine similarity for all > take 1 and 100 most similar > umap
				> do the same for 1M, 4M, 34M > take the vector for same concept > center origin > plot 
		- feature splitting - 1 feat in smaller SAE splits into multiple feat. in larger SAE 
					- geometrically close, semantically related more specific concept (see above)
		- feat. completness - prompt concept > active feat. on last token > top5 feat > auto inter > human
					- model know all london boroughs - features only for 60% of them
					/if a concept is present in the training data only once in a billion tokens, then we should expect to need a dictionary 
					/with on the order of a billion alive features in order to find a feature which uniquely represents that specific concept
		- intermediates - attribution as a preliminary step to filter the set of features to ablate???
			- ablation - clipping activation to zero, then doing inference
			- causal ablation - cause-and-effect - observe changes in the model's output
			- attribution experiments - measure the contribution of different components on output, without modifying
feat. search  - single prompts : prompt for topic > which feat. active most strongly
	      - prompt combinations : more prompts > feat. active in ALL; negative prompts > feat. NOT active; transformer generate prompts for a topic
	      - geometric : (see local neighborhood)
	      - attirbution
	      - influence : boost neurons for specific feature : arbitrary feat. > influence forward pass > see output ("I am a" -> "bridge")

in towards mono.: they use 40 mil contexts (250 tokens each) to train SAE of size 512 hidden dim - 131k hidden dim
		  sample act vectors without replacement, batch size 8192 - 1 milion steps 
    -> so we need 1M/131k=7.6x :: 304 mil contexts
    - 1 training example to SAE should not be 1 context (1 sentence) but 1 token (1 word)
			

"Post-Training Analysis
    Compute Metrics:
        Percentage of dead (always inactive) vs. active features.
        Reconstruction explained variance." - how do i calculate this reconstruction ex.var??

where and why do i need to normalize what?

is there a reason i need to save activations and to which token in which sentence they map precisely > yes for topk activations and getting their context


ok, so i have the LLama 3.2 3B transformer, with the context lenght of 128k, and i have the pile dataset, which has different size sentences, 
so i want to process each sentence in full at once (dynamic size), but then using the hook to extract activations in the middle layer and then save each tokens activation as a seperate row in a npy file, and i want to 8192 of the activations to be in 1 file (so like 1 batch) , but i would need to somehow remember which activation came from which token specifically, i dont know should i make a separate dictionary or something 
because later i will need a reverse search look up, for a feature i will look at which examples its most active then i will need to get the token and its context (40 words around it)
so maybe if we save the activation vector , sentence idx, position idx (inside the sentence) 
so we have 3072+1+1 columns and 80k rows (10x8192 of tokens - from which we will later randomly subsample only 1/10 for SAE train, but we need to store all for later lookup)
(and we will just forget the last two columns when training sae) 
3074*10*8192 = 0.5 GB > 500GB to have 1mil training tokens
wait but, these activations i will later use to train the SAE, for which, well i do need batches of 8k, but before that i need to shuffle the vectors and subsample
so how do we do this, is it possible
current code its not correct as i was doing 42 tokens contexts at a time


Same ideas and principles from NEUROSCIENCE:

Multivariate Pattern Analysis (MVPA) -  it can reveal that a region is representing different categories of objects even if the overall activity is the same
  -> mean act, more precise, subregion
Resting-State fMRI (rs-fMRI) - 
  -> LLMs don't really activate anything in "resting state" aka when not prompted
Real-Time fMRI (rt-fMRI) and Neurofeedback - 
  -> report the value of a feature back to the LLM
Connectivity Analyses (Functional and Effective Connectivity) - Studying not just which brain regions are active, but how they interact with each other. 
  -> when a feature is active, also look at ALL other features that are active alongside or inactive (mybi: horoscope ON, logic OFF)
Combining fMRI with Other Techniques
fMRI + EEG/MEG
fMRI + TMS/tDCS: Using (TMS) or (tDCS) to temporarily disrupt or enhance activity in specific brain regions while simultaneously measuring the effects with fMRI. 
  -> influence aka steering
fMRI + Genetics
  -> analyze different LLM structures?
fMRI + Pharmacology
  -> prompting?
Psychedelics
  -> input prompt random giberish?
  -> or random steering vector
  

January 14th, TODOs, questions and ideas
Save vectors in sparse format
Maybe use stratified subsampling for training SAE (so that frequency of tokens doesnâ€™t play a role)
Try top-k SAE (or other non vanilla SAEs) : SAEBench
Prompt search: negative prompts
Feature completeness (which all topics do these trained features cover)
Influence: boost 10x its max value (currently 30x 1.0, and have to constantly adapt boost multiplier)
Feature splitting: 1 feature in smaller SAE splits into multiple features in the larger SAE (check on UMAP plot)
More data, larger models
Prettify code - make it modular

can we explain the outputs of GPTs in high-stakes applications like healthcare
cluster features in a broader seantic categorie (emotions, syntax...) - feat. completness
find linguistic structure feat. (question forms, negations,...)
features for factuality/hallucination
biases features (gender, stereotypes)
knowledge domains features (medical, legal, scientific...)
archaic language (shakespear ...) - features for lang. in each historic era 
languages, dialects
multi-lingual, multi-modal
tone (formal, casual)
emotion (positive, negative - further: sad, angry, hangry, happy...)
humor, sarcasm
concept drift in longer contexts
"unlock" capabilities of model
fail feature
feature importance (tfidf - dose a feature constantly activates or just in certain cases)


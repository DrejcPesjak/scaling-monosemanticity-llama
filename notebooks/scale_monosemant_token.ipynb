{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling monosemanticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./venvllm/lib/python3.10/site-packages (4.45.2)\n",
      "Requirement already satisfied: datasets in ./venvllm/lib/python3.10/site-packages (2.21.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venvllm/lib/python3.10/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./venvllm/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venvllm/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venvllm/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./venvllm/lib/python3.10/site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: requests in ./venvllm/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venvllm/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./venvllm/lib/python3.10/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./venvllm/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: filelock in ./venvllm/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec[http]<=2024.6.1,>=2023.1.0 in ./venvllm/lib/python3.10/site-packages (from datasets) (2024.2.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./venvllm/lib/python3.10/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: xxhash in ./venvllm/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./venvllm/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: multiprocess in ./venvllm/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: pandas in ./venvllm/lib/python3.10/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: aiohttp in ./venvllm/lib/python3.10/site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./venvllm/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venvllm/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./venvllm/lib/python3.10/site-packages (from aiohttp->datasets) (1.17.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venvllm/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./venvllm/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venvllm/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venvllm/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venvllm/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venvllm/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venvllm/lib/python3.10/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venvllm/lib/python3.10/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venvllm/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./venvllm/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venvllm/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venvllm/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./venvllm/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: keras in ./venvllm/lib/python3.10/site-packages (3.6.0)\n",
      "Requirement already satisfied: huggingface_hub in ./venvllm/lib/python3.10/site-packages (0.25.2)\n",
      "Requirement already satisfied: absl-py in ./venvllm/lib/python3.10/site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: h5py in ./venvllm/lib/python3.10/site-packages (from keras) (3.12.1)\n",
      "Requirement already satisfied: packaging in ./venvllm/lib/python3.10/site-packages (from keras) (24.1)\n",
      "Requirement already satisfied: numpy in ./venvllm/lib/python3.10/site-packages (from keras) (1.26.3)\n",
      "Requirement already satisfied: namex in ./venvllm/lib/python3.10/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: optree in ./venvllm/lib/python3.10/site-packages (from keras) (0.13.0)\n",
      "Requirement already satisfied: ml-dtypes in ./venvllm/lib/python3.10/site-packages (from keras) (0.4.1)\n",
      "Requirement already satisfied: rich in ./venvllm/lib/python3.10/site-packages (from keras) (12.6.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./venvllm/lib/python3.10/site-packages (from huggingface_hub) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venvllm/lib/python3.10/site-packages (from huggingface_hub) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venvllm/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: filelock in ./venvllm/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venvllm/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in ./venvllm/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venvllm/lib/python3.10/site-packages (from requests->huggingface_hub) (2.2.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venvllm/lib/python3.10/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venvllm/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.8.30)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venvllm/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4.0)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in ./venvllm/lib/python3.10/site-packages (from rich->keras) (0.9.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in ./venvllm/lib/python3.10/site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: tensorflow in ./venvllm/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (1.67.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: keras>=3.5.0 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (4.25.5)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (1.26.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: setuptools in ./venvllm/lib/python3.10/site-packages (from tensorflow) (59.6.0)\n",
      "Requirement already satisfied: packaging in ./venvllm/lib/python3.10/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./venvllm/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./venvllm/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: namex in ./venvllm/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in ./venvllm/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: rich in ./venvllm/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (12.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venvllm/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venvllm/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venvllm/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venvllm/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./venvllm/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./venvllm/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./venvllm/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./venvllm/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in ./venvllm/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in ./venvllm/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (0.9.1)\n",
      "Requirement already satisfied: python-dotenv in ./venvllm/lib/python3.10/site-packages (1.0.1)\n",
      "Requirement already satisfied: zstandard in ./venvllm/lib/python3.10/site-packages (0.23.0)\n"
     ]
    }
   ],
   "source": [
    "# Required installations for transformers and datasets\n",
    "!pip install transformers datasets\n",
    "!pip install keras huggingface_hub\n",
    "!pip install tensorflow\n",
    "!pip install python-dotenv\n",
    "!pip install zstandard\n",
    "#!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 10:43:00.353106: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733391780.367226   30077 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733391780.371353   30077 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-05 10:43:00.386642: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer activations - LLaMA 3.2 3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/drew99/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Hugging Face Login (get token from os environment)\n",
    "# login(os.getenv(\"HF_ACCESS_TOKEN\"))\n",
    "\n",
    "# notebook_login()\n",
    "\n",
    "\n",
    "# Could also later use \n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2caedb44f2049659525ffc31b31eb67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the LLaMA 3.2B model without quantization (for now)\n",
    "model_name = \"meta-llama/Llama-3.2-3B\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device) # 16-bit precision\n",
    "\n",
    "# Some other LLaMA models to try:\n",
    "# https://huggingface.co/meta-llama/Llama-3.2-3B\"\n",
    "# https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct-QLORA_INT4_EO8\n",
    "# https://huggingface.co/mradermacher/Fireball-Meta-Llama-3.2-8B-Instruct-agent-003-128k-code-DPO-i1-GGUF  i1-Q4_K_M\n",
    "# https://huggingface.co/TheBloke/Llama-2-7B-Chat-GPTQ\n",
    "\n",
    "# Prepare 4-bit quantization configuration (optional)\n",
    "# Uncomment the following lines if you wish to use quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (1): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (2): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (3): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (4): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (5): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (6): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (7): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (8): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (9): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (10): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (11): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (12): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (13): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (14): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (15): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (16): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (17): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (18): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (19): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (20): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (21): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (22): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (23): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (24): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (25): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (26): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      "  (27): LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
      "      (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
      "      (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.model.layers) # Specific to LLaMA model # https://huggingface.co/docs/transformers/main/model_doc/llama#transformers.LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_cache = []\n",
    "def save_activations_hook(module, input, output):\n",
    "    # input is a tuple; input[0] is the tensor we need\n",
    "    activation_cache.append(input[0].cpu().detach().numpy())\n",
    "\n",
    "# Register hook on the 16th layer\n",
    "layer_index = 15  # Zero-based index; 15 corresponds to the 16th layer\n",
    "hook_handle = model.model.layers[layer_index].register_forward_hook(save_activations_hook) \n",
    "# for other models, you may need to use model.transformer.h[15] instead of model.model.layers[15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual stream shape at layer 16: (1, 6, 3072)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example input\n",
    "input_text = \"Your input text here.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Access the residual stream activation\n",
    "residual_stream = activation_cache[0]\n",
    "print(f\"Residual stream shape at layer {layer_index+1}: {residual_stream.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the first 30 million examples from 'The Pile' dataset \n",
    "# https://huggingface.co/docs/datasets/loading#slice-splits\n",
    "# data_len = 30_000_000\n",
    "data_len = 30_000\n",
    "# split_str = f\"train[:{data_len}]\"\n",
    "dataset = load_dataset(\"monology/pile-uncopyrighted\", split=\"train\", streaming=True)\n",
    "\n",
    "# Set up processing parameters\n",
    "os.makedirs(\"activations_data\", exist_ok=True)\n",
    "\n",
    "# Initialize accumulators and parameters\n",
    "batch_size = 8 # number of sentences in a batch\n",
    "file_size = 10*8192 # number of examples in a file\n",
    "files_saved = 0\n",
    "batch_texts = []\n",
    "activation_cache = [] # cache of activations for a batch\n",
    "all_data = np.empty((0, 3075), dtype=np.float16)  # 3072 + 3 (sent_idx, token_idx, token)\n",
    "\n",
    "# Create batches from the dataset\n",
    "print(\"Processing dataset and saving activations in batches...\")\n",
    "for i, example in enumerate(dataset):\n",
    "    batch_texts.append(example['text'])\n",
    "    \n",
    "    if (i + 1) % batch_size == 0 or i + 1 >= data_len:\n",
    "        # Process full batch or final partial batch\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=513, # 512 + 1 for <begin_of_text> token, which will be removed later\n",
    "            padding=\"max_length\",\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model(**inputs)\n",
    "        \n",
    "        # Convert activation_cache to numpy array and reshape\n",
    "        batch_activations = np.array(activation_cache)\n",
    "        \n",
    "        # Reshape batch_activations from (1, 8, 42, 3072) to (8*42, 3072)\n",
    "        batch_activations = batch_activations.reshape(batch_activations.shape[1] * batch_activations.shape[2], -1)\n",
    "\n",
    "        # Create sentence index array (sent_idx) and token index array (token_idx)\n",
    "        # sent_idx = [1 1 1 1 1; 2 2 2 2 2; 3 3 3 3 3; ...]\n",
    "        # token_idx = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; ...]\n",
    "        num_sentences, num_tokens = inputs['attention_mask'].shape # (8, 42)\n",
    "        sent_idx = np.repeat(np.arange(1, num_sentences + 1), num_tokens).reshape(-1, 1)  # Shape: (8*42, 1)\n",
    "        sent_idx = sent_idx + (i - batch_size) # offset by batch index\n",
    "        token_idx = np.tile(np.arange(1, num_tokens + 1), num_sentences).reshape(-1, 1)    # Shape: (8*42, 1)\n",
    "        token_idx = token_idx - 1 # offset by 1\n",
    "        # also save tokens id from tokenizer\n",
    "        tokens = inputs['input_ids'].cpu().numpy().reshape(-1, 1)\n",
    "        tokens = tokens - 64000 # offset by 64000 to not overflow float16\n",
    "                                # 128000 is the vocab size of Llama 3.2 3B, float16 is [-65504, 65504]\n",
    "                \n",
    "        # Stack activations, sent_idx, and token_idx\n",
    "        batch_activations = np.hstack((batch_activations, sent_idx, token_idx, tokens)).astype(np.float16)\n",
    "        # Remove rows where attention mask is 0\n",
    "        attention_mask = inputs['attention_mask'].cpu().numpy().reshape(-1)\n",
    "        # Remove <begin_of_text> token (token_idx = 0)\n",
    "        attention_mask[np.where(token_idx == 0)] = 0\n",
    "        batch_activations = batch_activations[attention_mask != 0]\n",
    "\n",
    "        # Stack to all_data\n",
    "        all_data = np.vstack((all_data, batch_activations))\n",
    "        print(f\"all_data shape: {all_data.shape}\")\n",
    "\n",
    "        # Save to file if file_size limit is reached\n",
    "        if all_data.shape[0] >= file_size:\n",
    "            data_to_save = all_data[:file_size, :]\n",
    "            np.save(f\"activations_data/activations_batch_{files_saved:04d}.npy\", data_to_save)\n",
    "            # data_saved = np.load(f\"activations_data/activations_batch_{files_saved:04d}.npy\")\n",
    "            # print(f\"Data saved: {np.array_equal(data_to_save[:,:-3], data_saved[:,:-3])}\")\n",
    "            files_saved += 1\n",
    "            print(f\"Saved file {files_saved} == {file_size*files_saved} examples\")\n",
    "            all_data = all_data[file_size:, :]  # Retain any remaining rows\n",
    "            \n",
    "        # Reset for next batch\n",
    "        batch_texts = []\n",
    "        activation_cache = []\n",
    "\n",
    "    if i + 1 >= data_len:\n",
    "        break\n",
    "\n",
    "# Save any remaining data\n",
    "if all_data.shape[0] > 0:\n",
    "    np.save(f\"activations_data/activations_batch_{files_saved:04d}.npy\", all_data)\n",
    "    del all_data\n",
    "\n",
    "print(\"Finished processing and saving all batches\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 14743 / 60 /60 # 4.095277 hours for 10mil\n",
    "# 129*81920 + 53548 # total num of tokens 10621228\n",
    "0.5 *130 # 65 GB for 10mil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/130\n",
      "Processed 2/130\n",
      "Processed 3/130\n",
      "Processed 4/130\n",
      "Processed 5/130\n",
      "Processed 6/130\n",
      "Processed 7/130\n",
      "Processed 8/130\n",
      "Processed 9/130\n",
      "Processed 10/130\n",
      "Processed 11/130\n",
      "Processed 12/130\n",
      "Processed 13/130\n",
      "Processed 14/130\n",
      "Processed 15/130\n",
      "Processed 16/130\n",
      "Processed 17/130\n",
      "Processed 18/130\n",
      "Processed 19/130\n",
      "Processed 20/130\n",
      "Processed 21/130\n",
      "Processed 22/130\n",
      "Processed 23/130\n",
      "Processed 24/130\n",
      "Processed 25/130\n",
      "Processed 26/130\n",
      "Processed 27/130\n",
      "Processed 28/130\n",
      "Processed 29/130\n",
      "Processed 30/130\n",
      "Processed 31/130\n",
      "Processed 32/130\n",
      "Processed 33/130\n",
      "Processed 34/130\n",
      "Processed 35/130\n",
      "Processed 36/130\n",
      "Processed 37/130\n",
      "Processed 38/130\n",
      "Processed 39/130\n",
      "Processed 40/130\n",
      "Processed 41/130\n",
      "Processed 42/130\n",
      "Processed 43/130\n",
      "Processed 44/130\n",
      "Processed 45/130\n",
      "Processed 46/130\n",
      "Processed 47/130\n",
      "Processed 48/130\n",
      "Processed 49/130\n",
      "Processed 50/130\n",
      "Processed 51/130\n",
      "Processed 52/130\n",
      "Processed 53/130\n",
      "Processed 54/130\n",
      "Processed 55/130\n",
      "Processed 56/130\n",
      "Processed 57/130\n",
      "Processed 58/130\n",
      "Processed 59/130\n",
      "Processed 60/130\n",
      "Processed 61/130\n",
      "Processed 62/130\n",
      "Processed 63/130\n",
      "Processed 64/130\n",
      "Processed 65/130\n",
      "Processed 66/130\n",
      "Processed 67/130\n",
      "Processed 68/130\n",
      "Processed 69/130\n",
      "Processed 70/130\n",
      "Processed 71/130\n",
      "Processed 72/130\n",
      "Processed 73/130\n",
      "Processed 74/130\n",
      "Processed 75/130\n",
      "Processed 76/130\n",
      "Processed 77/130\n",
      "Processed 78/130\n",
      "Processed 79/130\n",
      "Processed 80/130\n",
      "Processed 81/130\n",
      "Processed 82/130\n",
      "Processed 83/130\n",
      "Processed 84/130\n",
      "Processed 85/130\n",
      "Processed 86/130\n",
      "Processed 87/130\n",
      "Processed 88/130\n",
      "Processed 89/130\n",
      "Processed 90/130\n",
      "Processed 91/130\n",
      "Processed 92/130\n",
      "Processed 93/130\n",
      "Processed 94/130\n",
      "Processed 95/130\n",
      "Processed 96/130\n",
      "Processed 97/130\n",
      "Processed 98/130\n",
      "Processed 99/130\n",
      "Processed 100/130\n",
      "Processed 101/130\n",
      "Processed 102/130\n",
      "Processed 103/130\n",
      "Processed 104/130\n",
      "Processed 105/130\n",
      "Processed 106/130\n",
      "Processed 107/130\n",
      "Processed 108/130\n",
      "Processed 109/130\n",
      "Processed 110/130\n",
      "Processed 111/130\n",
      "Processed 112/130\n",
      "Processed 113/130\n",
      "Processed 114/130\n",
      "Processed 115/130\n",
      "Processed 116/130\n",
      "Processed 117/130\n",
      "Processed 118/130\n",
      "Processed 119/130\n",
      "Processed 120/130\n",
      "Processed 121/130\n",
      "Processed 122/130\n",
      "Processed 123/130\n",
      "Processed 124/130\n",
      "Processed 125/130\n",
      "Processed 126/130\n",
      "Processed 127/130\n",
      "Processed 128/130\n",
      "Processed 129/130\n",
      "Processed 130/130\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "act_files = glob.glob(\"activations_data/act*.npy\")\n",
    "\n",
    "for i, f in enumerate(act_files):\n",
    "    # remove begin token from all npy files\n",
    "    a = np.load(f)\n",
    "    a = a[a[:, -2] != 0]\n",
    "    np.save(f, a)\n",
    "\n",
    "    f1 = f.replace(\"/activations\", \"/last3\")\n",
    "    b = np.load(f1)\n",
    "    b = b[b[:, -2] != 0]\n",
    "    np.save(f1, b)\n",
    "    print(f\"Processed {i+1}/{len(act_files)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Unregister hook\n",
    "hook_handle.remove()\n",
    "# Shutdown the model use del and free gpu\n",
    "del model\n",
    "del tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Model deleted and GPU memory freed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3466077266.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    brek here\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "brek here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = np.load(\"activations_data/activations_batch_0000.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81920, 3075)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations.shape # of float16 = 2 bytes per element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float16')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# activations type\n",
    "activations.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.503808"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations.nbytes / 1e9 # in GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11623434.0 81688\n",
      "23151360.0 163366\n",
      "34777302.0 245054\n",
      "46312032.0 326726\n",
      "57830237.0 408416\n",
      "69505953.0 490108\n",
      "80956040.0 571802\n",
      "92678321.0 653485\n",
      "104193349.0 735170\n",
      "115686836.0 816862\n",
      "127271062.0 898559\n",
      "138902768.0 980232\n",
      "150540107.0 1061930\n",
      "162015917.0 1143622\n",
      "173477853.0 1225316\n",
      "184995436.0 1307001\n",
      "196678303.0 1388696\n",
      "208342423.0 1470382\n",
      "219898030.0 1552083\n",
      "231472490.0 1633772\n",
      "243074103.0 1715463\n",
      "254650738.0 1797153\n",
      "266142412.0 1878843\n",
      "277837685.0 1960530\n",
      "289499349.0 2042224\n",
      "301109122.0 2123924\n",
      "312616766.0 2205623\n",
      "324093408.0 2287298\n",
      "335626759.0 2368987\n",
      "347126740.0 2450672\n",
      "358621118.0 2532360\n",
      "370053656.0 2614048\n",
      "381674502.0 2695742\n",
      "393177965.0 2777435\n",
      "404740926.0 2859129\n",
      "416166199.0 2940820\n",
      "427781599.0 3022496\n",
      "439357500.0 3104184\n",
      "450888817.0 3185869\n",
      "462395700.0 3267557\n",
      "473922598.0 3349244\n",
      "485484815.0 3430938\n",
      "496970895.0 3512626\n",
      "508561730.0 3594316\n",
      "520055112.0 3676006\n",
      "531655296.0 3757696\n",
      "543204825.0 3839391\n",
      "554666735.0 3921062\n",
      "566287932.0 4002744\n",
      "577835047.0 4084446\n",
      "589332703.0 4166128\n",
      "600851831.0 4247822\n",
      "612447587.0 4329520\n",
      "624034112.0 4411200\n",
      "635459074.0 4492887\n",
      "646984882.0 4574583\n",
      "658516611.0 4656272\n",
      "670172245.0 4737968\n",
      "681707708.0 4819661\n",
      "693301966.0 4901353\n",
      "704896854.0 4983035\n",
      "716424075.0 5064722\n",
      "727934019.0 5146420\n",
      "739509548.0 5228101\n",
      "751002910.0 5309789\n",
      "762550904.0 5391478\n",
      "774058693.0 5473162\n",
      "785611925.0 5554850\n",
      "797046813.0 5636548\n",
      "808547356.0 5718238\n",
      "820014738.0 5799922\n",
      "831582888.0 5881615\n",
      "843196061.0 5963299\n",
      "854673155.0 6044991\n",
      "866248853.0 6126669\n",
      "877727778.0 6208365\n",
      "889251799.0 6290057\n",
      "900747214.0 6371730\n",
      "912310973.0 6453422\n",
      "923939383.0 6535095\n",
      "935495750.0 6616789\n",
      "947057464.0 6698481\n",
      "958561502.0 6780160\n",
      "970103559.0 6861852\n",
      "981438121.0 6943547\n",
      "992966142.0 7025236\n",
      "1004451753.0 7106922\n",
      "1015999862.0 7188602\n",
      "1027526199.0 7270291\n",
      "1039044503.0 7351982\n",
      "1050523575.0 7433675\n",
      "1062019010.0 7515366\n",
      "1073663666.0 7597052\n",
      "1085211442.0 7678744\n",
      "1096790099.0 7760442\n",
      "1108444021.0 7842121\n",
      "1120109869.0 7923803\n",
      "1131611922.0 8005496\n",
      "1143184145.0 8087189\n",
      "1154671147.0 8168886\n",
      "1166291513.0 8250563\n",
      "1177878209.0 8332245\n",
      "1189189240.0 8413933\n",
      "1200743900.0 8495627\n",
      "1212298985.0 8577307\n",
      "1223764801.0 8658992\n",
      "1235339834.0 8740677\n",
      "1246812388.0 8822375\n",
      "1258322318.0 8904056\n",
      "1269805003.0 8985749\n",
      "1281467401.0 9067435\n",
      "1293068639.0 9149124\n",
      "1304586538.0 9230814\n",
      "1316238177.0 9312511\n",
      "1327691578.0 9394215\n",
      "1339291271.0 9475903\n",
      "1350765083.0 9557587\n",
      "1362312830.0 9639270\n",
      "1373812405.0 9720952\n",
      "1385388909.0 9802633\n",
      "1396931535.0 9884321\n",
      "1408600018.0 9966014\n",
      "1420064160.0 10047701\n",
      "1431684471.0 10129395\n",
      "1443267196.0 10211080\n",
      "1454855243.0 10292759\n",
      "1466293252.0 10374454\n",
      "1477830881.0 10456138\n",
      "1489365841.0 10537829\n",
      "1496957372.0 10591228\n",
      "Scale factor: 11.888623072966611\n"
     ]
    }
   ],
   "source": [
    "# Calculate scale factor\n",
    "import glob\n",
    "\n",
    "data_dir = \"activations_data\"\n",
    " # Load all batches\n",
    "all_batches = sorted(glob.glob(f\"{data_dir}/activations_batch_*.npy\"))\n",
    "\n",
    "# Load and concatenate batches\n",
    "total_norm_squared = 0\n",
    "num_samples = 0\n",
    "for f in all_batches:\n",
    "    activations = np.load(f)\n",
    "    activations = activations[:, :-3] # remove last 3 columns (sent_idx, token_idx, and token)\n",
    "    activations = activations.astype(np.float32)\n",
    "    # max_a, min_a = np.finfo(activations.dtype).max, np.finfo(activations.dtype).min\n",
    "    # activations = np.nan_to_num(activations, nan=0, posinf=max_a, neginf=min_a) # replace inf with max/min float value\n",
    "    total_norm_squared += np.sum(np.linalg.norm(activations, axis=1)**2)\n",
    "    num_samples += activations.shape[0]\n",
    "    print(total_norm_squared, num_samples)\n",
    "mean_squared_norm = total_norm_squared / num_samples\n",
    "scale_factor = np.sqrt(mean_squared_norm)\n",
    "print(f\"Scale factor: {scale_factor}\")\n",
    "\n",
    "# Save scale factor - empty txt with factor in filename\n",
    "with open(f\"{data_dir}/scale_factor_{scale_factor:.4f}.txt\", \"w\") as f:\n",
    "    f.write(f\"{scale_factor}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale_factor = 34.12206415510119 # at 1.6mil tokens\n",
    "# scale_factor = 34.128712991170886 # at 10.6mil tokens\n",
    "scale_factor = 11.888623072966611 # 10mil but with <begin> token removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
    "        # Decoder\n",
    "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = torch.relu(self.encoder(x))\n",
    "        # encoded = torch.nn.LeakyReLU(0.01)(self.encoder(x))\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded, encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class ActivationDataset(Dataset):\n",
    "    def __init__(self, data_dir, batch_size, f_type, test_fraction=0.01, scale_factor=1.0, seed=42):\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = seed\n",
    "\n",
    "        if f_type in [\"train\", \"test\", \"all\"]:\n",
    "            self.f_type = f_type\n",
    "        else:\n",
    "            raise ValueError(\"f_type must be 'train' or 'test' or 'all'\")\n",
    "        \n",
    "        if not 0 <= test_fraction <= 1:\n",
    "            raise ValueError(\"test_fraction must be between 0 and 1\")\n",
    "        self.test_fraction = test_fraction\n",
    "\n",
    "        self.scale_factor = scale_factor\n",
    "        self.file_names = sorted([f for f in os.listdir(data_dir) if f.endswith('.npy') and f.startswith('activations_batch')])\n",
    "        \n",
    "        split_idx = int(len(self.file_names) * (1 - test_fraction))\n",
    "        if f_type == \"train\":\n",
    "            self.file_names = self.file_names[:split_idx]\n",
    "        elif f_type == \"test\":\n",
    "            self.file_names = self.file_names[split_idx:]\n",
    "        else: # all\n",
    "            pass\n",
    "\n",
    "        print(f\"Loaded {len(self.file_names)} batches for {f_type} set\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.data_dir, self.file_names[idx])\n",
    "        activations = np.load(file_path)\n",
    "        if self.f_type == \"all\":\n",
    "            sent_idx = activations[:, -3]\n",
    "            token_idx = activations[:, -2] \n",
    "            token = activations[:, -1]\n",
    "        # remove last 3 columns (sent_idx, token_idx, and token)\n",
    "        activations = activations[:, :-3]\n",
    "        # normalize activations\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        activations = torch.tensor(activations, dtype=torch.float32, device=device)\n",
    "        # print(\"Activation Range Before Normalization:\", torch.min(activations).item(), torch.max(activations).item())\n",
    "        activations = activations / self.scale_factor * np.sqrt(activations.shape[1])\n",
    "        # print(\"Activation Range After Normalization:\", torch.min(activations).item(), torch.max(activations).item())\n",
    "\n",
    "        if self.f_type == \"train\":\n",
    "            # Set seed for reproducibility\n",
    "            np.random.seed(self.seed)\n",
    "            # random subsample 8192 examples\n",
    "            indices = torch.randperm(activations.shape[0], device=activations.device)[:self.batch_size]\n",
    "            activations = activations[indices]\n",
    "        \n",
    "        if self.f_type == \"all\":\n",
    "            return activations, sent_idx, token_idx, token\n",
    "        else:\n",
    "            return activations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 128 batches for train set\n",
      "Training Parameters:\n",
      "Data Directory: activations_data\n",
      "Batch Size: 2048\n",
      "Test Fraction: 0.01\n",
      "Scale Factor: 11.888623072966611\n",
      "Seed: 42\n",
      "Optimizer: Adam\n",
      "Learning Rate: 0.0001\n",
      "L1 Lambda: 0.01\n",
      "Number of Epochs: 8\n",
      "SAE Input Dimension: 3072\n",
      "SAE Hidden Dimension: 20000\n",
      "-----------------------------------\n",
      "Batch number:  0\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.22624428570270538\n",
      "MSE Loss: 1.0640, L1 Loss: 0.01*106.5588\n",
      "Batch number:  1\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.22564105689525604\n",
      "MSE Loss: 1.0291, L1 Loss: 0.01*96.5958\n",
      "Batch number:  2\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.2249046117067337\n",
      "MSE Loss: 0.9733, L1 Loss: 0.01*87.9923\n",
      "Batch number:  3\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.22413066029548645\n",
      "MSE Loss: 0.9441, L1 Loss: 0.01*79.5778\n",
      "Batch number:  4\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.22330711781978607\n",
      "MSE Loss: 0.9208, L1 Loss: 0.01*71.4463\n",
      "Batch number:  5\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.22246263921260834\n",
      "MSE Loss: 0.9333, L1 Loss: 0.01*65.3388\n",
      "Batch number:  6\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.22161903977394104\n",
      "MSE Loss: 0.9029, L1 Loss: 0.01*58.2511\n",
      "Batch number:  7\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.2207873910665512\n",
      "MSE Loss: 0.9051, L1 Loss: 0.01*52.5818\n",
      "Batch number:  8\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.21997396647930145\n",
      "MSE Loss: 0.8925, L1 Loss: 0.01*47.2717\n",
      "Batch number:  9\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.2191818505525589\n",
      "MSE Loss: 0.8826, L1 Loss: 0.01*42.3262\n",
      "Batch number:  10\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.21841469407081604\n",
      "MSE Loss: 0.8875, L1 Loss: 0.01*37.8568\n",
      "Batch number:  11\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.21767492592334747\n",
      "MSE Loss: 0.8963, L1 Loss: 0.01*34.3671\n",
      "Batch number:  12\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.21696369349956512\n",
      "MSE Loss: 0.8937, L1 Loss: 0.01*31.0069\n",
      "Batch number:  13\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.21628202497959137\n",
      "MSE Loss: 0.8795, L1 Loss: 0.01*27.7320\n",
      "Batch number:  14\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.21563051640987396\n",
      "MSE Loss: 0.8833, L1 Loss: 0.01*24.7741\n",
      "Batch number:  15\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.21501018106937408\n",
      "MSE Loss: 0.8829, L1 Loss: 0.01*22.1792\n",
      "Batch number:  16\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.2144213318824768\n",
      "MSE Loss: 0.9027, L1 Loss: 0.01*20.3421\n",
      "Batch number:  17\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.21386371552944183\n",
      "MSE Loss: 0.9090, L1 Loss: 0.01*18.2833\n",
      "Batch number:  18\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.2133372575044632\n",
      "MSE Loss: 0.8977, L1 Loss: 0.01*16.7410\n",
      "Batch number:  19\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.21284057199954987\n",
      "MSE Loss: 0.9017, L1 Loss: 0.01*15.3985\n",
      "Batch number:  20\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.21237237751483917\n",
      "MSE Loss: 0.8998, L1 Loss: 0.01*14.1774\n",
      "Batch number:  21\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.21193155646324158\n",
      "MSE Loss: 0.9048, L1 Loss: 0.01*12.7973\n",
      "Batch number:  22\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.21151752769947052\n",
      "MSE Loss: 0.9025, L1 Loss: 0.01*11.7352\n",
      "Batch number:  23\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.21112917363643646\n",
      "MSE Loss: 0.9180, L1 Loss: 0.01*11.0986\n",
      "Batch number:  24\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.210765078663826\n",
      "MSE Loss: 0.9189, L1 Loss: 0.01*10.1409\n",
      "Batch number:  25\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.21042440831661224\n",
      "MSE Loss: 0.9224, L1 Loss: 0.01*9.4316\n",
      "Batch number:  26\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.210105761885643\n",
      "MSE Loss: 0.9110, L1 Loss: 0.01*8.6160\n",
      "Batch number:  27\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.209808349609375\n",
      "MSE Loss: 0.9221, L1 Loss: 0.01*8.3026\n",
      "Batch number:  28\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.2095305621623993\n",
      "MSE Loss: 0.9164, L1 Loss: 0.01*7.8766\n",
      "Batch number:  29\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.2092711329460144\n",
      "MSE Loss: 0.9196, L1 Loss: 0.01*7.6667\n",
      "Batch number:  30\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20902828872203827\n",
      "MSE Loss: 0.9136, L1 Loss: 0.01*7.1376\n",
      "Batch number:  31\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20880140364170074\n",
      "MSE Loss: 0.9132, L1 Loss: 0.01*6.9357\n",
      "Batch number:  32\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20858918130397797\n",
      "MSE Loss: 0.9252, L1 Loss: 0.01*6.5704\n",
      "Batch number:  33\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20839084684848785\n",
      "MSE Loss: 0.9161, L1 Loss: 0.01*6.3759\n",
      "Batch number:  34\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20820537209510803\n",
      "MSE Loss: 0.9213, L1 Loss: 0.01*6.1108\n",
      "Batch number:  35\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20803202688694\n",
      "MSE Loss: 0.9122, L1 Loss: 0.01*5.9775\n",
      "Batch number:  36\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.2078697681427002\n",
      "MSE Loss: 0.9265, L1 Loss: 0.01*5.9768\n",
      "Batch number:  37\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.2077176421880722\n",
      "MSE Loss: 0.9207, L1 Loss: 0.01*5.7097\n",
      "Batch number:  38\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20757514238357544\n",
      "MSE Loss: 0.9193, L1 Loss: 0.01*5.6841\n",
      "Batch number:  39\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20744137465953827\n",
      "MSE Loss: 0.9230, L1 Loss: 0.01*5.6323\n",
      "Batch number:  40\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20731571316719055\n",
      "MSE Loss: 0.9223, L1 Loss: 0.01*5.7263\n",
      "Batch number:  41\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20719730854034424\n",
      "MSE Loss: 0.9242, L1 Loss: 0.01*5.5914\n",
      "Batch number:  42\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20708586275577545\n",
      "MSE Loss: 0.9147, L1 Loss: 0.01*5.5485\n",
      "Batch number:  43\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20698070526123047\n",
      "MSE Loss: 0.9148, L1 Loss: 0.01*5.6889\n",
      "Batch number:  44\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20688080787658691\n",
      "MSE Loss: 0.9125, L1 Loss: 0.01*5.6236\n",
      "Batch number:  45\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20678593218326569\n",
      "MSE Loss: 0.9047, L1 Loss: 0.01*5.8412\n",
      "Batch number:  46\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20669515430927277\n",
      "MSE Loss: 0.9026, L1 Loss: 0.01*5.8097\n",
      "Batch number:  47\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.2066081464290619\n",
      "MSE Loss: 0.9004, L1 Loss: 0.01*5.8866\n",
      "Batch number:  48\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20652472972869873\n",
      "MSE Loss: 0.9010, L1 Loss: 0.01*5.9542\n",
      "Batch number:  49\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20644447207450867\n",
      "MSE Loss: 0.8933, L1 Loss: 0.01*6.0253\n",
      "Batch number:  50\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.2063668966293335\n",
      "MSE Loss: 0.8875, L1 Loss: 0.01*6.2318\n",
      "Batch number:  51\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20629119873046875\n",
      "MSE Loss: 0.8825, L1 Loss: 0.01*6.4981\n",
      "Batch number:  52\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20621605217456818\n",
      "MSE Loss: 0.8897, L1 Loss: 0.01*6.6515\n",
      "Batch number:  53\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20614191889762878\n",
      "MSE Loss: 0.8807, L1 Loss: 0.01*6.9006\n",
      "Batch number:  54\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20606757700443268\n",
      "MSE Loss: 0.8678, L1 Loss: 0.01*6.8841\n",
      "Batch number:  55\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.2059931606054306\n",
      "MSE Loss: 0.8644, L1 Loss: 0.01*7.1388\n",
      "Batch number:  56\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20591774582862854\n",
      "MSE Loss: 0.8608, L1 Loss: 0.01*7.4562\n",
      "Batch number:  57\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20584100484848022\n",
      "MSE Loss: 0.8666, L1 Loss: 0.01*7.7257\n",
      "Batch number:  58\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.2057618647813797\n",
      "MSE Loss: 0.8533, L1 Loss: 0.01*7.8422\n",
      "Batch number:  59\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20568078756332397\n",
      "MSE Loss: 0.8560, L1 Loss: 0.01*8.0292\n",
      "Batch number:  60\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20559746026992798\n",
      "MSE Loss: 0.8431, L1 Loss: 0.01*8.2417\n",
      "Batch number:  61\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20551146566867828\n",
      "MSE Loss: 0.8436, L1 Loss: 0.01*8.4111\n",
      "Batch number:  62\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20542289316654205\n",
      "MSE Loss: 0.8321, L1 Loss: 0.01*8.4991\n",
      "Batch number:  63\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20533186197280884\n",
      "MSE Loss: 0.8334, L1 Loss: 0.01*8.7065\n",
      "Batch number:  64\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.205238476395607\n",
      "MSE Loss: 0.8215, L1 Loss: 0.01*9.0043\n",
      "Batch number:  65\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20514199137687683\n",
      "MSE Loss: 0.8240, L1 Loss: 0.01*8.9870\n",
      "Batch number:  66\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20504379272460938\n",
      "MSE Loss: 0.8131, L1 Loss: 0.01*9.2023\n",
      "Batch number:  67\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20494404435157776\n",
      "MSE Loss: 0.8172, L1 Loss: 0.01*9.3686\n",
      "Batch number:  68\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20484302937984467\n",
      "MSE Loss: 0.8034, L1 Loss: 0.01*9.0162\n",
      "Batch number:  69\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.2047416716814041\n",
      "MSE Loss: 0.8066, L1 Loss: 0.01*9.2465\n",
      "Batch number:  70\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.2046402096748352\n",
      "MSE Loss: 0.7981, L1 Loss: 0.01*9.2914\n",
      "Batch number:  71\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20453807711601257\n",
      "MSE Loss: 0.7973, L1 Loss: 0.01*9.5070\n",
      "Batch number:  72\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20443597435951233\n",
      "MSE Loss: 0.8022, L1 Loss: 0.01*9.6615\n",
      "Batch number:  73\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20433345437049866\n",
      "MSE Loss: 0.7982, L1 Loss: 0.01*9.5492\n",
      "Batch number:  74\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.2042311131954193\n",
      "MSE Loss: 0.7905, L1 Loss: 0.01*9.7538\n",
      "Batch number:  75\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20412896573543549\n",
      "MSE Loss: 0.7926, L1 Loss: 0.01*9.4863\n",
      "Batch number:  76\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20402729511260986\n",
      "MSE Loss: 0.7845, L1 Loss: 0.01*9.5374\n",
      "Batch number:  77\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.2039259523153305\n",
      "MSE Loss: 0.7863, L1 Loss: 0.01*9.4671\n",
      "Batch number:  78\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20382487773895264\n",
      "MSE Loss: 0.7809, L1 Loss: 0.01*9.6256\n",
      "Batch number:  79\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20372380316257477\n",
      "MSE Loss: 0.7834, L1 Loss: 0.01*9.6218\n",
      "Batch number:  80\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20362354815006256\n",
      "MSE Loss: 0.7737, L1 Loss: 0.01*9.4038\n",
      "Batch number:  81\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20352429151535034\n",
      "MSE Loss: 0.7729, L1 Loss: 0.01*9.4644\n",
      "Batch number:  82\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20342592895030975\n",
      "MSE Loss: 0.7739, L1 Loss: 0.01*9.4013\n",
      "Batch number:  83\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20332810282707214\n",
      "MSE Loss: 0.7736, L1 Loss: 0.01*9.2827\n",
      "Batch number:  84\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20323094725608826\n",
      "MSE Loss: 0.7561, L1 Loss: 0.01*9.1032\n",
      "Batch number:  85\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20313532650470734\n",
      "MSE Loss: 0.7642, L1 Loss: 0.01*9.3792\n",
      "Batch number:  86\n",
      "Active Features: 99.99%\n",
      "Decoder Weight Norm (Mean): 0.20304086804389954\n",
      "MSE Loss: 0.7634, L1 Loss: 0.01*9.2785\n",
      "Batch number:  87\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20294775068759918\n",
      "MSE Loss: 0.7569, L1 Loss: 0.01*9.3070\n",
      "Batch number:  88\n",
      "Active Features: 99.99%\n",
      "Decoder Weight Norm (Mean): 0.2028563767671585\n",
      "MSE Loss: 0.7546, L1 Loss: 0.01*9.1789\n",
      "Batch number:  89\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20276683568954468\n",
      "MSE Loss: 0.7545, L1 Loss: 0.01*9.1321\n",
      "Batch number:  90\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20267906785011292\n",
      "MSE Loss: 0.7541, L1 Loss: 0.01*9.1281\n",
      "Batch number:  91\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20259258151054382\n",
      "MSE Loss: 0.7476, L1 Loss: 0.01*9.1623\n",
      "Batch number:  92\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.2025080770254135\n",
      "MSE Loss: 0.7549, L1 Loss: 0.01*9.2755\n",
      "Batch number:  93\n",
      "Active Features: 99.99%\n",
      "Decoder Weight Norm (Mean): 0.2024255096912384\n",
      "MSE Loss: 0.7506, L1 Loss: 0.01*9.2358\n",
      "Batch number:  94\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.2023448795080185\n",
      "MSE Loss: 0.7460, L1 Loss: 0.01*9.1848\n",
      "Batch number:  95\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20226611196994781\n",
      "MSE Loss: 0.7452, L1 Loss: 0.01*9.2574\n",
      "Batch number:  96\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20218905806541443\n",
      "MSE Loss: 0.7440, L1 Loss: 0.01*9.3589\n",
      "Batch number:  97\n",
      "Active Features: 99.99%\n",
      "Decoder Weight Norm (Mean): 0.20211374759674072\n",
      "MSE Loss: 0.7372, L1 Loss: 0.01*9.1890\n",
      "Batch number:  98\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20204013586044312\n",
      "MSE Loss: 0.7469, L1 Loss: 0.01*9.2169\n",
      "Batch number:  99\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20196743309497833\n",
      "MSE Loss: 0.7329, L1 Loss: 0.01*9.2393\n",
      "Batch number:  100\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.2018958479166031\n",
      "MSE Loss: 0.7384, L1 Loss: 0.01*9.2854\n",
      "Batch number:  101\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.2018253058195114\n",
      "MSE Loss: 0.7357, L1 Loss: 0.01*9.1563\n",
      "Batch number:  102\n",
      "Active Features: 99.98%\n",
      "Decoder Weight Norm (Mean): 0.20175711810588837\n",
      "MSE Loss: 0.7172, L1 Loss: 0.01*8.9933\n",
      "Batch number:  103\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20169013738632202\n",
      "MSE Loss: 0.7268, L1 Loss: 0.01*9.2757\n",
      "Batch number:  104\n",
      "Active Features: 99.98%\n",
      "Decoder Weight Norm (Mean): 0.20162437856197357\n",
      "MSE Loss: 0.7204, L1 Loss: 0.01*9.2268\n",
      "Batch number:  105\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20156022906303406\n",
      "MSE Loss: 0.7200, L1 Loss: 0.01*9.0017\n",
      "Batch number:  106\n",
      "Active Features: 99.99%\n",
      "Decoder Weight Norm (Mean): 0.20149736106395721\n",
      "MSE Loss: 0.7094, L1 Loss: 0.01*9.2525\n",
      "Batch number:  107\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20143619179725647\n",
      "MSE Loss: 0.7127, L1 Loss: 0.01*9.1766\n",
      "Batch number:  108\n",
      "Active Features: 99.99%\n",
      "Decoder Weight Norm (Mean): 0.20137625932693481\n",
      "MSE Loss: 0.7068, L1 Loss: 0.01*9.3222\n",
      "Batch number:  109\n",
      "Active Features: 99.99%\n",
      "Decoder Weight Norm (Mean): 0.20131734013557434\n",
      "MSE Loss: 0.7129, L1 Loss: 0.01*9.1439\n",
      "Batch number:  110\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.2012595683336258\n",
      "MSE Loss: 0.7259, L1 Loss: 0.01*9.4067\n",
      "Batch number:  111\n",
      "Active Features: 99.98%\n",
      "Decoder Weight Norm (Mean): 0.20120255649089813\n",
      "MSE Loss: 0.7077, L1 Loss: 0.01*9.3913\n",
      "Batch number:  112\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.2011469602584839\n",
      "MSE Loss: 0.7036, L1 Loss: 0.01*9.3069\n",
      "Batch number:  113\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20109230279922485\n",
      "MSE Loss: 0.7120, L1 Loss: 0.01*9.1764\n",
      "Batch number:  114\n",
      "Active Features: 99.98%\n",
      "Decoder Weight Norm (Mean): 0.20103903114795685\n",
      "MSE Loss: 0.7036, L1 Loss: 0.01*9.0437\n",
      "Batch number:  115\n",
      "Active Features: 99.99%\n",
      "Decoder Weight Norm (Mean): 0.2009871006011963\n",
      "MSE Loss: 0.7048, L1 Loss: 0.01*9.3152\n",
      "Batch number:  116\n",
      "Active Features: 99.98%\n",
      "Decoder Weight Norm (Mean): 0.20093616843223572\n",
      "MSE Loss: 0.6930, L1 Loss: 0.01*9.1924\n",
      "Batch number:  117\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.2008870393037796\n",
      "MSE Loss: 0.6941, L1 Loss: 0.01*9.2973\n",
      "Batch number:  118\n",
      "Active Features: 99.98%\n",
      "Decoder Weight Norm (Mean): 0.20083896815776825\n",
      "MSE Loss: 0.6941, L1 Loss: 0.01*9.2057\n",
      "Batch number:  119\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20079240202903748\n",
      "MSE Loss: 0.6931, L1 Loss: 0.01*9.2851\n",
      "Batch number:  120\n",
      "Active Features: 99.99%\n",
      "Decoder Weight Norm (Mean): 0.20074760913848877\n",
      "MSE Loss: 0.6930, L1 Loss: 0.01*9.2777\n",
      "Batch number:  121\n",
      "Active Features: 99.99%\n",
      "Decoder Weight Norm (Mean): 0.20070432126522064\n",
      "MSE Loss: 0.6979, L1 Loss: 0.01*9.3891\n",
      "Batch number:  122\n",
      "Active Features: 99.98%\n",
      "Decoder Weight Norm (Mean): 0.20066218078136444\n",
      "MSE Loss: 0.6860, L1 Loss: 0.01*9.3406\n",
      "Batch number:  123\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20062077045440674\n",
      "MSE Loss: 0.6858, L1 Loss: 0.01*9.5149\n",
      "Batch number:  124\n",
      "Active Features: 99.98%\n",
      "Decoder Weight Norm (Mean): 0.20058003067970276\n",
      "MSE Loss: 0.6888, L1 Loss: 0.01*9.4592\n",
      "Batch number:  125\n",
      "Active Features: 99.98%\n",
      "Decoder Weight Norm (Mean): 0.20054052770137787\n",
      "MSE Loss: 0.6741, L1 Loss: 0.01*9.4454\n",
      "Batch number:  126\n",
      "Active Features: 99.97%\n",
      "Decoder Weight Norm (Mean): 0.20050232112407684\n",
      "MSE Loss: 0.6716, L1 Loss: 0.01*9.3638\n",
      "Batch number:  127\n",
      "Active Features: 99.98%\n",
      "Decoder Weight Norm (Mean): 0.20046518743038177\n",
      "MSE Loss: 0.6786, L1 Loss: 0.01*9.3844\n",
      "Epoch [1/8], Loss: 124.2575 ----------------------------\n",
      "Batch number:  0\n",
      "Active Features: 99.98%\n",
      "Decoder Weight Norm (Mean): 0.20042863488197327\n",
      "MSE Loss: 0.6818, L1 Loss: 0.01*9.5283\n",
      "Batch number:  1\n",
      "Active Features: 99.97%\n",
      "Decoder Weight Norm (Mean): 0.20039303600788116\n",
      "MSE Loss: 0.6690, L1 Loss: 0.01*9.5549\n",
      "Batch number:  2\n",
      "Active Features: 99.98%\n",
      "Decoder Weight Norm (Mean): 0.2003587782382965\n",
      "MSE Loss: 0.6765, L1 Loss: 0.01*9.5243\n",
      "Batch number:  3\n",
      "Active Features: 99.98%\n",
      "Decoder Weight Norm (Mean): 0.20032599568367004\n",
      "MSE Loss: 0.6666, L1 Loss: 0.01*9.5690\n",
      "Batch number:  4\n",
      "Active Features: 99.98%\n",
      "Decoder Weight Norm (Mean): 0.2002945989370346\n",
      "MSE Loss: 0.6663, L1 Loss: 0.01*9.5307\n",
      "Batch number:  5\n",
      "Active Features: 99.96%\n",
      "Decoder Weight Norm (Mean): 0.20026463270187378\n",
      "MSE Loss: 0.6699, L1 Loss: 0.01*9.6435\n",
      "Batch number:  6\n",
      "Active Features: 99.98%\n",
      "Decoder Weight Norm (Mean): 0.20023588836193085\n",
      "MSE Loss: 0.6682, L1 Loss: 0.01*9.4470\n",
      "Batch number:  7\n",
      "Active Features: 99.98%\n",
      "Decoder Weight Norm (Mean): 0.20020782947540283\n",
      "MSE Loss: 0.6667, L1 Loss: 0.01*9.8221\n",
      "Batch number:  8\n",
      "Active Features: 100.00%\n",
      "Decoder Weight Norm (Mean): 0.20018082857131958\n",
      "MSE Loss: 0.6603, L1 Loss: 0.01*9.5658\n",
      "Batch number:  9\n",
      "Active Features: 99.99%\n",
      "Decoder Weight Norm (Mean): 0.20015472173690796\n",
      "MSE Loss: 0.6522, L1 Loss: 0.01*9.5769\n",
      "Batch number:  10\n",
      "Active Features: 99.97%\n",
      "Decoder Weight Norm (Mean): 0.20012956857681274\n",
      "MSE Loss: 0.6609, L1 Loss: 0.01*9.6633\n",
      "Batch number:  11\n",
      "Active Features: 99.98%\n",
      "Decoder Weight Norm (Mean): 0.2001054733991623\n",
      "MSE Loss: 0.6599, L1 Loss: 0.01*9.7587\n",
      "Batch number:  12\n",
      "Active Features: 99.95%\n",
      "Decoder Weight Norm (Mean): 0.20008249580860138\n",
      "MSE Loss: 0.6582, L1 Loss: 0.01*9.7138\n",
      "Batch number:  13\n",
      "Active Features: 99.97%\n",
      "Decoder Weight Norm (Mean): 0.2000606805086136\n",
      "MSE Loss: 0.6518, L1 Loss: 0.01*9.5732\n",
      "Batch number:  14\n",
      "Active Features: 99.99%\n",
      "Decoder Weight Norm (Mean): 0.2000400871038437\n",
      "MSE Loss: 0.6498, L1 Loss: 0.01*9.5584\n",
      "Batch number:  15\n",
      "Active Features: 99.98%\n",
      "Decoder Weight Norm (Mean): 0.2000206857919693\n",
      "MSE Loss: 0.6523, L1 Loss: 0.01*9.6048\n",
      "Batch number:  16\n",
      "Active Features: 99.99%\n",
      "Decoder Weight Norm (Mean): 0.2000025063753128\n",
      "MSE Loss: 0.6580, L1 Loss: 0.01*9.8520\n",
      "Batch number:  17\n",
      "Active Features: 99.96%\n",
      "Decoder Weight Norm (Mean): 0.19998574256896973\n",
      "MSE Loss: 0.6511, L1 Loss: 0.01*9.8253\n",
      "Batch number:  18\n",
      "Active Features: 99.97%\n",
      "Decoder Weight Norm (Mean): 0.19997014105319977\n",
      "MSE Loss: 0.6559, L1 Loss: 0.01*9.6623\n",
      "Batch number:  19\n",
      "Active Features: 99.95%\n",
      "Decoder Weight Norm (Mean): 0.1999555081129074\n",
      "MSE Loss: 0.6493, L1 Loss: 0.01*9.7358\n",
      "Batch number:  20\n",
      "Active Features: 99.97%\n",
      "Decoder Weight Norm (Mean): 0.19994209706783295\n",
      "MSE Loss: 0.6460, L1 Loss: 0.01*9.6639\n",
      "Batch number:  21\n",
      "Active Features: 99.97%\n",
      "Decoder Weight Norm (Mean): 0.19992998242378235\n",
      "MSE Loss: 0.6494, L1 Loss: 0.01*9.6804\n",
      "Batch number:  22\n",
      "Active Features: 99.97%\n",
      "Decoder Weight Norm (Mean): 0.19991932809352875\n",
      "MSE Loss: 0.6365, L1 Loss: 0.01*9.7699\n",
      "Batch number:  23\n",
      "Active Features: 99.97%\n",
      "Decoder Weight Norm (Mean): 0.19990971684455872\n",
      "MSE Loss: 0.6463, L1 Loss: 0.01*9.8368\n",
      "Batch number:  24\n",
      "Active Features: 99.98%\n",
      "Decoder Weight Norm (Mean): 0.19990120828151703\n",
      "MSE Loss: 0.6394, L1 Loss: 0.01*9.9736\n",
      "Batch number:  25\n",
      "Active Features: 99.94%\n",
      "Decoder Weight Norm (Mean): 0.19989366829395294\n",
      "MSE Loss: 0.6374, L1 Loss: 0.01*9.8090\n",
      "Batch number:  26\n",
      "Active Features: 99.97%\n",
      "Decoder Weight Norm (Mean): 0.19988742470741272\n",
      "MSE Loss: 0.6319, L1 Loss: 0.01*9.7836\n",
      "Batch number:  27\n",
      "Active Features: 99.94%\n",
      "Decoder Weight Norm (Mean): 0.1998828500509262\n",
      "MSE Loss: 0.6291, L1 Loss: 0.01*9.7558\n",
      "Batch number:  28\n",
      "Active Features: 99.94%\n",
      "Decoder Weight Norm (Mean): 0.19987979531288147\n",
      "MSE Loss: 0.6332, L1 Loss: 0.01*9.7854\n",
      "Batch number:  29\n",
      "Active Features: 99.95%\n",
      "Decoder Weight Norm (Mean): 0.1998778134584427\n",
      "MSE Loss: 0.6350, L1 Loss: 0.01*9.7730\n",
      "Batch number:  30\n",
      "Active Features: 99.94%\n",
      "Decoder Weight Norm (Mean): 0.19987672567367554\n",
      "MSE Loss: 0.6291, L1 Loss: 0.01*9.8316\n",
      "Batch number:  31\n",
      "Active Features: 99.96%\n",
      "Decoder Weight Norm (Mean): 0.1998765766620636\n",
      "MSE Loss: 0.6212, L1 Loss: 0.01*9.6972\n",
      "Batch number:  32\n",
      "Active Features: 99.95%\n",
      "Decoder Weight Norm (Mean): 0.1998773217201233\n",
      "MSE Loss: 0.6317, L1 Loss: 0.01*9.9443\n",
      "Batch number:  33\n",
      "Active Features: 99.95%\n",
      "Decoder Weight Norm (Mean): 0.19987912476062775\n",
      "MSE Loss: 0.6256, L1 Loss: 0.01*9.8870\n",
      "Batch number:  34\n",
      "Active Features: 99.94%\n",
      "Decoder Weight Norm (Mean): 0.19988201558589935\n",
      "MSE Loss: 0.6229, L1 Loss: 0.01*9.9685\n",
      "Batch number:  35\n",
      "Active Features: 99.95%\n",
      "Decoder Weight Norm (Mean): 0.19988635182380676\n",
      "MSE Loss: 0.6202, L1 Loss: 0.01*9.8204\n",
      "Batch number:  36\n",
      "Active Features: 99.98%\n",
      "Decoder Weight Norm (Mean): 0.19989117980003357\n",
      "MSE Loss: 0.6250, L1 Loss: 0.01*10.0116\n",
      "Batch number:  37\n",
      "Active Features: 99.94%\n",
      "Decoder Weight Norm (Mean): 0.19989696145057678\n",
      "MSE Loss: 0.6322, L1 Loss: 0.01*9.9372\n",
      "Batch number:  38\n",
      "Active Features: 99.96%\n",
      "Decoder Weight Norm (Mean): 0.19990363717079163\n",
      "MSE Loss: 0.6230, L1 Loss: 0.01*9.9373\n",
      "Batch number:  39\n",
      "Active Features: 99.96%\n",
      "Decoder Weight Norm (Mean): 0.19991129636764526\n",
      "MSE Loss: 0.6224, L1 Loss: 0.01*9.9089\n",
      "Batch number:  40\n",
      "Active Features: 99.95%\n",
      "Decoder Weight Norm (Mean): 0.19991986453533173\n",
      "MSE Loss: 0.6238, L1 Loss: 0.01*9.9694\n",
      "Batch number:  41\n",
      "Active Features: 99.91%\n",
      "Decoder Weight Norm (Mean): 0.19992931187152863\n",
      "MSE Loss: 0.6183, L1 Loss: 0.01*10.0210\n",
      "Batch number:  42\n",
      "Active Features: 99.94%\n",
      "Decoder Weight Norm (Mean): 0.1999398171901703\n",
      "MSE Loss: 0.6121, L1 Loss: 0.01*9.8581\n",
      "Batch number:  43\n",
      "Active Features: 99.92%\n",
      "Decoder Weight Norm (Mean): 0.1999511867761612\n",
      "MSE Loss: 0.6164, L1 Loss: 0.01*10.0149\n",
      "Batch number:  44\n",
      "Active Features: 99.91%\n",
      "Decoder Weight Norm (Mean): 0.19996309280395508\n",
      "MSE Loss: 0.6169, L1 Loss: 0.01*9.8894\n",
      "Batch number:  45\n",
      "Active Features: 99.91%\n",
      "Decoder Weight Norm (Mean): 0.19997568428516388\n",
      "MSE Loss: 0.6127, L1 Loss: 0.01*10.0684\n",
      "Batch number:  46\n",
      "Active Features: 99.95%\n",
      "Decoder Weight Norm (Mean): 0.19998852908611298\n",
      "MSE Loss: 0.6198, L1 Loss: 0.01*9.9230\n",
      "Batch number:  47\n",
      "Active Features: 99.90%\n",
      "Decoder Weight Norm (Mean): 0.2000022530555725\n",
      "MSE Loss: 0.6046, L1 Loss: 0.01*10.0468\n",
      "Batch number:  48\n",
      "Active Features: 99.91%\n",
      "Decoder Weight Norm (Mean): 0.2000172734260559\n",
      "MSE Loss: 0.6095, L1 Loss: 0.01*9.9965\n",
      "Batch number:  49\n",
      "Active Features: 99.95%\n",
      "Decoder Weight Norm (Mean): 0.20003312826156616\n",
      "MSE Loss: 0.6102, L1 Loss: 0.01*9.9661\n",
      "Batch number:  50\n",
      "Active Features: 99.91%\n",
      "Decoder Weight Norm (Mean): 0.2000505030155182\n",
      "MSE Loss: 0.6069, L1 Loss: 0.01*10.0269\n",
      "Batch number:  51\n",
      "Active Features: 99.94%\n",
      "Decoder Weight Norm (Mean): 0.2000684291124344\n",
      "MSE Loss: 0.6144, L1 Loss: 0.01*10.0011\n",
      "Batch number:  52\n",
      "Active Features: 99.91%\n",
      "Decoder Weight Norm (Mean): 0.20008674263954163\n",
      "MSE Loss: 0.6021, L1 Loss: 0.01*10.1004\n",
      "Batch number:  53\n",
      "Active Features: 99.94%\n",
      "Decoder Weight Norm (Mean): 0.20010612905025482\n",
      "MSE Loss: 0.6114, L1 Loss: 0.01*10.1869\n",
      "Batch number:  54\n",
      "Active Features: 99.93%\n",
      "Decoder Weight Norm (Mean): 0.2001262754201889\n",
      "MSE Loss: 0.6001, L1 Loss: 0.01*9.9908\n",
      "Batch number:  55\n",
      "Active Features: 99.93%\n",
      "Decoder Weight Norm (Mean): 0.20014688372612\n",
      "MSE Loss: 0.6035, L1 Loss: 0.01*10.0996\n",
      "Batch number:  56\n",
      "Active Features: 99.86%\n",
      "Decoder Weight Norm (Mean): 0.20016804337501526\n",
      "MSE Loss: 0.6012, L1 Loss: 0.01*10.0775\n",
      "Batch number:  57\n",
      "Active Features: 99.85%\n",
      "Decoder Weight Norm (Mean): 0.20019036531448364\n",
      "MSE Loss: 0.6054, L1 Loss: 0.01*10.1618\n",
      "Batch number:  58\n",
      "Active Features: 99.87%\n",
      "Decoder Weight Norm (Mean): 0.20021329820156097\n",
      "MSE Loss: 0.5992, L1 Loss: 0.01*10.1229\n",
      "Batch number:  59\n",
      "Active Features: 99.90%\n",
      "Decoder Weight Norm (Mean): 0.2002372443675995\n",
      "MSE Loss: 0.5975, L1 Loss: 0.01*10.1720\n",
      "Batch number:  60\n",
      "Active Features: 99.91%\n",
      "Decoder Weight Norm (Mean): 0.20026209950447083\n",
      "MSE Loss: 0.5985, L1 Loss: 0.01*10.0567\n",
      "Batch number:  61\n",
      "Active Features: 99.87%\n",
      "Decoder Weight Norm (Mean): 0.20028802752494812\n",
      "MSE Loss: 0.6011, L1 Loss: 0.01*10.1288\n",
      "Batch number:  62\n",
      "Active Features: 99.88%\n",
      "Decoder Weight Norm (Mean): 0.20031487941741943\n",
      "MSE Loss: 0.5984, L1 Loss: 0.01*10.0689\n",
      "Batch number:  63\n",
      "Active Features: 99.91%\n",
      "Decoder Weight Norm (Mean): 0.20034241676330566\n",
      "MSE Loss: 0.6026, L1 Loss: 0.01*10.1207\n",
      "Batch number:  64\n",
      "Active Features: 99.91%\n",
      "Decoder Weight Norm (Mean): 0.20037062466144562\n",
      "MSE Loss: 0.5951, L1 Loss: 0.01*9.9664\n",
      "Batch number:  65\n",
      "Active Features: 99.84%\n",
      "Decoder Weight Norm (Mean): 0.20039969682693481\n",
      "MSE Loss: 0.5913, L1 Loss: 0.01*10.0596\n",
      "Batch number:  66\n",
      "Active Features: 99.88%\n",
      "Decoder Weight Norm (Mean): 0.20043016970157623\n",
      "MSE Loss: 0.5918, L1 Loss: 0.01*10.1694\n",
      "Batch number:  67\n",
      "Active Features: 99.91%\n",
      "Decoder Weight Norm (Mean): 0.2004615217447281\n",
      "MSE Loss: 0.5989, L1 Loss: 0.01*10.1463\n",
      "Batch number:  68\n",
      "Active Features: 99.83%\n",
      "Decoder Weight Norm (Mean): 0.20049376785755157\n",
      "MSE Loss: 0.5892, L1 Loss: 0.01*10.0381\n",
      "Batch number:  69\n",
      "Active Features: 99.91%\n",
      "Decoder Weight Norm (Mean): 0.20052656531333923\n",
      "MSE Loss: 0.5936, L1 Loss: 0.01*10.1952\n",
      "Batch number:  70\n",
      "Active Features: 99.92%\n",
      "Decoder Weight Norm (Mean): 0.2005600482225418\n",
      "MSE Loss: 0.5940, L1 Loss: 0.01*10.1270\n",
      "Batch number:  71\n",
      "Active Features: 99.91%\n",
      "Decoder Weight Norm (Mean): 0.20059366524219513\n",
      "MSE Loss: 0.5894, L1 Loss: 0.01*10.1636\n",
      "Batch number:  72\n",
      "Active Features: 99.88%\n",
      "Decoder Weight Norm (Mean): 0.20062772929668427\n",
      "MSE Loss: 0.5935, L1 Loss: 0.01*10.2132\n",
      "Batch number:  73\n",
      "Active Features: 99.86%\n",
      "Decoder Weight Norm (Mean): 0.2006622552871704\n",
      "MSE Loss: 0.5835, L1 Loss: 0.01*10.0427\n",
      "Batch number:  74\n",
      "Active Features: 99.81%\n",
      "Decoder Weight Norm (Mean): 0.20069724321365356\n",
      "MSE Loss: 0.5792, L1 Loss: 0.01*10.2854\n",
      "Batch number:  75\n",
      "Active Features: 99.87%\n",
      "Decoder Weight Norm (Mean): 0.20073342323303223\n",
      "MSE Loss: 0.5913, L1 Loss: 0.01*10.1970\n",
      "Batch number:  76\n",
      "Active Features: 99.88%\n",
      "Decoder Weight Norm (Mean): 0.20077015459537506\n",
      "MSE Loss: 0.5837, L1 Loss: 0.01*10.3309\n",
      "Batch number:  77\n",
      "Active Features: 99.84%\n",
      "Decoder Weight Norm (Mean): 0.2008066326379776\n",
      "MSE Loss: 0.5815, L1 Loss: 0.01*10.1381\n",
      "Batch number:  78\n",
      "Active Features: 99.86%\n",
      "Decoder Weight Norm (Mean): 0.20084354281425476\n",
      "MSE Loss: 0.5881, L1 Loss: 0.01*10.2925\n",
      "Batch number:  79\n",
      "Active Features: 99.82%\n",
      "Decoder Weight Norm (Mean): 0.20088107883930206\n",
      "MSE Loss: 0.5895, L1 Loss: 0.01*10.3384\n",
      "Batch number:  80\n",
      "Active Features: 99.81%\n",
      "Decoder Weight Norm (Mean): 0.20091895759105682\n",
      "MSE Loss: 0.5817, L1 Loss: 0.01*10.2764\n",
      "Batch number:  81\n",
      "Active Features: 99.80%\n",
      "Decoder Weight Norm (Mean): 0.20095723867416382\n",
      "MSE Loss: 0.5836, L1 Loss: 0.01*10.2443\n",
      "Batch number:  82\n",
      "Active Features: 99.83%\n",
      "Decoder Weight Norm (Mean): 0.20099665224552155\n",
      "MSE Loss: 0.5791, L1 Loss: 0.01*10.1626\n",
      "Batch number:  83\n",
      "Active Features: 99.83%\n",
      "Decoder Weight Norm (Mean): 0.20103633403778076\n",
      "MSE Loss: 0.5868, L1 Loss: 0.01*10.2181\n",
      "Batch number:  84\n",
      "Active Features: 99.80%\n",
      "Decoder Weight Norm (Mean): 0.20107613503932953\n",
      "MSE Loss: 0.5700, L1 Loss: 0.01*10.1531\n",
      "Batch number:  85\n",
      "Active Features: 99.73%\n",
      "Decoder Weight Norm (Mean): 0.2011166512966156\n",
      "MSE Loss: 0.5720, L1 Loss: 0.01*10.3464\n",
      "Batch number:  86\n",
      "Active Features: 99.84%\n",
      "Decoder Weight Norm (Mean): 0.20115765929222107\n",
      "MSE Loss: 0.5733, L1 Loss: 0.01*10.2842\n",
      "Batch number:  87\n",
      "Active Features: 99.73%\n",
      "Decoder Weight Norm (Mean): 0.20119953155517578\n",
      "MSE Loss: 0.5713, L1 Loss: 0.01*10.4029\n",
      "Batch number:  88\n",
      "Active Features: 99.74%\n",
      "Decoder Weight Norm (Mean): 0.20124205946922302\n",
      "MSE Loss: 0.5751, L1 Loss: 0.01*10.2912\n",
      "Batch number:  89\n",
      "Active Features: 99.84%\n",
      "Decoder Weight Norm (Mean): 0.2012854516506195\n",
      "MSE Loss: 0.5736, L1 Loss: 0.01*10.2672\n",
      "Batch number:  90\n",
      "Active Features: 99.83%\n",
      "Decoder Weight Norm (Mean): 0.2013295292854309\n",
      "MSE Loss: 0.5699, L1 Loss: 0.01*10.1932\n",
      "Batch number:  91\n",
      "Active Features: 99.75%\n",
      "Decoder Weight Norm (Mean): 0.20137380063533783\n",
      "MSE Loss: 0.5732, L1 Loss: 0.01*10.3870\n",
      "Batch number:  92\n",
      "Active Features: 99.78%\n",
      "Decoder Weight Norm (Mean): 0.20141904056072235\n",
      "MSE Loss: 0.5735, L1 Loss: 0.01*10.4475\n",
      "Batch number:  93\n",
      "Active Features: 99.76%\n",
      "Decoder Weight Norm (Mean): 0.2014651596546173\n",
      "MSE Loss: 0.5759, L1 Loss: 0.01*10.4002\n",
      "Batch number:  94\n",
      "Active Features: 99.73%\n",
      "Decoder Weight Norm (Mean): 0.20151188969612122\n",
      "MSE Loss: 0.5702, L1 Loss: 0.01*10.3223\n",
      "Batch number:  95\n",
      "Active Features: 99.61%\n",
      "Decoder Weight Norm (Mean): 0.20155981183052063\n",
      "MSE Loss: 0.5668, L1 Loss: 0.01*10.4443\n",
      "Batch number:  96\n",
      "Active Features: 99.76%\n",
      "Decoder Weight Norm (Mean): 0.20160812139511108\n",
      "MSE Loss: 0.5755, L1 Loss: 0.01*10.4436\n",
      "Batch number:  97\n",
      "Active Features: 99.76%\n",
      "Decoder Weight Norm (Mean): 0.20165687799453735\n",
      "MSE Loss: 0.5712, L1 Loss: 0.01*10.3391\n",
      "Batch number:  98\n",
      "Active Features: 99.84%\n",
      "Decoder Weight Norm (Mean): 0.20170585811138153\n",
      "MSE Loss: 0.5709, L1 Loss: 0.01*10.3586\n",
      "Batch number:  99\n",
      "Active Features: 99.73%\n",
      "Decoder Weight Norm (Mean): 0.20175501704216003\n",
      "MSE Loss: 0.5693, L1 Loss: 0.01*10.3413\n",
      "Batch number:  100\n",
      "Active Features: 99.70%\n",
      "Decoder Weight Norm (Mean): 0.2018045037984848\n",
      "MSE Loss: 0.5757, L1 Loss: 0.01*10.3759\n",
      "Batch number:  101\n",
      "Active Features: 99.61%\n",
      "Decoder Weight Norm (Mean): 0.20185446739196777\n",
      "MSE Loss: 0.5677, L1 Loss: 0.01*10.4602\n",
      "Batch number:  102\n",
      "Active Features: 99.67%\n",
      "Decoder Weight Norm (Mean): 0.2019048035144806\n",
      "MSE Loss: 0.5594, L1 Loss: 0.01*10.2106\n",
      "Batch number:  103\n",
      "Active Features: 99.68%\n",
      "Decoder Weight Norm (Mean): 0.2019556164741516\n",
      "MSE Loss: 0.5624, L1 Loss: 0.01*10.4957\n",
      "Batch number:  104\n",
      "Active Features: 99.73%\n",
      "Decoder Weight Norm (Mean): 0.20200660824775696\n",
      "MSE Loss: 0.5665, L1 Loss: 0.01*10.4468\n",
      "Batch number:  105\n",
      "Active Features: 99.80%\n",
      "Decoder Weight Norm (Mean): 0.20205765962600708\n",
      "MSE Loss: 0.5684, L1 Loss: 0.01*10.4572\n",
      "Batch number:  106\n",
      "Active Features: 99.62%\n",
      "Decoder Weight Norm (Mean): 0.20210886001586914\n",
      "MSE Loss: 0.5605, L1 Loss: 0.01*10.5596\n",
      "Batch number:  107\n",
      "Active Features: 99.67%\n",
      "Decoder Weight Norm (Mean): 0.20216025412082672\n",
      "MSE Loss: 0.5603, L1 Loss: 0.01*10.4032\n",
      "Batch number:  108\n",
      "Active Features: 99.69%\n",
      "Decoder Weight Norm (Mean): 0.2022118866443634\n",
      "MSE Loss: 0.5599, L1 Loss: 0.01*10.5225\n",
      "Batch number:  109\n",
      "Active Features: 99.67%\n",
      "Decoder Weight Norm (Mean): 0.20226389169692993\n",
      "MSE Loss: 0.5578, L1 Loss: 0.01*10.4802\n",
      "Batch number:  110\n",
      "Active Features: 99.67%\n",
      "Decoder Weight Norm (Mean): 0.20231594145298004\n",
      "MSE Loss: 0.5749, L1 Loss: 0.01*10.4570\n",
      "Batch number:  111\n",
      "Active Features: 99.63%\n",
      "Decoder Weight Norm (Mean): 0.20236830413341522\n",
      "MSE Loss: 0.5626, L1 Loss: 0.01*10.5512\n",
      "Batch number:  112\n",
      "Active Features: 99.69%\n",
      "Decoder Weight Norm (Mean): 0.20242062211036682\n",
      "MSE Loss: 0.5543, L1 Loss: 0.01*10.5493\n",
      "Batch number:  113\n",
      "Active Features: 99.64%\n",
      "Decoder Weight Norm (Mean): 0.2024729996919632\n",
      "MSE Loss: 0.5684, L1 Loss: 0.01*10.5460\n",
      "Batch number:  114\n",
      "Active Features: 99.72%\n",
      "Decoder Weight Norm (Mean): 0.20252566039562225\n",
      "MSE Loss: 0.5595, L1 Loss: 0.01*10.3795\n",
      "Batch number:  115\n",
      "Active Features: 99.65%\n",
      "Decoder Weight Norm (Mean): 0.2025780975818634\n",
      "MSE Loss: 0.5565, L1 Loss: 0.01*10.6521\n",
      "Batch number:  116\n",
      "Active Features: 99.49%\n",
      "Decoder Weight Norm (Mean): 0.20263086259365082\n",
      "MSE Loss: 0.5538, L1 Loss: 0.01*10.5544\n",
      "Batch number:  117\n",
      "Active Features: 99.62%\n",
      "Decoder Weight Norm (Mean): 0.2026841938495636\n",
      "MSE Loss: 0.5545, L1 Loss: 0.01*10.6053\n",
      "Batch number:  118\n",
      "Active Features: 99.60%\n",
      "Decoder Weight Norm (Mean): 0.20273810625076294\n",
      "MSE Loss: 0.5586, L1 Loss: 0.01*10.4962\n",
      "Batch number:  119\n",
      "Active Features: 99.56%\n",
      "Decoder Weight Norm (Mean): 0.20279259979724884\n",
      "MSE Loss: 0.5508, L1 Loss: 0.01*10.6278\n",
      "Batch number:  120\n",
      "Active Features: 99.57%\n",
      "Decoder Weight Norm (Mean): 0.20284698903560638\n",
      "MSE Loss: 0.5510, L1 Loss: 0.01*10.5847\n",
      "Batch number:  121\n",
      "Active Features: 99.47%\n",
      "Decoder Weight Norm (Mean): 0.20290149748325348\n",
      "MSE Loss: 0.5565, L1 Loss: 0.01*10.6634\n",
      "Batch number:  122\n",
      "Active Features: 99.65%\n",
      "Decoder Weight Norm (Mean): 0.20295675098896027\n",
      "MSE Loss: 0.5477, L1 Loss: 0.01*10.4793\n",
      "Batch number:  123\n",
      "Active Features: 99.61%\n",
      "Decoder Weight Norm (Mean): 0.2030121237039566\n",
      "MSE Loss: 0.5548, L1 Loss: 0.01*10.6738\n",
      "Batch number:  124\n",
      "Active Features: 99.58%\n",
      "Decoder Weight Norm (Mean): 0.20306755602359772\n",
      "MSE Loss: 0.5511, L1 Loss: 0.01*10.6426\n",
      "Batch number:  125\n",
      "Active Features: 99.50%\n",
      "Decoder Weight Norm (Mean): 0.20312334597110748\n",
      "MSE Loss: 0.5414, L1 Loss: 0.01*10.7602\n",
      "Batch number:  126\n",
      "Active Features: 99.41%\n",
      "Decoder Weight Norm (Mean): 0.2031792402267456\n",
      "MSE Loss: 0.5423, L1 Loss: 0.01*10.5441\n",
      "Batch number:  127\n",
      "Active Features: 99.41%\n",
      "Decoder Weight Norm (Mean): 0.20323576033115387\n",
      "MSE Loss: 0.5542, L1 Loss: 0.01*10.6111\n",
      "Epoch [2/8], Loss: 89.9410 ----------------------------\n",
      "Batch number:  0\n",
      "Active Features: 99.58%\n",
      "Decoder Weight Norm (Mean): 0.2032925933599472\n",
      "MSE Loss: 0.5520, L1 Loss: 0.01*10.7760\n",
      "Batch number:  1\n",
      "Active Features: 99.48%\n",
      "Decoder Weight Norm (Mean): 0.20334941148757935\n",
      "MSE Loss: 0.5416, L1 Loss: 0.01*10.6980\n",
      "Batch number:  2\n",
      "Active Features: 99.50%\n",
      "Decoder Weight Norm (Mean): 0.2034066915512085\n",
      "MSE Loss: 0.5530, L1 Loss: 0.01*10.7196\n",
      "Batch number:  3\n",
      "Active Features: 99.36%\n",
      "Decoder Weight Norm (Mean): 0.2034648358821869\n",
      "MSE Loss: 0.5357, L1 Loss: 0.01*10.8545\n",
      "Batch number:  4\n",
      "Active Features: 99.38%\n",
      "Decoder Weight Norm (Mean): 0.20352281630039215\n",
      "MSE Loss: 0.5373, L1 Loss: 0.01*10.7301\n",
      "Batch number:  5\n",
      "Active Features: 99.37%\n",
      "Decoder Weight Norm (Mean): 0.20358142256736755\n",
      "MSE Loss: 0.5498, L1 Loss: 0.01*10.8573\n",
      "Batch number:  6\n",
      "Active Features: 99.43%\n",
      "Decoder Weight Norm (Mean): 0.20364060997962952\n",
      "MSE Loss: 0.5442, L1 Loss: 0.01*10.5576\n",
      "Batch number:  7\n",
      "Active Features: 99.59%\n",
      "Decoder Weight Norm (Mean): 0.203700453042984\n",
      "MSE Loss: 0.5521, L1 Loss: 0.01*10.9602\n",
      "Batch number:  8\n",
      "Active Features: 99.59%\n",
      "Decoder Weight Norm (Mean): 0.20376043021678925\n",
      "MSE Loss: 0.5464, L1 Loss: 0.01*10.7211\n",
      "Batch number:  9\n",
      "Active Features: 99.42%\n",
      "Decoder Weight Norm (Mean): 0.2038206309080124\n",
      "MSE Loss: 0.5387, L1 Loss: 0.01*10.8173\n",
      "Batch number:  10\n",
      "Active Features: 99.37%\n",
      "Decoder Weight Norm (Mean): 0.20388054847717285\n",
      "MSE Loss: 0.5472, L1 Loss: 0.01*10.7982\n",
      "Batch number:  11\n",
      "Active Features: 99.39%\n",
      "Decoder Weight Norm (Mean): 0.2039409726858139\n",
      "MSE Loss: 0.5413, L1 Loss: 0.01*10.8793\n",
      "Batch number:  12\n",
      "Active Features: 99.32%\n",
      "Decoder Weight Norm (Mean): 0.2040017992258072\n",
      "MSE Loss: 0.5355, L1 Loss: 0.01*10.9289\n",
      "Batch number:  13\n",
      "Active Features: 99.45%\n",
      "Decoder Weight Norm (Mean): 0.2040625959634781\n",
      "MSE Loss: 0.5379, L1 Loss: 0.01*10.6824\n",
      "Batch number:  14\n",
      "Active Features: 99.39%\n",
      "Decoder Weight Norm (Mean): 0.2041240632534027\n",
      "MSE Loss: 0.5363, L1 Loss: 0.01*10.7853\n",
      "Batch number:  15\n",
      "Active Features: 99.36%\n",
      "Decoder Weight Norm (Mean): 0.20418570935726166\n",
      "MSE Loss: 0.5407, L1 Loss: 0.01*10.7940\n",
      "Batch number:  16\n",
      "Active Features: 99.49%\n",
      "Decoder Weight Norm (Mean): 0.20424821972846985\n",
      "MSE Loss: 0.5444, L1 Loss: 0.01*10.8482\n",
      "Batch number:  17\n",
      "Active Features: 99.34%\n",
      "Decoder Weight Norm (Mean): 0.20431067049503326\n",
      "MSE Loss: 0.5360, L1 Loss: 0.01*10.8222\n",
      "Batch number:  18\n",
      "Active Features: 99.34%\n",
      "Decoder Weight Norm (Mean): 0.20437350869178772\n",
      "MSE Loss: 0.5402, L1 Loss: 0.01*10.8363\n",
      "Batch number:  19\n",
      "Active Features: 99.41%\n",
      "Decoder Weight Norm (Mean): 0.20443658530712128\n",
      "MSE Loss: 0.5448, L1 Loss: 0.01*10.7180\n",
      "Batch number:  20\n",
      "Active Features: 99.27%\n",
      "Decoder Weight Norm (Mean): 0.20450013875961304\n",
      "MSE Loss: 0.5412, L1 Loss: 0.01*10.8531\n",
      "Batch number:  21\n",
      "Active Features: 99.37%\n",
      "Decoder Weight Norm (Mean): 0.2045641988515854\n",
      "MSE Loss: 0.5366, L1 Loss: 0.01*10.8917\n",
      "Batch number:  22\n",
      "Active Features: 99.35%\n",
      "Decoder Weight Norm (Mean): 0.2046279013156891\n",
      "MSE Loss: 0.5378, L1 Loss: 0.01*10.7898\n",
      "Batch number:  23\n",
      "Active Features: 99.25%\n",
      "Decoder Weight Norm (Mean): 0.20469188690185547\n",
      "MSE Loss: 0.5442, L1 Loss: 0.01*10.9625\n",
      "Batch number:  24\n",
      "Active Features: 99.23%\n",
      "Decoder Weight Norm (Mean): 0.20475609600543976\n",
      "MSE Loss: 0.5367, L1 Loss: 0.01*10.9562\n",
      "Batch number:  25\n",
      "Active Features: 99.41%\n",
      "Decoder Weight Norm (Mean): 0.2048204094171524\n",
      "MSE Loss: 0.5415, L1 Loss: 0.01*10.9287\n",
      "Batch number:  26\n",
      "Active Features: 99.28%\n",
      "Decoder Weight Norm (Mean): 0.2048850953578949\n",
      "MSE Loss: 0.5287, L1 Loss: 0.01*10.8338\n",
      "Batch number:  27\n",
      "Active Features: 99.25%\n",
      "Decoder Weight Norm (Mean): 0.2049499899148941\n",
      "MSE Loss: 0.5249, L1 Loss: 0.01*10.8709\n",
      "Batch number:  28\n",
      "Active Features: 99.30%\n",
      "Decoder Weight Norm (Mean): 0.20501568913459778\n",
      "MSE Loss: 0.5346, L1 Loss: 0.01*10.8512\n",
      "Batch number:  29\n",
      "Active Features: 99.34%\n",
      "Decoder Weight Norm (Mean): 0.20508158206939697\n",
      "MSE Loss: 0.5357, L1 Loss: 0.01*10.7281\n",
      "Batch number:  30\n",
      "Active Features: 99.19%\n",
      "Decoder Weight Norm (Mean): 0.20514735579490662\n",
      "MSE Loss: 0.5305, L1 Loss: 0.01*10.9124\n",
      "Batch number:  31\n",
      "Active Features: 99.30%\n",
      "Decoder Weight Norm (Mean): 0.20521308481693268\n",
      "MSE Loss: 0.5268, L1 Loss: 0.01*10.8524\n",
      "Batch number:  32\n",
      "Active Features: 99.27%\n",
      "Decoder Weight Norm (Mean): 0.20527832210063934\n",
      "MSE Loss: 0.5285, L1 Loss: 0.01*10.9777\n",
      "Batch number:  33\n",
      "Active Features: 99.22%\n",
      "Decoder Weight Norm (Mean): 0.20534354448318481\n",
      "MSE Loss: 0.5257, L1 Loss: 0.01*10.8852\n",
      "Batch number:  34\n",
      "Active Features: 98.94%\n",
      "Decoder Weight Norm (Mean): 0.20540843904018402\n",
      "MSE Loss: 0.5267, L1 Loss: 0.01*11.0588\n",
      "Batch number:  35\n",
      "Active Features: 99.33%\n",
      "Decoder Weight Norm (Mean): 0.20547333359718323\n",
      "MSE Loss: 0.5313, L1 Loss: 0.01*10.9035\n",
      "Batch number:  36\n",
      "Active Features: 99.12%\n",
      "Decoder Weight Norm (Mean): 0.20553812384605408\n",
      "MSE Loss: 0.5276, L1 Loss: 0.01*11.0836\n",
      "Batch number:  37\n",
      "Active Features: 99.19%\n",
      "Decoder Weight Norm (Mean): 0.20560286939144135\n",
      "MSE Loss: 0.5298, L1 Loss: 0.01*10.9875\n",
      "Batch number:  38\n",
      "Active Features: 99.19%\n",
      "Decoder Weight Norm (Mean): 0.20566809177398682\n",
      "MSE Loss: 0.5307, L1 Loss: 0.01*10.9436\n",
      "Batch number:  39\n",
      "Active Features: 99.17%\n",
      "Decoder Weight Norm (Mean): 0.20573364198207855\n",
      "MSE Loss: 0.5318, L1 Loss: 0.01*10.9806\n",
      "Batch number:  40\n",
      "Active Features: 99.09%\n",
      "Decoder Weight Norm (Mean): 0.20579960942268372\n",
      "MSE Loss: 0.5246, L1 Loss: 0.01*11.0299\n",
      "Batch number:  41\n",
      "Active Features: 99.09%\n",
      "Decoder Weight Norm (Mean): 0.20586591958999634\n",
      "MSE Loss: 0.5267, L1 Loss: 0.01*11.0495\n",
      "Batch number:  42\n",
      "Active Features: 98.95%\n",
      "Decoder Weight Norm (Mean): 0.20593273639678955\n",
      "MSE Loss: 0.5133, L1 Loss: 0.01*10.9366\n",
      "Batch number:  43\n",
      "Active Features: 99.14%\n",
      "Decoder Weight Norm (Mean): 0.20599916577339172\n",
      "MSE Loss: 0.5269, L1 Loss: 0.01*11.0861\n",
      "Batch number:  44\n",
      "Active Features: 99.34%\n",
      "Decoder Weight Norm (Mean): 0.20606566965579987\n",
      "MSE Loss: 0.5244, L1 Loss: 0.01*11.0330\n",
      "Batch number:  45\n",
      "Active Features: 99.06%\n",
      "Decoder Weight Norm (Mean): 0.2061319500207901\n",
      "MSE Loss: 0.5249, L1 Loss: 0.01*11.2062\n",
      "Batch number:  46\n",
      "Active Features: 99.18%\n",
      "Decoder Weight Norm (Mean): 0.20619802176952362\n",
      "MSE Loss: 0.5271, L1 Loss: 0.01*10.9683\n",
      "Batch number:  47\n",
      "Active Features: 98.84%\n",
      "Decoder Weight Norm (Mean): 0.2062641829252243\n",
      "MSE Loss: 0.5141, L1 Loss: 0.01*11.0392\n",
      "Batch number:  48\n",
      "Active Features: 98.89%\n",
      "Decoder Weight Norm (Mean): 0.20633065700531006\n",
      "MSE Loss: 0.5244, L1 Loss: 0.01*11.0892\n",
      "Batch number:  49\n",
      "Active Features: 98.98%\n",
      "Decoder Weight Norm (Mean): 0.2063971310853958\n",
      "MSE Loss: 0.5229, L1 Loss: 0.01*11.0375\n",
      "Batch number:  50\n",
      "Active Features: 98.77%\n",
      "Decoder Weight Norm (Mean): 0.20646363496780396\n",
      "MSE Loss: 0.5215, L1 Loss: 0.01*11.0654\n",
      "Batch number:  51\n",
      "Active Features: 99.21%\n",
      "Decoder Weight Norm (Mean): 0.20653055608272552\n",
      "MSE Loss: 0.5228, L1 Loss: 0.01*11.0437\n",
      "Batch number:  52\n",
      "Active Features: 98.88%\n",
      "Decoder Weight Norm (Mean): 0.20659716427326202\n",
      "MSE Loss: 0.5234, L1 Loss: 0.01*11.2537\n",
      "Batch number:  53\n",
      "Active Features: 98.77%\n",
      "Decoder Weight Norm (Mean): 0.20666401088237762\n",
      "MSE Loss: 0.5207, L1 Loss: 0.01*11.1298\n",
      "Batch number:  54\n",
      "Active Features: 99.11%\n",
      "Decoder Weight Norm (Mean): 0.20673102140426636\n",
      "MSE Loss: 0.5136, L1 Loss: 0.01*11.0825\n",
      "Batch number:  55\n",
      "Active Features: 98.78%\n",
      "Decoder Weight Norm (Mean): 0.20679812133312225\n",
      "MSE Loss: 0.5143, L1 Loss: 0.01*11.1633\n",
      "Batch number:  56\n",
      "Active Features: 98.64%\n",
      "Decoder Weight Norm (Mean): 0.20686501264572144\n",
      "MSE Loss: 0.5131, L1 Loss: 0.01*11.1704\n",
      "Batch number:  57\n",
      "Active Features: 98.65%\n",
      "Decoder Weight Norm (Mean): 0.2069317102432251\n",
      "MSE Loss: 0.5210, L1 Loss: 0.01*11.1374\n",
      "Batch number:  58\n",
      "Active Features: 99.00%\n",
      "Decoder Weight Norm (Mean): 0.20699891448020935\n",
      "MSE Loss: 0.5110, L1 Loss: 0.01*11.1584\n",
      "Batch number:  59\n",
      "Active Features: 98.81%\n",
      "Decoder Weight Norm (Mean): 0.20706655085086823\n",
      "MSE Loss: 0.5152, L1 Loss: 0.01*11.2155\n",
      "Batch number:  60\n",
      "Active Features: 98.78%\n",
      "Decoder Weight Norm (Mean): 0.20713473856449127\n",
      "MSE Loss: 0.5166, L1 Loss: 0.01*11.3424\n",
      "Batch number:  61\n",
      "Active Features: 98.89%\n",
      "Decoder Weight Norm (Mean): 0.20720277726650238\n",
      "MSE Loss: 0.5147, L1 Loss: 0.01*11.1850\n",
      "Batch number:  62\n",
      "Active Features: 98.87%\n",
      "Decoder Weight Norm (Mean): 0.20727089047431946\n",
      "MSE Loss: 0.5121, L1 Loss: 0.01*11.1519\n",
      "Batch number:  63\n",
      "Active Features: 99.00%\n",
      "Decoder Weight Norm (Mean): 0.207339346408844\n",
      "MSE Loss: 0.5203, L1 Loss: 0.01*11.1684\n",
      "Batch number:  64\n",
      "Active Features: 99.11%\n",
      "Decoder Weight Norm (Mean): 0.2074083387851715\n",
      "MSE Loss: 0.5132, L1 Loss: 0.01*11.0889\n",
      "Batch number:  65\n",
      "Active Features: 98.64%\n",
      "Decoder Weight Norm (Mean): 0.20747748017311096\n",
      "MSE Loss: 0.5161, L1 Loss: 0.01*11.1662\n",
      "Batch number:  66\n",
      "Active Features: 98.65%\n",
      "Decoder Weight Norm (Mean): 0.2075473517179489\n",
      "MSE Loss: 0.5099, L1 Loss: 0.01*11.1360\n",
      "Batch number:  67\n",
      "Active Features: 98.50%\n",
      "Decoder Weight Norm (Mean): 0.2076171338558197\n",
      "MSE Loss: 0.5112, L1 Loss: 0.01*11.1910\n",
      "Batch number:  68\n",
      "Active Features: 98.84%\n",
      "Decoder Weight Norm (Mean): 0.20768730342388153\n",
      "MSE Loss: 0.5096, L1 Loss: 0.01*11.0593\n",
      "Batch number:  69\n",
      "Active Features: 98.68%\n",
      "Decoder Weight Norm (Mean): 0.207757368683815\n",
      "MSE Loss: 0.5101, L1 Loss: 0.01*11.2618\n",
      "Batch number:  70\n",
      "Active Features: 98.80%\n",
      "Decoder Weight Norm (Mean): 0.2078273445367813\n",
      "MSE Loss: 0.5118, L1 Loss: 0.01*11.1068\n",
      "Batch number:  71\n",
      "Active Features: 98.74%\n",
      "Decoder Weight Norm (Mean): 0.20789730548858643\n",
      "MSE Loss: 0.5072, L1 Loss: 0.01*11.1923\n",
      "Batch number:  72\n",
      "Active Features: 98.67%\n",
      "Decoder Weight Norm (Mean): 0.20796723663806915\n",
      "MSE Loss: 0.5151, L1 Loss: 0.01*11.3822\n",
      "Batch number:  73\n",
      "Active Features: 98.69%\n",
      "Decoder Weight Norm (Mean): 0.20803715288639069\n",
      "MSE Loss: 0.5078, L1 Loss: 0.01*11.2048\n",
      "Batch number:  74\n",
      "Active Features: 98.28%\n",
      "Decoder Weight Norm (Mean): 0.20810680091381073\n",
      "MSE Loss: 0.5063, L1 Loss: 0.01*11.3837\n",
      "Batch number:  75\n",
      "Active Features: 98.78%\n",
      "Decoder Weight Norm (Mean): 0.20817627012729645\n",
      "MSE Loss: 0.5051, L1 Loss: 0.01*11.1992\n",
      "Batch number:  76\n",
      "Active Features: 99.03%\n",
      "Decoder Weight Norm (Mean): 0.20824530720710754\n",
      "MSE Loss: 0.5062, L1 Loss: 0.01*11.2973\n",
      "Batch number:  77\n",
      "Active Features: 98.77%\n",
      "Decoder Weight Norm (Mean): 0.20831429958343506\n",
      "MSE Loss: 0.5066, L1 Loss: 0.01*11.2330\n",
      "Batch number:  78\n",
      "Active Features: 98.66%\n",
      "Decoder Weight Norm (Mean): 0.20838309824466705\n",
      "MSE Loss: 0.5078, L1 Loss: 0.01*11.3811\n",
      "Batch number:  79\n",
      "Active Features: 98.42%\n",
      "Decoder Weight Norm (Mean): 0.20845185220241547\n",
      "MSE Loss: 0.5040, L1 Loss: 0.01*11.3869\n",
      "Batch number:  80\n",
      "Active Features: 98.22%\n",
      "Decoder Weight Norm (Mean): 0.20852065086364746\n",
      "MSE Loss: 0.5065, L1 Loss: 0.01*11.3383\n",
      "Batch number:  81\n",
      "Active Features: 98.53%\n",
      "Decoder Weight Norm (Mean): 0.20858949422836304\n",
      "MSE Loss: 0.5003, L1 Loss: 0.01*11.3477\n",
      "Batch number:  82\n",
      "Active Features: 98.58%\n",
      "Decoder Weight Norm (Mean): 0.20865868031978607\n",
      "MSE Loss: 0.5054, L1 Loss: 0.01*11.2143\n",
      "Batch number:  83\n",
      "Active Features: 98.98%\n",
      "Decoder Weight Norm (Mean): 0.2087278962135315\n",
      "MSE Loss: 0.5161, L1 Loss: 0.01*11.3131\n",
      "Batch number:  84\n",
      "Active Features: 98.41%\n",
      "Decoder Weight Norm (Mean): 0.20879721641540527\n",
      "MSE Loss: 0.4919, L1 Loss: 0.01*11.1554\n",
      "Batch number:  85\n",
      "Active Features: 98.45%\n",
      "Decoder Weight Norm (Mean): 0.20886681973934174\n",
      "MSE Loss: 0.5004, L1 Loss: 0.01*11.3505\n",
      "Batch number:  86\n",
      "Active Features: 97.94%\n",
      "Decoder Weight Norm (Mean): 0.20893631875514984\n",
      "MSE Loss: 0.4979, L1 Loss: 0.01*11.3046\n",
      "Batch number:  87\n",
      "Active Features: 98.15%\n",
      "Decoder Weight Norm (Mean): 0.20900633931159973\n",
      "MSE Loss: 0.4963, L1 Loss: 0.01*11.3833\n",
      "Batch number:  88\n",
      "Active Features: 98.21%\n",
      "Decoder Weight Norm (Mean): 0.20907674729824066\n",
      "MSE Loss: 0.5009, L1 Loss: 0.01*11.2684\n",
      "Batch number:  89\n",
      "Active Features: 98.24%\n",
      "Decoder Weight Norm (Mean): 0.20914752781391144\n",
      "MSE Loss: 0.4951, L1 Loss: 0.01*11.1487\n",
      "Batch number:  90\n",
      "Active Features: 98.78%\n",
      "Decoder Weight Norm (Mean): 0.2092183232307434\n",
      "MSE Loss: 0.5048, L1 Loss: 0.01*11.3505\n",
      "Batch number:  91\n",
      "Active Features: 98.11%\n",
      "Decoder Weight Norm (Mean): 0.20928901433944702\n",
      "MSE Loss: 0.4920, L1 Loss: 0.01*11.3513\n",
      "Batch number:  92\n",
      "Active Features: 98.03%\n",
      "Decoder Weight Norm (Mean): 0.20936022698879242\n",
      "MSE Loss: 0.4975, L1 Loss: 0.01*11.4548\n",
      "Batch number:  93\n",
      "Active Features: 98.05%\n",
      "Decoder Weight Norm (Mean): 0.20943190157413483\n",
      "MSE Loss: 0.4955, L1 Loss: 0.01*11.3571\n",
      "Batch number:  94\n",
      "Active Features: 97.99%\n",
      "Decoder Weight Norm (Mean): 0.20950378477573395\n",
      "MSE Loss: 0.5001, L1 Loss: 0.01*11.3811\n",
      "Batch number:  95\n",
      "Active Features: 97.58%\n",
      "Decoder Weight Norm (Mean): 0.20957611501216888\n",
      "MSE Loss: 0.4976, L1 Loss: 0.01*11.4667\n",
      "Batch number:  96\n",
      "Active Features: 98.00%\n",
      "Decoder Weight Norm (Mean): 0.20964892208576202\n",
      "MSE Loss: 0.5005, L1 Loss: 0.01*11.5503\n",
      "Batch number:  97\n",
      "Active Features: 98.31%\n",
      "Decoder Weight Norm (Mean): 0.20972196757793427\n",
      "MSE Loss: 0.4983, L1 Loss: 0.01*11.4383\n",
      "Batch number:  98\n",
      "Active Features: 98.22%\n",
      "Decoder Weight Norm (Mean): 0.20979496836662292\n",
      "MSE Loss: 0.5007, L1 Loss: 0.01*11.4271\n",
      "Batch number:  99\n",
      "Active Features: 98.19%\n",
      "Decoder Weight Norm (Mean): 0.20986762642860413\n",
      "MSE Loss: 0.4998, L1 Loss: 0.01*11.3557\n",
      "Batch number:  100\n",
      "Active Features: 98.35%\n",
      "Decoder Weight Norm (Mean): 0.20994022488594055\n",
      "MSE Loss: 0.5029, L1 Loss: 0.01*11.4576\n",
      "Batch number:  101\n",
      "Active Features: 97.97%\n",
      "Decoder Weight Norm (Mean): 0.2100127637386322\n",
      "MSE Loss: 0.4955, L1 Loss: 0.01*11.4235\n",
      "Batch number:  102\n",
      "Active Features: 98.13%\n",
      "Decoder Weight Norm (Mean): 0.21008510887622833\n",
      "MSE Loss: 0.4934, L1 Loss: 0.01*11.2925\n",
      "Batch number:  103\n",
      "Active Features: 97.97%\n",
      "Decoder Weight Norm (Mean): 0.21015717089176178\n",
      "MSE Loss: 0.4911, L1 Loss: 0.01*11.5058\n",
      "Batch number:  104\n",
      "Active Features: 97.84%\n",
      "Decoder Weight Norm (Mean): 0.2102293223142624\n",
      "MSE Loss: 0.4984, L1 Loss: 0.01*11.4474\n",
      "Batch number:  105\n",
      "Active Features: 98.12%\n",
      "Decoder Weight Norm (Mean): 0.2103014588356018\n",
      "MSE Loss: 0.4897, L1 Loss: 0.01*11.3221\n",
      "Batch number:  106\n",
      "Active Features: 97.78%\n",
      "Decoder Weight Norm (Mean): 0.2103736698627472\n",
      "MSE Loss: 0.4911, L1 Loss: 0.01*11.5865\n",
      "Batch number:  107\n",
      "Active Features: 97.80%\n",
      "Decoder Weight Norm (Mean): 0.21044646203517914\n",
      "MSE Loss: 0.4945, L1 Loss: 0.01*11.3560\n",
      "Batch number:  108\n",
      "Active Features: 97.48%\n",
      "Decoder Weight Norm (Mean): 0.2105194330215454\n",
      "MSE Loss: 0.4871, L1 Loss: 0.01*11.4825\n",
      "Batch number:  109\n",
      "Active Features: 98.00%\n",
      "Decoder Weight Norm (Mean): 0.21059247851371765\n",
      "MSE Loss: 0.4934, L1 Loss: 0.01*11.4737\n",
      "Batch number:  110\n",
      "Active Features: 98.20%\n",
      "Decoder Weight Norm (Mean): 0.21066537499427795\n",
      "MSE Loss: 0.5067, L1 Loss: 0.01*11.4229\n",
      "Batch number:  111\n",
      "Active Features: 97.92%\n",
      "Decoder Weight Norm (Mean): 0.21073788404464722\n",
      "MSE Loss: 0.4909, L1 Loss: 0.01*11.5976\n",
      "Batch number:  112\n",
      "Active Features: 97.81%\n",
      "Decoder Weight Norm (Mean): 0.21081046760082245\n",
      "MSE Loss: 0.4898, L1 Loss: 0.01*11.4288\n",
      "Batch number:  113\n",
      "Active Features: 97.78%\n",
      "Decoder Weight Norm (Mean): 0.21088294684886932\n",
      "MSE Loss: 0.4959, L1 Loss: 0.01*11.5433\n",
      "Batch number:  114\n",
      "Active Features: 98.05%\n",
      "Decoder Weight Norm (Mean): 0.21095521748065948\n",
      "MSE Loss: 0.4971, L1 Loss: 0.01*11.3099\n",
      "Batch number:  115\n",
      "Active Features: 97.48%\n",
      "Decoder Weight Norm (Mean): 0.2110273391008377\n",
      "MSE Loss: 0.4880, L1 Loss: 0.01*11.5478\n",
      "Batch number:  116\n",
      "Active Features: 97.58%\n",
      "Decoder Weight Norm (Mean): 0.2110995501279831\n",
      "MSE Loss: 0.4851, L1 Loss: 0.01*11.5247\n",
      "Batch number:  117\n",
      "Active Features: 97.82%\n",
      "Decoder Weight Norm (Mean): 0.2111719399690628\n",
      "MSE Loss: 0.4941, L1 Loss: 0.01*11.6584\n",
      "Batch number:  118\n",
      "Active Features: 97.54%\n",
      "Decoder Weight Norm (Mean): 0.21124418079853058\n",
      "MSE Loss: 0.4888, L1 Loss: 0.01*11.5028\n",
      "Batch number:  119\n",
      "Active Features: 97.68%\n",
      "Decoder Weight Norm (Mean): 0.2113165408372879\n",
      "MSE Loss: 0.4928, L1 Loss: 0.01*11.5490\n",
      "Batch number:  120\n",
      "Active Features: 97.36%\n",
      "Decoder Weight Norm (Mean): 0.21138891577720642\n",
      "MSE Loss: 0.4891, L1 Loss: 0.01*11.5926\n",
      "Batch number:  121\n",
      "Active Features: 97.70%\n",
      "Decoder Weight Norm (Mean): 0.2114613652229309\n",
      "MSE Loss: 0.4929, L1 Loss: 0.01*11.6492\n",
      "Batch number:  122\n",
      "Active Features: 97.77%\n",
      "Decoder Weight Norm (Mean): 0.211534321308136\n",
      "MSE Loss: 0.4843, L1 Loss: 0.01*11.3834\n",
      "Batch number:  123\n",
      "Active Features: 97.86%\n",
      "Decoder Weight Norm (Mean): 0.21160706877708435\n",
      "MSE Loss: 0.4890, L1 Loss: 0.01*11.6811\n",
      "Batch number:  124\n",
      "Active Features: 97.71%\n",
      "Decoder Weight Norm (Mean): 0.21167965233325958\n",
      "MSE Loss: 0.4874, L1 Loss: 0.01*11.5785\n",
      "Batch number:  125\n",
      "Active Features: 97.12%\n",
      "Decoder Weight Norm (Mean): 0.2117525041103363\n",
      "MSE Loss: 0.4795, L1 Loss: 0.01*11.7033\n",
      "Batch number:  126\n",
      "Active Features: 97.17%\n",
      "Decoder Weight Norm (Mean): 0.21182548999786377\n",
      "MSE Loss: 0.4809, L1 Loss: 0.01*11.5557\n",
      "Batch number:  127\n",
      "Active Features: 97.72%\n",
      "Decoder Weight Norm (Mean): 0.2118985801935196\n",
      "MSE Loss: 0.4896, L1 Loss: 0.01*11.5329\n",
      "Epoch [3/8], Loss: 80.1761 ----------------------------\n",
      "Batch number:  0\n",
      "Active Features: 97.45%\n",
      "Decoder Weight Norm (Mean): 0.21197174489498138\n",
      "MSE Loss: 0.4870, L1 Loss: 0.01*11.6552\n",
      "Batch number:  1\n",
      "Active Features: 97.19%\n",
      "Decoder Weight Norm (Mean): 0.212044820189476\n",
      "MSE Loss: 0.4767, L1 Loss: 0.01*11.6181\n",
      "Batch number:  2\n",
      "Active Features: 97.73%\n",
      "Decoder Weight Norm (Mean): 0.21211816370487213\n",
      "MSE Loss: 0.4892, L1 Loss: 0.01*11.5953\n",
      "Batch number:  3\n",
      "Active Features: 97.20%\n",
      "Decoder Weight Norm (Mean): 0.2121921330690384\n",
      "MSE Loss: 0.4814, L1 Loss: 0.01*11.6899\n",
      "Batch number:  4\n",
      "Active Features: 96.81%\n",
      "Decoder Weight Norm (Mean): 0.2122662514448166\n",
      "MSE Loss: 0.4822, L1 Loss: 0.01*11.6440\n",
      "Batch number:  5\n",
      "Active Features: 97.12%\n",
      "Decoder Weight Norm (Mean): 0.21234066784381866\n",
      "MSE Loss: 0.4866, L1 Loss: 0.01*11.6712\n",
      "Batch number:  6\n",
      "Active Features: 97.20%\n",
      "Decoder Weight Norm (Mean): 0.21241524815559387\n",
      "MSE Loss: 0.4826, L1 Loss: 0.01*11.4875\n",
      "Batch number:  7\n",
      "Active Features: 97.02%\n",
      "Decoder Weight Norm (Mean): 0.21248966455459595\n",
      "MSE Loss: 0.4871, L1 Loss: 0.01*11.7737\n",
      "Batch number:  8\n",
      "Active Features: 97.33%\n",
      "Decoder Weight Norm (Mean): 0.2125643491744995\n",
      "MSE Loss: 0.4828, L1 Loss: 0.01*11.6110\n",
      "Batch number:  9\n",
      "Active Features: 96.95%\n",
      "Decoder Weight Norm (Mean): 0.21263925731182098\n",
      "MSE Loss: 0.4743, L1 Loss: 0.01*11.7246\n",
      "Batch number:  10\n",
      "Active Features: 96.94%\n",
      "Decoder Weight Norm (Mean): 0.21271386742591858\n",
      "MSE Loss: 0.4825, L1 Loss: 0.01*11.5952\n",
      "Batch number:  11\n",
      "Active Features: 96.87%\n",
      "Decoder Weight Norm (Mean): 0.21278861165046692\n",
      "MSE Loss: 0.4857, L1 Loss: 0.01*11.7530\n",
      "Batch number:  12\n",
      "Active Features: 96.54%\n",
      "Decoder Weight Norm (Mean): 0.21286356449127197\n",
      "MSE Loss: 0.4814, L1 Loss: 0.01*11.7362\n",
      "Batch number:  13\n",
      "Active Features: 96.65%\n",
      "Decoder Weight Norm (Mean): 0.2129390388727188\n",
      "MSE Loss: 0.4737, L1 Loss: 0.01*11.5410\n",
      "Batch number:  14\n",
      "Active Features: 97.01%\n",
      "Decoder Weight Norm (Mean): 0.2130143940448761\n",
      "MSE Loss: 0.4755, L1 Loss: 0.01*11.5789\n",
      "Batch number:  15\n",
      "Active Features: 97.09%\n",
      "Decoder Weight Norm (Mean): 0.2130894958972931\n",
      "MSE Loss: 0.4762, L1 Loss: 0.01*11.6238\n",
      "Batch number:  16\n",
      "Active Features: 97.21%\n",
      "Decoder Weight Norm (Mean): 0.21316425502300262\n",
      "MSE Loss: 0.4849, L1 Loss: 0.01*11.7733\n",
      "Batch number:  17\n",
      "Active Features: 96.36%\n",
      "Decoder Weight Norm (Mean): 0.21323905885219574\n",
      "MSE Loss: 0.4789, L1 Loss: 0.01*11.7565\n",
      "Batch number:  18\n",
      "Active Features: 97.10%\n",
      "Decoder Weight Norm (Mean): 0.21331378817558289\n",
      "MSE Loss: 0.4848, L1 Loss: 0.01*11.7593\n",
      "Batch number:  19\n",
      "Active Features: 97.19%\n",
      "Decoder Weight Norm (Mean): 0.2133888155221939\n",
      "MSE Loss: 0.4856, L1 Loss: 0.01*11.7524\n",
      "Batch number:  20\n",
      "Active Features: 96.54%\n",
      "Decoder Weight Norm (Mean): 0.21346381306648254\n",
      "MSE Loss: 0.4784, L1 Loss: 0.01*11.7083\n",
      "Batch number:  21\n",
      "Active Features: 96.84%\n",
      "Decoder Weight Norm (Mean): 0.21353885531425476\n",
      "MSE Loss: 0.4775, L1 Loss: 0.01*11.7061\n",
      "Batch number:  22\n",
      "Active Features: 97.00%\n",
      "Decoder Weight Norm (Mean): 0.2136145383119583\n",
      "MSE Loss: 0.4797, L1 Loss: 0.01*11.6302\n",
      "Batch number:  23\n",
      "Active Features: 96.75%\n",
      "Decoder Weight Norm (Mean): 0.2136906236410141\n",
      "MSE Loss: 0.4879, L1 Loss: 0.01*11.7811\n",
      "Batch number:  24\n",
      "Active Features: 96.41%\n",
      "Decoder Weight Norm (Mean): 0.21376684308052063\n",
      "MSE Loss: 0.4807, L1 Loss: 0.01*11.6567\n",
      "Batch number:  25\n",
      "Active Features: 96.38%\n",
      "Decoder Weight Norm (Mean): 0.2138427197933197\n",
      "MSE Loss: 0.4797, L1 Loss: 0.01*11.7567\n",
      "Batch number:  26\n",
      "Active Features: 96.44%\n",
      "Decoder Weight Norm (Mean): 0.2139187455177307\n",
      "MSE Loss: 0.4723, L1 Loss: 0.01*11.7322\n",
      "Batch number:  27\n",
      "Active Features: 96.72%\n",
      "Decoder Weight Norm (Mean): 0.21399462223052979\n",
      "MSE Loss: 0.4706, L1 Loss: 0.01*11.6619\n",
      "Batch number:  28\n",
      "Active Features: 96.43%\n",
      "Decoder Weight Norm (Mean): 0.21407000720500946\n",
      "MSE Loss: 0.4763, L1 Loss: 0.01*11.6631\n",
      "Batch number:  29\n",
      "Active Features: 97.62%\n",
      "Decoder Weight Norm (Mean): 0.21414567530155182\n",
      "MSE Loss: 0.4814, L1 Loss: 0.01*11.6285\n",
      "Batch number:  30\n",
      "Active Features: 96.45%\n",
      "Decoder Weight Norm (Mean): 0.2142208218574524\n",
      "MSE Loss: 0.4792, L1 Loss: 0.01*11.7047\n",
      "Batch number:  31\n",
      "Active Features: 96.70%\n",
      "Decoder Weight Norm (Mean): 0.21429604291915894\n",
      "MSE Loss: 0.4725, L1 Loss: 0.01*11.6097\n",
      "Batch number:  32\n",
      "Active Features: 96.28%\n",
      "Decoder Weight Norm (Mean): 0.21437059342861176\n",
      "MSE Loss: 0.4793, L1 Loss: 0.01*11.9420\n",
      "Batch number:  33\n",
      "Active Features: 95.98%\n",
      "Decoder Weight Norm (Mean): 0.21444526314735413\n",
      "MSE Loss: 0.4739, L1 Loss: 0.01*11.7789\n",
      "Batch number:  34\n",
      "Active Features: 95.85%\n",
      "Decoder Weight Norm (Mean): 0.21451976895332336\n",
      "MSE Loss: 0.4667, L1 Loss: 0.01*11.8771\n",
      "Batch number:  35\n",
      "Active Features: 96.64%\n",
      "Decoder Weight Norm (Mean): 0.2145942747592926\n",
      "MSE Loss: 0.4707, L1 Loss: 0.01*11.7847\n",
      "Batch number:  36\n",
      "Active Features: 96.34%\n",
      "Decoder Weight Norm (Mean): 0.21466833353042603\n",
      "MSE Loss: 0.4729, L1 Loss: 0.01*11.8746\n",
      "Batch number:  37\n",
      "Active Features: 96.16%\n",
      "Decoder Weight Norm (Mean): 0.2147424966096878\n",
      "MSE Loss: 0.4766, L1 Loss: 0.01*11.7613\n",
      "Batch number:  38\n",
      "Active Features: 95.95%\n",
      "Decoder Weight Norm (Mean): 0.21481718122959137\n",
      "MSE Loss: 0.4722, L1 Loss: 0.01*11.7304\n",
      "Batch number:  39\n",
      "Active Features: 96.21%\n",
      "Decoder Weight Norm (Mean): 0.21489217877388\n",
      "MSE Loss: 0.4744, L1 Loss: 0.01*11.7528\n",
      "Batch number:  40\n",
      "Active Features: 95.99%\n",
      "Decoder Weight Norm (Mean): 0.21496757864952087\n",
      "MSE Loss: 0.4673, L1 Loss: 0.01*11.7873\n",
      "Batch number:  41\n",
      "Active Features: 96.05%\n",
      "Decoder Weight Norm (Mean): 0.21504266560077667\n",
      "MSE Loss: 0.4677, L1 Loss: 0.01*11.7938\n",
      "Batch number:  42\n",
      "Active Features: 96.29%\n",
      "Decoder Weight Norm (Mean): 0.2151176631450653\n",
      "MSE Loss: 0.4663, L1 Loss: 0.01*11.8398\n",
      "Batch number:  43\n",
      "Active Features: 95.59%\n",
      "Decoder Weight Norm (Mean): 0.21519242227077484\n",
      "MSE Loss: 0.4708, L1 Loss: 0.01*11.9563\n",
      "Batch number:  44\n",
      "Active Features: 96.45%\n",
      "Decoder Weight Norm (Mean): 0.21526752412319183\n",
      "MSE Loss: 0.4690, L1 Loss: 0.01*11.8246\n",
      "Batch number:  45\n",
      "Active Features: 96.77%\n",
      "Decoder Weight Norm (Mean): 0.2153427004814148\n",
      "MSE Loss: 0.4667, L1 Loss: 0.01*11.9258\n",
      "Batch number:  46\n",
      "Active Features: 96.21%\n",
      "Decoder Weight Norm (Mean): 0.21541744470596313\n",
      "MSE Loss: 0.4726, L1 Loss: 0.01*11.7696\n",
      "Batch number:  47\n",
      "Active Features: 95.78%\n",
      "Decoder Weight Norm (Mean): 0.21549224853515625\n",
      "MSE Loss: 0.4570, L1 Loss: 0.01*11.8306\n",
      "Batch number:  48\n",
      "Active Features: 95.93%\n",
      "Decoder Weight Norm (Mean): 0.21556706726551056\n",
      "MSE Loss: 0.4692, L1 Loss: 0.01*11.9111\n",
      "Batch number:  49\n",
      "Active Features: 96.19%\n",
      "Decoder Weight Norm (Mean): 0.21564199030399323\n",
      "MSE Loss: 0.4712, L1 Loss: 0.01*11.8144\n",
      "Batch number:  50\n",
      "Active Features: 95.74%\n",
      "Decoder Weight Norm (Mean): 0.2157174050807953\n",
      "MSE Loss: 0.4652, L1 Loss: 0.01*11.6951\n",
      "Batch number:  51\n",
      "Active Features: 96.33%\n",
      "Decoder Weight Norm (Mean): 0.21579279005527496\n",
      "MSE Loss: 0.4722, L1 Loss: 0.01*11.8180\n",
      "Batch number:  52\n",
      "Active Features: 95.91%\n",
      "Decoder Weight Norm (Mean): 0.21586786210536957\n",
      "MSE Loss: 0.4615, L1 Loss: 0.01*11.9352\n",
      "Batch number:  53\n",
      "Active Features: 96.23%\n",
      "Decoder Weight Norm (Mean): 0.2159428596496582\n",
      "MSE Loss: 0.4688, L1 Loss: 0.01*11.9672\n",
      "Batch number:  54\n",
      "Active Features: 95.84%\n",
      "Decoder Weight Norm (Mean): 0.2160179167985916\n",
      "MSE Loss: 0.4652, L1 Loss: 0.01*11.7113\n",
      "Batch number:  55\n",
      "Active Features: 95.70%\n",
      "Decoder Weight Norm (Mean): 0.2160928100347519\n",
      "MSE Loss: 0.4653, L1 Loss: 0.01*11.9337\n",
      "Batch number:  56\n",
      "Active Features: 95.78%\n",
      "Decoder Weight Norm (Mean): 0.21616747975349426\n",
      "MSE Loss: 0.4630, L1 Loss: 0.01*11.8671\n",
      "Batch number:  57\n",
      "Active Features: 95.36%\n",
      "Decoder Weight Norm (Mean): 0.21624208986759186\n",
      "MSE Loss: 0.4765, L1 Loss: 0.01*11.9899\n",
      "Batch number:  58\n",
      "Active Features: 95.62%\n",
      "Decoder Weight Norm (Mean): 0.216317281126976\n",
      "MSE Loss: 0.4625, L1 Loss: 0.01*11.8874\n",
      "Batch number:  59\n",
      "Active Features: 95.69%\n",
      "Decoder Weight Norm (Mean): 0.2163926661014557\n",
      "MSE Loss: 0.4659, L1 Loss: 0.01*12.0336\n",
      "Batch number:  60\n",
      "Active Features: 96.02%\n",
      "Decoder Weight Norm (Mean): 0.21646776795387268\n",
      "MSE Loss: 0.4635, L1 Loss: 0.01*12.0316\n",
      "Batch number:  61\n",
      "Active Features: 95.28%\n",
      "Decoder Weight Norm (Mean): 0.21654242277145386\n",
      "MSE Loss: 0.4591, L1 Loss: 0.01*11.8950\n",
      "Batch number:  62\n",
      "Active Features: 95.78%\n",
      "Decoder Weight Norm (Mean): 0.21661707758903503\n",
      "MSE Loss: 0.4642, L1 Loss: 0.01*11.8816\n",
      "Batch number:  63\n",
      "Active Features: 95.48%\n",
      "Decoder Weight Norm (Mean): 0.21669183671474457\n",
      "MSE Loss: 0.4693, L1 Loss: 0.01*11.8335\n",
      "Batch number:  64\n",
      "Active Features: 96.78%\n",
      "Decoder Weight Norm (Mean): 0.216767355799675\n",
      "MSE Loss: 0.4649, L1 Loss: 0.01*11.8239\n",
      "Batch number:  65\n",
      "Active Features: 95.22%\n",
      "Decoder Weight Norm (Mean): 0.21684247255325317\n",
      "MSE Loss: 0.4644, L1 Loss: 0.01*11.8114\n",
      "Batch number:  66\n",
      "Active Features: 95.23%\n",
      "Decoder Weight Norm (Mean): 0.2169177234172821\n",
      "MSE Loss: 0.4566, L1 Loss: 0.01*11.8833\n",
      "Batch number:  67\n",
      "Active Features: 95.14%\n",
      "Decoder Weight Norm (Mean): 0.21699310839176178\n",
      "MSE Loss: 0.4586, L1 Loss: 0.01*11.9041\n",
      "Batch number:  68\n",
      "Active Features: 95.36%\n",
      "Decoder Weight Norm (Mean): 0.21706874668598175\n",
      "MSE Loss: 0.4656, L1 Loss: 0.01*11.8628\n",
      "Batch number:  69\n",
      "Active Features: 95.64%\n",
      "Decoder Weight Norm (Mean): 0.21714423596858978\n",
      "MSE Loss: 0.4615, L1 Loss: 0.01*11.8145\n",
      "Batch number:  70\n",
      "Active Features: 95.99%\n",
      "Decoder Weight Norm (Mean): 0.21721991896629333\n",
      "MSE Loss: 0.4582, L1 Loss: 0.01*11.8470\n",
      "Batch number:  71\n",
      "Active Features: 94.87%\n",
      "Decoder Weight Norm (Mean): 0.21729545295238495\n",
      "MSE Loss: 0.4563, L1 Loss: 0.01*11.8898\n",
      "Batch number:  72\n",
      "Active Features: 95.20%\n",
      "Decoder Weight Norm (Mean): 0.2173711657524109\n",
      "MSE Loss: 0.4675, L1 Loss: 0.01*11.9698\n",
      "Batch number:  73\n",
      "Active Features: 94.77%\n",
      "Decoder Weight Norm (Mean): 0.21744726598262787\n",
      "MSE Loss: 0.4598, L1 Loss: 0.01*11.8600\n",
      "Batch number:  74\n",
      "Active Features: 95.04%\n",
      "Decoder Weight Norm (Mean): 0.21752338111400604\n",
      "MSE Loss: 0.4579, L1 Loss: 0.01*12.0812\n",
      "Batch number:  75\n",
      "Active Features: 94.92%\n",
      "Decoder Weight Norm (Mean): 0.21759945154190063\n",
      "MSE Loss: 0.4617, L1 Loss: 0.01*11.9153\n",
      "Batch number:  76\n",
      "Active Features: 94.97%\n",
      "Decoder Weight Norm (Mean): 0.21767504513263702\n",
      "MSE Loss: 0.4566, L1 Loss: 0.01*11.9666\n",
      "Batch number:  77\n",
      "Active Features: 95.31%\n",
      "Decoder Weight Norm (Mean): 0.21775014698505402\n",
      "MSE Loss: 0.4557, L1 Loss: 0.01*11.8702\n",
      "Batch number:  78\n",
      "Active Features: 95.67%\n",
      "Decoder Weight Norm (Mean): 0.2178250253200531\n",
      "MSE Loss: 0.4640, L1 Loss: 0.01*12.0829\n",
      "Batch number:  79\n",
      "Active Features: 95.34%\n",
      "Decoder Weight Norm (Mean): 0.21790023148059845\n",
      "MSE Loss: 0.4602, L1 Loss: 0.01*12.0292\n",
      "Batch number:  80\n",
      "Active Features: 93.86%\n",
      "Decoder Weight Norm (Mean): 0.21797548234462738\n",
      "MSE Loss: 0.4557, L1 Loss: 0.01*12.0525\n",
      "Batch number:  81\n",
      "Active Features: 94.77%\n",
      "Decoder Weight Norm (Mean): 0.21805062890052795\n",
      "MSE Loss: 0.4583, L1 Loss: 0.01*12.0048\n",
      "Batch number:  82\n",
      "Active Features: 95.09%\n",
      "Decoder Weight Norm (Mean): 0.21812596917152405\n",
      "MSE Loss: 0.4600, L1 Loss: 0.01*11.9251\n",
      "Batch number:  83\n",
      "Active Features: 95.73%\n",
      "Decoder Weight Norm (Mean): 0.2182014286518097\n",
      "MSE Loss: 0.4640, L1 Loss: 0.01*11.9292\n",
      "Batch number:  84\n",
      "Active Features: 94.23%\n",
      "Decoder Weight Norm (Mean): 0.21827660501003265\n",
      "MSE Loss: 0.4540, L1 Loss: 0.01*11.9281\n",
      "Batch number:  85\n",
      "Active Features: 93.58%\n",
      "Decoder Weight Norm (Mean): 0.21835213899612427\n",
      "MSE Loss: 0.4468, L1 Loss: 0.01*12.0272\n",
      "Batch number:  86\n",
      "Active Features: 93.52%\n",
      "Decoder Weight Norm (Mean): 0.21842747926712036\n",
      "MSE Loss: 0.4499, L1 Loss: 0.01*12.0179\n",
      "Batch number:  87\n",
      "Active Features: 93.84%\n",
      "Decoder Weight Norm (Mean): 0.2185029685497284\n",
      "MSE Loss: 0.4475, L1 Loss: 0.01*12.0130\n",
      "Batch number:  88\n",
      "Active Features: 93.99%\n",
      "Decoder Weight Norm (Mean): 0.2185787558555603\n",
      "MSE Loss: 0.4575, L1 Loss: 0.01*11.9580\n",
      "Batch number:  89\n",
      "Active Features: 94.22%\n",
      "Decoder Weight Norm (Mean): 0.21865487098693848\n",
      "MSE Loss: 0.4531, L1 Loss: 0.01*11.9775\n",
      "Batch number:  90\n",
      "Active Features: 95.23%\n",
      "Decoder Weight Norm (Mean): 0.2187310755252838\n",
      "MSE Loss: 0.4570, L1 Loss: 0.01*11.9971\n",
      "Batch number:  91\n",
      "Active Features: 94.34%\n",
      "Decoder Weight Norm (Mean): 0.21880726516246796\n",
      "MSE Loss: 0.4518, L1 Loss: 0.01*12.0366\n",
      "Batch number:  92\n",
      "Active Features: 93.39%\n",
      "Decoder Weight Norm (Mean): 0.21888354420661926\n",
      "MSE Loss: 0.4559, L1 Loss: 0.01*12.0323\n",
      "Batch number:  93\n",
      "Active Features: 95.38%\n",
      "Decoder Weight Norm (Mean): 0.21896019577980042\n",
      "MSE Loss: 0.4500, L1 Loss: 0.01*11.9459\n",
      "Batch number:  94\n",
      "Active Features: 93.19%\n",
      "Decoder Weight Norm (Mean): 0.2190367430448532\n",
      "MSE Loss: 0.4538, L1 Loss: 0.01*12.0842\n",
      "Batch number:  95\n",
      "Active Features: 93.24%\n",
      "Decoder Weight Norm (Mean): 0.21911358833312988\n",
      "MSE Loss: 0.4576, L1 Loss: 0.01*12.1123\n",
      "Batch number:  96\n",
      "Active Features: 94.11%\n",
      "Decoder Weight Norm (Mean): 0.21919094026088715\n",
      "MSE Loss: 0.4562, L1 Loss: 0.01*12.1375\n",
      "Batch number:  97\n",
      "Active Features: 93.86%\n",
      "Decoder Weight Norm (Mean): 0.21926811337471008\n",
      "MSE Loss: 0.4451, L1 Loss: 0.01*11.9290\n",
      "Batch number:  98\n",
      "Active Features: 94.23%\n",
      "Decoder Weight Norm (Mean): 0.21934472024440765\n",
      "MSE Loss: 0.4543, L1 Loss: 0.01*12.1201\n",
      "Batch number:  99\n",
      "Active Features: 93.89%\n",
      "Decoder Weight Norm (Mean): 0.21942123770713806\n",
      "MSE Loss: 0.4456, L1 Loss: 0.01*11.8990\n",
      "Batch number:  100\n",
      "Active Features: 94.12%\n",
      "Decoder Weight Norm (Mean): 0.2194976508617401\n",
      "MSE Loss: 0.4573, L1 Loss: 0.01*12.0352\n",
      "Batch number:  101\n",
      "Active Features: 92.44%\n",
      "Decoder Weight Norm (Mean): 0.21957460045814514\n",
      "MSE Loss: 0.4480, L1 Loss: 0.01*12.0565\n",
      "Batch number:  102\n",
      "Active Features: 94.53%\n",
      "Decoder Weight Norm (Mean): 0.2196515053510666\n",
      "MSE Loss: 0.4484, L1 Loss: 0.01*11.8600\n",
      "Batch number:  103\n",
      "Active Features: 93.62%\n",
      "Decoder Weight Norm (Mean): 0.2197285145521164\n",
      "MSE Loss: 0.4490, L1 Loss: 0.01*12.1929\n",
      "Batch number:  104\n",
      "Active Features: 93.18%\n",
      "Decoder Weight Norm (Mean): 0.21980541944503784\n",
      "MSE Loss: 0.4521, L1 Loss: 0.01*12.0933\n",
      "Batch number:  105\n",
      "Active Features: 93.78%\n",
      "Decoder Weight Norm (Mean): 0.21988222002983093\n",
      "MSE Loss: 0.4482, L1 Loss: 0.01*11.9943\n",
      "Batch number:  106\n",
      "Active Features: 93.14%\n",
      "Decoder Weight Norm (Mean): 0.21995848417282104\n",
      "MSE Loss: 0.4485, L1 Loss: 0.01*12.0771\n",
      "Batch number:  107\n",
      "Active Features: 93.58%\n",
      "Decoder Weight Norm (Mean): 0.220034658908844\n",
      "MSE Loss: 0.4476, L1 Loss: 0.01*11.9858\n",
      "Batch number:  108\n",
      "Active Features: 93.16%\n",
      "Decoder Weight Norm (Mean): 0.22011080384254456\n",
      "MSE Loss: 0.4446, L1 Loss: 0.01*11.9912\n",
      "Batch number:  109\n",
      "Active Features: 94.03%\n",
      "Decoder Weight Norm (Mean): 0.2201869785785675\n",
      "MSE Loss: 0.4464, L1 Loss: 0.01*11.9821\n",
      "Batch number:  110\n",
      "Active Features: 94.07%\n",
      "Decoder Weight Norm (Mean): 0.22026310861110687\n",
      "MSE Loss: 0.4603, L1 Loss: 0.01*12.1551\n",
      "Batch number:  111\n",
      "Active Features: 93.75%\n",
      "Decoder Weight Norm (Mean): 0.22033876180648804\n",
      "MSE Loss: 0.4520, L1 Loss: 0.01*12.2347\n",
      "Batch number:  112\n",
      "Active Features: 93.91%\n",
      "Decoder Weight Norm (Mean): 0.22041459381580353\n",
      "MSE Loss: 0.4476, L1 Loss: 0.01*12.0952\n",
      "Batch number:  113\n",
      "Active Features: 93.66%\n",
      "Decoder Weight Norm (Mean): 0.22049066424369812\n",
      "MSE Loss: 0.4538, L1 Loss: 0.01*12.1858\n",
      "Batch number:  114\n",
      "Active Features: 94.49%\n",
      "Decoder Weight Norm (Mean): 0.22056683897972107\n",
      "MSE Loss: 0.4540, L1 Loss: 0.01*11.9740\n",
      "Batch number:  115\n",
      "Active Features: 93.62%\n",
      "Decoder Weight Norm (Mean): 0.22064317762851715\n",
      "MSE Loss: 0.4493, L1 Loss: 0.01*12.1807\n",
      "Batch number:  116\n",
      "Active Features: 92.92%\n",
      "Decoder Weight Norm (Mean): 0.220719575881958\n",
      "MSE Loss: 0.4378, L1 Loss: 0.01*11.9958\n",
      "Batch number:  117\n",
      "Active Features: 93.58%\n",
      "Decoder Weight Norm (Mean): 0.2207958996295929\n",
      "MSE Loss: 0.4498, L1 Loss: 0.01*12.1091\n",
      "Batch number:  118\n",
      "Active Features: 93.55%\n",
      "Decoder Weight Norm (Mean): 0.22087255120277405\n",
      "MSE Loss: 0.4513, L1 Loss: 0.01*12.0654\n",
      "Batch number:  119\n",
      "Active Features: 92.85%\n",
      "Decoder Weight Norm (Mean): 0.2209494560956955\n",
      "MSE Loss: 0.4508, L1 Loss: 0.01*12.1609\n",
      "Batch number:  120\n",
      "Active Features: 93.44%\n",
      "Decoder Weight Norm (Mean): 0.2210262566804886\n",
      "MSE Loss: 0.4450, L1 Loss: 0.01*12.0730\n",
      "Batch number:  121\n",
      "Active Features: 93.23%\n",
      "Decoder Weight Norm (Mean): 0.22110304236412048\n",
      "MSE Loss: 0.4528, L1 Loss: 0.01*12.3244\n",
      "Batch number:  122\n",
      "Active Features: 93.34%\n",
      "Decoder Weight Norm (Mean): 0.2211795151233673\n",
      "MSE Loss: 0.4446, L1 Loss: 0.01*12.0596\n",
      "Batch number:  123\n",
      "Active Features: 93.91%\n",
      "Decoder Weight Norm (Mean): 0.22125589847564697\n",
      "MSE Loss: 0.4494, L1 Loss: 0.01*12.2268\n",
      "Batch number:  124\n",
      "Active Features: 93.20%\n",
      "Decoder Weight Norm (Mean): 0.22133231163024902\n",
      "MSE Loss: 0.4484, L1 Loss: 0.01*12.1679\n",
      "Batch number:  125\n",
      "Active Features: 93.01%\n",
      "Decoder Weight Norm (Mean): 0.22140859067440033\n",
      "MSE Loss: 0.4420, L1 Loss: 0.01*12.2456\n",
      "Batch number:  126\n",
      "Active Features: 92.11%\n",
      "Decoder Weight Norm (Mean): 0.22148504853248596\n",
      "MSE Loss: 0.4370, L1 Loss: 0.01*12.0373\n",
      "Batch number:  127\n",
      "Active Features: 92.84%\n",
      "Decoder Weight Norm (Mean): 0.22156117856502533\n",
      "MSE Loss: 0.4477, L1 Loss: 0.01*12.0592\n",
      "Epoch [4/8], Loss: 74.5930 ----------------------------\n",
      "Batch number:  0\n",
      "Active Features: 93.63%\n",
      "Decoder Weight Norm (Mean): 0.22163739800453186\n",
      "MSE Loss: 0.4476, L1 Loss: 0.01*12.2286\n",
      "Batch number:  1\n",
      "Active Features: 92.56%\n",
      "Decoder Weight Norm (Mean): 0.22171342372894287\n",
      "MSE Loss: 0.4344, L1 Loss: 0.01*12.2139\n",
      "Batch number:  2\n",
      "Active Features: 92.78%\n",
      "Decoder Weight Norm (Mean): 0.2217894047498703\n",
      "MSE Loss: 0.4490, L1 Loss: 0.01*12.0884\n",
      "Batch number:  3\n",
      "Active Features: 92.59%\n",
      "Decoder Weight Norm (Mean): 0.2218659669160843\n",
      "MSE Loss: 0.4346, L1 Loss: 0.01*12.2882\n",
      "Batch number:  4\n",
      "Active Features: 91.55%\n",
      "Decoder Weight Norm (Mean): 0.22194181382656097\n",
      "MSE Loss: 0.4338, L1 Loss: 0.01*12.0831\n",
      "Batch number:  5\n",
      "Active Features: 91.87%\n",
      "Decoder Weight Norm (Mean): 0.2220178097486496\n",
      "MSE Loss: 0.4421, L1 Loss: 0.01*12.3015\n",
      "Batch number:  6\n",
      "Active Features: 92.94%\n",
      "Decoder Weight Norm (Mean): 0.22209401428699493\n",
      "MSE Loss: 0.4466, L1 Loss: 0.01*12.0319\n",
      "Batch number:  7\n",
      "Active Features: 92.90%\n",
      "Decoder Weight Norm (Mean): 0.22217027842998505\n",
      "MSE Loss: 0.4483, L1 Loss: 0.01*12.3879\n",
      "Batch number:  8\n",
      "Active Features: 93.46%\n",
      "Decoder Weight Norm (Mean): 0.22224633395671844\n",
      "MSE Loss: 0.4471, L1 Loss: 0.01*12.1337\n",
      "Batch number:  9\n",
      "Active Features: 92.18%\n",
      "Decoder Weight Norm (Mean): 0.22232235968112946\n",
      "MSE Loss: 0.4381, L1 Loss: 0.01*12.2275\n",
      "Batch number:  10\n",
      "Active Features: 91.95%\n",
      "Decoder Weight Norm (Mean): 0.2223980873823166\n",
      "MSE Loss: 0.4478, L1 Loss: 0.01*12.2178\n",
      "Batch number:  11\n",
      "Active Features: 91.81%\n",
      "Decoder Weight Norm (Mean): 0.22247357666492462\n",
      "MSE Loss: 0.4417, L1 Loss: 0.01*12.3209\n",
      "Batch number:  12\n",
      "Active Features: 91.71%\n",
      "Decoder Weight Norm (Mean): 0.22254902124404907\n",
      "MSE Loss: 0.4372, L1 Loss: 0.01*12.2661\n",
      "Batch number:  13\n",
      "Active Features: 92.98%\n",
      "Decoder Weight Norm (Mean): 0.2226245105266571\n",
      "MSE Loss: 0.4407, L1 Loss: 0.01*12.1147\n",
      "Batch number:  14\n",
      "Active Features: 91.39%\n",
      "Decoder Weight Norm (Mean): 0.22269989550113678\n",
      "MSE Loss: 0.4344, L1 Loss: 0.01*12.0754\n",
      "Batch number:  15\n",
      "Active Features: 92.64%\n",
      "Decoder Weight Norm (Mean): 0.22277499735355377\n",
      "MSE Loss: 0.4426, L1 Loss: 0.01*12.1807\n",
      "Batch number:  16\n",
      "Active Features: 92.22%\n",
      "Decoder Weight Norm (Mean): 0.22285063564777374\n",
      "MSE Loss: 0.4429, L1 Loss: 0.01*12.2975\n",
      "Batch number:  17\n",
      "Active Features: 92.38%\n",
      "Decoder Weight Norm (Mean): 0.22292611002922058\n",
      "MSE Loss: 0.4438, L1 Loss: 0.01*12.3447\n",
      "Batch number:  18\n",
      "Active Features: 92.05%\n",
      "Decoder Weight Norm (Mean): 0.22300148010253906\n",
      "MSE Loss: 0.4444, L1 Loss: 0.01*12.2499\n",
      "Batch number:  19\n",
      "Active Features: 92.03%\n",
      "Decoder Weight Norm (Mean): 0.22307704389095306\n",
      "MSE Loss: 0.4425, L1 Loss: 0.01*12.1716\n",
      "Batch number:  20\n",
      "Active Features: 91.98%\n",
      "Decoder Weight Norm (Mean): 0.22315238416194916\n",
      "MSE Loss: 0.4374, L1 Loss: 0.01*12.2629\n",
      "Batch number:  21\n",
      "Active Features: 92.64%\n",
      "Decoder Weight Norm (Mean): 0.2232275754213333\n",
      "MSE Loss: 0.4423, L1 Loss: 0.01*12.1331\n",
      "Batch number:  22\n",
      "Active Features: 91.56%\n",
      "Decoder Weight Norm (Mean): 0.22330297529697418\n",
      "MSE Loss: 0.4397, L1 Loss: 0.01*12.1302\n",
      "Batch number:  23\n",
      "Active Features: 91.88%\n",
      "Decoder Weight Norm (Mean): 0.22337861359119415\n",
      "MSE Loss: 0.4475, L1 Loss: 0.01*12.4802\n",
      "Batch number:  24\n",
      "Active Features: 91.47%\n",
      "Decoder Weight Norm (Mean): 0.22345438599586487\n",
      "MSE Loss: 0.4420, L1 Loss: 0.01*12.3323\n",
      "Batch number:  25\n",
      "Active Features: 90.98%\n",
      "Decoder Weight Norm (Mean): 0.22353026270866394\n",
      "MSE Loss: 0.4376, L1 Loss: 0.01*12.2026\n",
      "Batch number:  26\n",
      "Active Features: 91.82%\n",
      "Decoder Weight Norm (Mean): 0.22360648214817047\n",
      "MSE Loss: 0.4359, L1 Loss: 0.01*12.2716\n",
      "Batch number:  27\n",
      "Active Features: 90.78%\n",
      "Decoder Weight Norm (Mean): 0.22368250787258148\n",
      "MSE Loss: 0.4262, L1 Loss: 0.01*12.1619\n",
      "Batch number:  28\n",
      "Active Features: 90.93%\n",
      "Decoder Weight Norm (Mean): 0.2237582951784134\n",
      "MSE Loss: 0.4403, L1 Loss: 0.01*12.1806\n",
      "Batch number:  29\n",
      "Active Features: 92.84%\n",
      "Decoder Weight Norm (Mean): 0.22383446991443634\n",
      "MSE Loss: 0.4442, L1 Loss: 0.01*12.0541\n",
      "Batch number:  30\n",
      "Active Features: 91.16%\n",
      "Decoder Weight Norm (Mean): 0.2239101529121399\n",
      "MSE Loss: 0.4323, L1 Loss: 0.01*12.1838\n",
      "Batch number:  31\n",
      "Active Features: 91.24%\n",
      "Decoder Weight Norm (Mean): 0.22398510575294495\n",
      "MSE Loss: 0.4347, L1 Loss: 0.01*12.1442\n",
      "Batch number:  32\n",
      "Active Features: 91.39%\n",
      "Decoder Weight Norm (Mean): 0.22405976057052612\n",
      "MSE Loss: 0.4376, L1 Loss: 0.01*12.2865\n",
      "Batch number:  33\n",
      "Active Features: 91.64%\n",
      "Decoder Weight Norm (Mean): 0.22413448989391327\n",
      "MSE Loss: 0.4346, L1 Loss: 0.01*12.2314\n",
      "Batch number:  34\n",
      "Active Features: 90.84%\n",
      "Decoder Weight Norm (Mean): 0.22420886158943176\n",
      "MSE Loss: 0.4300, L1 Loss: 0.01*12.2832\n",
      "Batch number:  35\n",
      "Active Features: 91.61%\n",
      "Decoder Weight Norm (Mean): 0.22428344190120697\n",
      "MSE Loss: 0.4335, L1 Loss: 0.01*12.2314\n",
      "Batch number:  36\n",
      "Active Features: 91.81%\n",
      "Decoder Weight Norm (Mean): 0.2243577539920807\n",
      "MSE Loss: 0.4369, L1 Loss: 0.01*12.3492\n",
      "Batch number:  37\n",
      "Active Features: 90.75%\n",
      "Decoder Weight Norm (Mean): 0.22443217039108276\n",
      "MSE Loss: 0.4398, L1 Loss: 0.01*12.2659\n",
      "Batch number:  38\n",
      "Active Features: 91.50%\n",
      "Decoder Weight Norm (Mean): 0.22450648248195648\n",
      "MSE Loss: 0.4371, L1 Loss: 0.01*12.3275\n",
      "Batch number:  39\n",
      "Active Features: 91.42%\n",
      "Decoder Weight Norm (Mean): 0.22458100318908691\n",
      "MSE Loss: 0.4387, L1 Loss: 0.01*12.2337\n",
      "Batch number:  40\n",
      "Active Features: 90.48%\n",
      "Decoder Weight Norm (Mean): 0.2246556282043457\n",
      "MSE Loss: 0.4291, L1 Loss: 0.01*12.2035\n",
      "Batch number:  41\n",
      "Active Features: 91.48%\n",
      "Decoder Weight Norm (Mean): 0.22473016381263733\n",
      "MSE Loss: 0.4346, L1 Loss: 0.01*12.3184\n",
      "Batch number:  42\n",
      "Active Features: 91.22%\n",
      "Decoder Weight Norm (Mean): 0.22480449080467224\n",
      "MSE Loss: 0.4307, L1 Loss: 0.01*12.2316\n",
      "Batch number:  43\n",
      "Active Features: 91.53%\n",
      "Decoder Weight Norm (Mean): 0.22487856447696686\n",
      "MSE Loss: 0.4357, L1 Loss: 0.01*12.3279\n",
      "Batch number:  44\n",
      "Active Features: 92.14%\n",
      "Decoder Weight Norm (Mean): 0.22495253384113312\n",
      "MSE Loss: 0.4324, L1 Loss: 0.01*12.2103\n",
      "Batch number:  45\n",
      "Active Features: 91.92%\n",
      "Decoder Weight Norm (Mean): 0.22502650320529938\n",
      "MSE Loss: 0.4378, L1 Loss: 0.01*12.4004\n",
      "Batch number:  46\n",
      "Active Features: 90.83%\n",
      "Decoder Weight Norm (Mean): 0.22510045766830444\n",
      "MSE Loss: 0.4389, L1 Loss: 0.01*12.2439\n",
      "Batch number:  47\n",
      "Active Features: 90.25%\n",
      "Decoder Weight Norm (Mean): 0.2251749038696289\n",
      "MSE Loss: 0.4278, L1 Loss: 0.01*12.3493\n",
      "Batch number:  48\n",
      "Active Features: 90.58%\n",
      "Decoder Weight Norm (Mean): 0.22524960339069366\n",
      "MSE Loss: 0.4362, L1 Loss: 0.01*12.4674\n",
      "Batch number:  49\n",
      "Active Features: 90.48%\n",
      "Decoder Weight Norm (Mean): 0.22532464563846588\n",
      "MSE Loss: 0.4360, L1 Loss: 0.01*12.3129\n",
      "Batch number:  50\n",
      "Active Features: 90.34%\n",
      "Decoder Weight Norm (Mean): 0.22539997100830078\n",
      "MSE Loss: 0.4342, L1 Loss: 0.01*12.2606\n",
      "Batch number:  51\n",
      "Active Features: 91.82%\n",
      "Decoder Weight Norm (Mean): 0.2254752367734909\n",
      "MSE Loss: 0.4349, L1 Loss: 0.01*12.3212\n",
      "Batch number:  52\n",
      "Active Features: 90.19%\n",
      "Decoder Weight Norm (Mean): 0.2255503088235855\n",
      "MSE Loss: 0.4304, L1 Loss: 0.01*12.3713\n",
      "Batch number:  53\n",
      "Active Features: 91.83%\n",
      "Decoder Weight Norm (Mean): 0.22562548518180847\n",
      "MSE Loss: 0.4369, L1 Loss: 0.01*12.3938\n",
      "Batch number:  54\n",
      "Active Features: 90.56%\n",
      "Decoder Weight Norm (Mean): 0.22570070624351501\n",
      "MSE Loss: 0.4278, L1 Loss: 0.01*12.2471\n",
      "Batch number:  55\n",
      "Active Features: 90.64%\n",
      "Decoder Weight Norm (Mean): 0.2257758229970932\n",
      "MSE Loss: 0.4312, L1 Loss: 0.01*12.3408\n",
      "Batch number:  56\n",
      "Active Features: 89.53%\n",
      "Decoder Weight Norm (Mean): 0.22585101425647736\n",
      "MSE Loss: 0.4270, L1 Loss: 0.01*12.3830\n",
      "Batch number:  57\n",
      "Active Features: 90.24%\n",
      "Decoder Weight Norm (Mean): 0.225925475358963\n",
      "MSE Loss: 0.4330, L1 Loss: 0.01*12.3690\n",
      "Batch number:  58\n",
      "Active Features: 89.09%\n",
      "Decoder Weight Norm (Mean): 0.22599980235099792\n",
      "MSE Loss: 0.4264, L1 Loss: 0.01*12.3656\n",
      "Batch number:  59\n",
      "Active Features: 90.43%\n",
      "Decoder Weight Norm (Mean): 0.22607421875\n",
      "MSE Loss: 0.4307, L1 Loss: 0.01*12.4832\n",
      "Batch number:  60\n",
      "Active Features: 90.66%\n",
      "Decoder Weight Norm (Mean): 0.22614867985248566\n",
      "MSE Loss: 0.4270, L1 Loss: 0.01*12.4552\n",
      "Batch number:  61\n",
      "Active Features: 90.20%\n",
      "Decoder Weight Norm (Mean): 0.22622279822826385\n",
      "MSE Loss: 0.4310, L1 Loss: 0.01*12.3991\n",
      "Batch number:  62\n",
      "Active Features: 90.84%\n",
      "Decoder Weight Norm (Mean): 0.22629699110984802\n",
      "MSE Loss: 0.4297, L1 Loss: 0.01*12.2574\n",
      "Batch number:  63\n",
      "Active Features: 89.56%\n",
      "Decoder Weight Norm (Mean): 0.2263708859682083\n",
      "MSE Loss: 0.4310, L1 Loss: 0.01*12.3964\n",
      "Batch number:  64\n",
      "Active Features: 91.97%\n",
      "Decoder Weight Norm (Mean): 0.22644469141960144\n",
      "MSE Loss: 0.4349, L1 Loss: 0.01*12.2898\n",
      "Batch number:  65\n",
      "Active Features: 89.63%\n",
      "Decoder Weight Norm (Mean): 0.22651849687099457\n",
      "MSE Loss: 0.4273, L1 Loss: 0.01*12.2508\n",
      "Batch number:  66\n",
      "Active Features: 89.12%\n",
      "Decoder Weight Norm (Mean): 0.22659306228160858\n",
      "MSE Loss: 0.4241, L1 Loss: 0.01*12.2757\n",
      "Batch number:  67\n",
      "Active Features: 89.41%\n",
      "Decoder Weight Norm (Mean): 0.22666771709918976\n",
      "MSE Loss: 0.4309, L1 Loss: 0.01*12.3822\n",
      "Batch number:  68\n",
      "Active Features: 90.12%\n",
      "Decoder Weight Norm (Mean): 0.22674252092838287\n",
      "MSE Loss: 0.4264, L1 Loss: 0.01*12.1220\n",
      "Batch number:  69\n",
      "Active Features: 90.18%\n",
      "Decoder Weight Norm (Mean): 0.22681693732738495\n",
      "MSE Loss: 0.4270, L1 Loss: 0.01*12.2188\n",
      "Batch number:  70\n",
      "Active Features: 89.94%\n",
      "Decoder Weight Norm (Mean): 0.2268916666507721\n",
      "MSE Loss: 0.4278, L1 Loss: 0.01*12.2564\n",
      "Batch number:  71\n",
      "Active Features: 89.64%\n",
      "Decoder Weight Norm (Mean): 0.22696684300899506\n",
      "MSE Loss: 0.4290, L1 Loss: 0.01*12.3877\n",
      "Batch number:  72\n",
      "Active Features: 90.06%\n",
      "Decoder Weight Norm (Mean): 0.22704221308231354\n",
      "MSE Loss: 0.4327, L1 Loss: 0.01*12.3493\n",
      "Batch number:  73\n",
      "Active Features: 88.97%\n",
      "Decoder Weight Norm (Mean): 0.22711795568466187\n",
      "MSE Loss: 0.4250, L1 Loss: 0.01*12.2981\n",
      "Batch number:  74\n",
      "Active Features: 88.13%\n",
      "Decoder Weight Norm (Mean): 0.2271934300661087\n",
      "MSE Loss: 0.4155, L1 Loss: 0.01*12.4652\n",
      "Batch number:  75\n",
      "Active Features: 90.66%\n",
      "Decoder Weight Norm (Mean): 0.22726880013942719\n",
      "MSE Loss: 0.4254, L1 Loss: 0.01*12.3308\n",
      "Batch number:  76\n",
      "Active Features: 91.19%\n",
      "Decoder Weight Norm (Mean): 0.2273436039686203\n",
      "MSE Loss: 0.4263, L1 Loss: 0.01*12.3970\n",
      "Batch number:  77\n",
      "Active Features: 90.53%\n",
      "Decoder Weight Norm (Mean): 0.22741787135601044\n",
      "MSE Loss: 0.4247, L1 Loss: 0.01*12.3156\n",
      "Batch number:  78\n",
      "Active Features: 90.48%\n",
      "Decoder Weight Norm (Mean): 0.22749166190624237\n",
      "MSE Loss: 0.4234, L1 Loss: 0.01*12.3769\n",
      "Batch number:  79\n",
      "Active Features: 89.50%\n",
      "Decoder Weight Norm (Mean): 0.22756530344486237\n",
      "MSE Loss: 0.4296, L1 Loss: 0.01*12.5843\n",
      "Batch number:  80\n",
      "Active Features: 88.62%\n",
      "Decoder Weight Norm (Mean): 0.22763881087303162\n",
      "MSE Loss: 0.4274, L1 Loss: 0.01*12.4991\n",
      "Batch number:  81\n",
      "Active Features: 88.51%\n",
      "Decoder Weight Norm (Mean): 0.2277122437953949\n",
      "MSE Loss: 0.4230, L1 Loss: 0.01*12.2650\n",
      "Batch number:  82\n",
      "Active Features: 89.03%\n",
      "Decoder Weight Norm (Mean): 0.22778573632240295\n",
      "MSE Loss: 0.4272, L1 Loss: 0.01*12.4331\n",
      "Batch number:  83\n",
      "Active Features: 89.48%\n",
      "Decoder Weight Norm (Mean): 0.22785893082618713\n",
      "MSE Loss: 0.4303, L1 Loss: 0.01*12.3671\n",
      "Batch number:  84\n",
      "Active Features: 87.67%\n",
      "Decoder Weight Norm (Mean): 0.22793236374855042\n",
      "MSE Loss: 0.4106, L1 Loss: 0.01*12.2362\n",
      "Batch number:  85\n",
      "Active Features: 89.28%\n",
      "Decoder Weight Norm (Mean): 0.2280060052871704\n",
      "MSE Loss: 0.4208, L1 Loss: 0.01*12.4835\n",
      "Batch number:  86\n",
      "Active Features: 87.76%\n",
      "Decoder Weight Norm (Mean): 0.22807978093624115\n",
      "MSE Loss: 0.4161, L1 Loss: 0.01*12.4223\n",
      "Batch number:  87\n",
      "Active Features: 87.73%\n",
      "Decoder Weight Norm (Mean): 0.2281535416841507\n",
      "MSE Loss: 0.4166, L1 Loss: 0.01*12.4036\n",
      "Batch number:  88\n",
      "Active Features: 88.29%\n",
      "Decoder Weight Norm (Mean): 0.2282274067401886\n",
      "MSE Loss: 0.4217, L1 Loss: 0.01*12.3154\n",
      "Batch number:  89\n",
      "Active Features: 89.16%\n",
      "Decoder Weight Norm (Mean): 0.22830148041248322\n",
      "MSE Loss: 0.4232, L1 Loss: 0.01*12.4818\n",
      "Batch number:  90\n",
      "Active Features: 89.62%\n",
      "Decoder Weight Norm (Mean): 0.2283753603696823\n",
      "MSE Loss: 0.4237, L1 Loss: 0.01*12.3172\n",
      "Batch number:  91\n",
      "Active Features: 89.25%\n",
      "Decoder Weight Norm (Mean): 0.22844913601875305\n",
      "MSE Loss: 0.4225, L1 Loss: 0.01*12.3712\n",
      "Batch number:  92\n",
      "Active Features: 87.28%\n",
      "Decoder Weight Norm (Mean): 0.22852301597595215\n",
      "MSE Loss: 0.4234, L1 Loss: 0.01*12.5724\n",
      "Batch number:  93\n",
      "Active Features: 89.82%\n",
      "Decoder Weight Norm (Mean): 0.22859713435173035\n",
      "MSE Loss: 0.4254, L1 Loss: 0.01*12.4896\n",
      "Batch number:  94\n",
      "Active Features: 87.71%\n",
      "Decoder Weight Norm (Mean): 0.2286710888147354\n",
      "MSE Loss: 0.4264, L1 Loss: 0.01*12.5519\n",
      "Batch number:  95\n",
      "Active Features: 86.82%\n",
      "Decoder Weight Norm (Mean): 0.2287454605102539\n",
      "MSE Loss: 0.4254, L1 Loss: 0.01*12.5682\n",
      "Batch number:  96\n",
      "Active Features: 88.06%\n",
      "Decoder Weight Norm (Mean): 0.2288198620080948\n",
      "MSE Loss: 0.4268, L1 Loss: 0.01*12.5022\n",
      "Batch number:  97\n",
      "Active Features: 87.83%\n",
      "Decoder Weight Norm (Mean): 0.22889447212219238\n",
      "MSE Loss: 0.4206, L1 Loss: 0.01*12.4979\n",
      "Batch number:  98\n",
      "Active Features: 88.84%\n",
      "Decoder Weight Norm (Mean): 0.22896873950958252\n",
      "MSE Loss: 0.4267, L1 Loss: 0.01*12.4598\n",
      "Batch number:  99\n",
      "Active Features: 87.58%\n",
      "Decoder Weight Norm (Mean): 0.2290431559085846\n",
      "MSE Loss: 0.4211, L1 Loss: 0.01*12.4439\n",
      "Batch number:  100\n",
      "Active Features: 89.03%\n",
      "Decoder Weight Norm (Mean): 0.22911714017391205\n",
      "MSE Loss: 0.4263, L1 Loss: 0.01*12.4587\n",
      "Batch number:  101\n",
      "Active Features: 86.88%\n",
      "Decoder Weight Norm (Mean): 0.22919106483459473\n",
      "MSE Loss: 0.4202, L1 Loss: 0.01*12.4839\n",
      "Batch number:  102\n",
      "Active Features: 88.75%\n",
      "Decoder Weight Norm (Mean): 0.2292652726173401\n",
      "MSE Loss: 0.4197, L1 Loss: 0.01*12.2403\n",
      "Batch number:  103\n",
      "Active Features: 88.20%\n",
      "Decoder Weight Norm (Mean): 0.22933940589427948\n",
      "MSE Loss: 0.4140, L1 Loss: 0.01*12.4001\n",
      "Batch number:  104\n",
      "Active Features: 86.60%\n",
      "Decoder Weight Norm (Mean): 0.2294131964445114\n",
      "MSE Loss: 0.4256, L1 Loss: 0.01*12.4562\n",
      "Batch number:  105\n",
      "Active Features: 88.16%\n",
      "Decoder Weight Norm (Mean): 0.2294871062040329\n",
      "MSE Loss: 0.4215, L1 Loss: 0.01*12.4179\n",
      "Batch number:  106\n",
      "Active Features: 86.63%\n",
      "Decoder Weight Norm (Mean): 0.22956083714962006\n",
      "MSE Loss: 0.4180, L1 Loss: 0.01*12.5265\n",
      "Batch number:  107\n",
      "Active Features: 87.50%\n",
      "Decoder Weight Norm (Mean): 0.22963476181030273\n",
      "MSE Loss: 0.4173, L1 Loss: 0.01*12.3958\n",
      "Batch number:  108\n",
      "Active Features: 87.64%\n",
      "Decoder Weight Norm (Mean): 0.2297087162733078\n",
      "MSE Loss: 0.4140, L1 Loss: 0.01*12.5132\n",
      "Batch number:  109\n",
      "Active Features: 87.06%\n",
      "Decoder Weight Norm (Mean): 0.2297825664281845\n",
      "MSE Loss: 0.4192, L1 Loss: 0.01*12.3869\n",
      "Batch number:  110\n",
      "Active Features: 89.51%\n",
      "Decoder Weight Norm (Mean): 0.22985634207725525\n",
      "MSE Loss: 0.4326, L1 Loss: 0.01*12.5223\n",
      "Batch number:  111\n",
      "Active Features: 87.47%\n",
      "Decoder Weight Norm (Mean): 0.229930117726326\n",
      "MSE Loss: 0.4245, L1 Loss: 0.01*12.5453\n",
      "Batch number:  112\n",
      "Active Features: 88.10%\n",
      "Decoder Weight Norm (Mean): 0.23000410199165344\n",
      "MSE Loss: 0.4193, L1 Loss: 0.01*12.3905\n",
      "Batch number:  113\n",
      "Active Features: 86.83%\n",
      "Decoder Weight Norm (Mean): 0.23007769882678986\n",
      "MSE Loss: 0.4250, L1 Loss: 0.01*12.4826\n",
      "Batch number:  114\n",
      "Active Features: 87.96%\n",
      "Decoder Weight Norm (Mean): 0.2301514595746994\n",
      "MSE Loss: 0.4245, L1 Loss: 0.01*12.4301\n",
      "Batch number:  115\n",
      "Active Features: 87.41%\n",
      "Decoder Weight Norm (Mean): 0.23022527992725372\n",
      "MSE Loss: 0.4194, L1 Loss: 0.01*12.5784\n",
      "Batch number:  116\n",
      "Active Features: 86.45%\n",
      "Decoder Weight Norm (Mean): 0.2302989661693573\n",
      "MSE Loss: 0.4161, L1 Loss: 0.01*12.4178\n",
      "Batch number:  117\n",
      "Active Features: 87.37%\n",
      "Decoder Weight Norm (Mean): 0.2303725779056549\n",
      "MSE Loss: 0.4206, L1 Loss: 0.01*12.6158\n",
      "Batch number:  118\n",
      "Active Features: 87.06%\n",
      "Decoder Weight Norm (Mean): 0.2304455041885376\n",
      "MSE Loss: 0.4164, L1 Loss: 0.01*12.4423\n",
      "Batch number:  119\n",
      "Active Features: 86.45%\n",
      "Decoder Weight Norm (Mean): 0.230518639087677\n",
      "MSE Loss: 0.4180, L1 Loss: 0.01*12.4291\n",
      "Batch number:  120\n",
      "Active Features: 87.08%\n",
      "Decoder Weight Norm (Mean): 0.23059163987636566\n",
      "MSE Loss: 0.4143, L1 Loss: 0.01*12.4816\n",
      "Batch number:  121\n",
      "Active Features: 86.56%\n",
      "Decoder Weight Norm (Mean): 0.23066483438014984\n",
      "MSE Loss: 0.4240, L1 Loss: 0.01*12.6032\n",
      "Batch number:  122\n",
      "Active Features: 87.17%\n",
      "Decoder Weight Norm (Mean): 0.23073779046535492\n",
      "MSE Loss: 0.4183, L1 Loss: 0.01*12.4952\n",
      "Batch number:  123\n",
      "Active Features: 87.96%\n",
      "Decoder Weight Norm (Mean): 0.23081044852733612\n",
      "MSE Loss: 0.4175, L1 Loss: 0.01*12.6157\n",
      "Batch number:  124\n",
      "Active Features: 85.95%\n",
      "Decoder Weight Norm (Mean): 0.23088276386260986\n",
      "MSE Loss: 0.4192, L1 Loss: 0.01*12.6176\n",
      "Batch number:  125\n",
      "Active Features: 85.70%\n",
      "Decoder Weight Norm (Mean): 0.23095500469207764\n",
      "MSE Loss: 0.4114, L1 Loss: 0.01*12.6126\n",
      "Batch number:  126\n",
      "Active Features: 85.56%\n",
      "Decoder Weight Norm (Mean): 0.2310270220041275\n",
      "MSE Loss: 0.4134, L1 Loss: 0.01*12.3854\n",
      "Batch number:  127\n",
      "Active Features: 86.39%\n",
      "Decoder Weight Norm (Mean): 0.23109930753707886\n",
      "MSE Loss: 0.4207, L1 Loss: 0.01*12.5019\n",
      "Epoch [5/8], Loss: 70.8034 ----------------------------\n",
      "Batch number:  0\n",
      "Active Features: 87.84%\n",
      "Decoder Weight Norm (Mean): 0.23117166757583618\n",
      "MSE Loss: 0.4135, L1 Loss: 0.01*12.5693\n",
      "Batch number:  1\n",
      "Active Features: 86.93%\n",
      "Decoder Weight Norm (Mean): 0.23124374449253082\n",
      "MSE Loss: 0.4073, L1 Loss: 0.01*12.5857\n",
      "Batch number:  2\n",
      "Active Features: 86.48%\n",
      "Decoder Weight Norm (Mean): 0.23131555318832397\n",
      "MSE Loss: 0.4224, L1 Loss: 0.01*12.4338\n",
      "Batch number:  3\n",
      "Active Features: 85.85%\n",
      "Decoder Weight Norm (Mean): 0.2313881516456604\n",
      "MSE Loss: 0.4131, L1 Loss: 0.01*12.6297\n",
      "Batch number:  4\n",
      "Active Features: 84.55%\n",
      "Decoder Weight Norm (Mean): 0.2314605414867401\n",
      "MSE Loss: 0.4130, L1 Loss: 0.01*12.5433\n",
      "Batch number:  5\n",
      "Active Features: 85.66%\n",
      "Decoder Weight Norm (Mean): 0.23153267800807953\n",
      "MSE Loss: 0.4147, L1 Loss: 0.01*12.6659\n",
      "Batch number:  6\n",
      "Active Features: 86.78%\n",
      "Decoder Weight Norm (Mean): 0.23160451650619507\n",
      "MSE Loss: 0.4160, L1 Loss: 0.01*12.4101\n",
      "Batch number:  7\n",
      "Active Features: 86.11%\n",
      "Decoder Weight Norm (Mean): 0.23167665302753448\n",
      "MSE Loss: 0.4172, L1 Loss: 0.01*12.7244\n",
      "Batch number:  8\n",
      "Active Features: 86.17%\n",
      "Decoder Weight Norm (Mean): 0.23174899816513062\n",
      "MSE Loss: 0.4144, L1 Loss: 0.01*12.4515\n",
      "Batch number:  9\n",
      "Active Features: 87.48%\n",
      "Decoder Weight Norm (Mean): 0.2318211793899536\n",
      "MSE Loss: 0.4098, L1 Loss: 0.01*12.6051\n",
      "Batch number:  10\n",
      "Active Features: 87.37%\n",
      "Decoder Weight Norm (Mean): 0.23189295828342438\n",
      "MSE Loss: 0.4145, L1 Loss: 0.01*12.5004\n",
      "Batch number:  11\n",
      "Active Features: 85.54%\n",
      "Decoder Weight Norm (Mean): 0.23196430504322052\n",
      "MSE Loss: 0.4126, L1 Loss: 0.01*12.6533\n",
      "Batch number:  12\n",
      "Active Features: 85.96%\n",
      "Decoder Weight Norm (Mean): 0.2320355921983719\n",
      "MSE Loss: 0.4155, L1 Loss: 0.01*12.6194\n",
      "Batch number:  13\n",
      "Active Features: 86.54%\n",
      "Decoder Weight Norm (Mean): 0.23210711777210236\n",
      "MSE Loss: 0.4102, L1 Loss: 0.01*12.4135\n",
      "Batch number:  14\n",
      "Active Features: 86.03%\n",
      "Decoder Weight Norm (Mean): 0.23217855393886566\n",
      "MSE Loss: 0.4116, L1 Loss: 0.01*12.5163\n",
      "Batch number:  15\n",
      "Active Features: 85.37%\n",
      "Decoder Weight Norm (Mean): 0.23224984109401703\n",
      "MSE Loss: 0.4110, L1 Loss: 0.01*12.5573\n",
      "Batch number:  16\n",
      "Active Features: 85.52%\n",
      "Decoder Weight Norm (Mean): 0.23232126235961914\n",
      "MSE Loss: 0.4182, L1 Loss: 0.01*12.6537\n",
      "Batch number:  17\n",
      "Active Features: 84.89%\n",
      "Decoder Weight Norm (Mean): 0.23239286243915558\n",
      "MSE Loss: 0.4110, L1 Loss: 0.01*12.6471\n",
      "Batch number:  18\n",
      "Active Features: 86.20%\n",
      "Decoder Weight Norm (Mean): 0.23246429860591888\n",
      "MSE Loss: 0.4132, L1 Loss: 0.01*12.5369\n",
      "Batch number:  19\n",
      "Active Features: 86.63%\n",
      "Decoder Weight Norm (Mean): 0.2325357049703598\n",
      "MSE Loss: 0.4159, L1 Loss: 0.01*12.4964\n",
      "Batch number:  20\n",
      "Active Features: 85.47%\n",
      "Decoder Weight Norm (Mean): 0.23260697722434998\n",
      "MSE Loss: 0.4121, L1 Loss: 0.01*12.6479\n",
      "Batch number:  21\n",
      "Active Features: 86.56%\n",
      "Decoder Weight Norm (Mean): 0.23267802596092224\n",
      "MSE Loss: 0.4133, L1 Loss: 0.01*12.4784\n",
      "Batch number:  22\n",
      "Active Features: 84.90%\n",
      "Decoder Weight Norm (Mean): 0.23274916410446167\n",
      "MSE Loss: 0.4140, L1 Loss: 0.01*12.5450\n",
      "Batch number:  23\n",
      "Active Features: 86.63%\n",
      "Decoder Weight Norm (Mean): 0.23282067477703094\n",
      "MSE Loss: 0.4211, L1 Loss: 0.01*12.6959\n",
      "Batch number:  24\n",
      "Active Features: 85.41%\n",
      "Decoder Weight Norm (Mean): 0.23289237916469574\n",
      "MSE Loss: 0.4124, L1 Loss: 0.01*12.5962\n",
      "Batch number:  25\n",
      "Active Features: 85.11%\n",
      "Decoder Weight Norm (Mean): 0.2329639345407486\n",
      "MSE Loss: 0.4132, L1 Loss: 0.01*12.6567\n",
      "Batch number:  26\n",
      "Active Features: 85.50%\n",
      "Decoder Weight Norm (Mean): 0.23303571343421936\n",
      "MSE Loss: 0.4067, L1 Loss: 0.01*12.5779\n",
      "Batch number:  27\n",
      "Active Features: 84.91%\n",
      "Decoder Weight Norm (Mean): 0.23310741782188416\n",
      "MSE Loss: 0.4075, L1 Loss: 0.01*12.5237\n",
      "Batch number:  28\n",
      "Active Features: 84.99%\n",
      "Decoder Weight Norm (Mean): 0.2331790179014206\n",
      "MSE Loss: 0.4113, L1 Loss: 0.01*12.4869\n",
      "Batch number:  29\n",
      "Active Features: 87.45%\n",
      "Decoder Weight Norm (Mean): 0.23325090110301971\n",
      "MSE Loss: 0.4165, L1 Loss: 0.01*12.4761\n",
      "Batch number:  30\n",
      "Active Features: 85.04%\n",
      "Decoder Weight Norm (Mean): 0.23332226276397705\n",
      "MSE Loss: 0.4036, L1 Loss: 0.01*12.5329\n",
      "Batch number:  31\n",
      "Active Features: 84.89%\n",
      "Decoder Weight Norm (Mean): 0.2333931028842926\n",
      "MSE Loss: 0.4074, L1 Loss: 0.01*12.5177\n",
      "Batch number:  32\n",
      "Active Features: 85.19%\n",
      "Decoder Weight Norm (Mean): 0.23346368968486786\n",
      "MSE Loss: 0.4115, L1 Loss: 0.01*12.7142\n",
      "Batch number:  33\n",
      "Active Features: 85.30%\n",
      "Decoder Weight Norm (Mean): 0.23353412747383118\n",
      "MSE Loss: 0.4070, L1 Loss: 0.01*12.4730\n",
      "Batch number:  34\n",
      "Active Features: 83.19%\n",
      "Decoder Weight Norm (Mean): 0.2336045801639557\n",
      "MSE Loss: 0.4059, L1 Loss: 0.01*12.5454\n",
      "Batch number:  35\n",
      "Active Features: 83.92%\n",
      "Decoder Weight Norm (Mean): 0.23367521166801453\n",
      "MSE Loss: 0.4118, L1 Loss: 0.01*12.5764\n",
      "Batch number:  36\n",
      "Active Features: 84.87%\n",
      "Decoder Weight Norm (Mean): 0.2337455451488495\n",
      "MSE Loss: 0.4070, L1 Loss: 0.01*12.5495\n",
      "Batch number:  37\n",
      "Active Features: 84.87%\n",
      "Decoder Weight Norm (Mean): 0.23381610214710236\n",
      "MSE Loss: 0.4092, L1 Loss: 0.01*12.5782\n",
      "Batch number:  38\n",
      "Active Features: 83.56%\n",
      "Decoder Weight Norm (Mean): 0.2338862419128418\n",
      "MSE Loss: 0.4075, L1 Loss: 0.01*12.5977\n",
      "Batch number:  39\n",
      "Active Features: 85.71%\n",
      "Decoder Weight Norm (Mean): 0.23395618796348572\n",
      "MSE Loss: 0.4162, L1 Loss: 0.01*12.6275\n",
      "Batch number:  40\n",
      "Active Features: 83.14%\n",
      "Decoder Weight Norm (Mean): 0.23402616381645203\n",
      "MSE Loss: 0.4062, L1 Loss: 0.01*12.5444\n",
      "Batch number:  41\n",
      "Active Features: 83.79%\n",
      "Decoder Weight Norm (Mean): 0.23409569263458252\n",
      "MSE Loss: 0.4094, L1 Loss: 0.01*12.7112\n",
      "Batch number:  42\n",
      "Active Features: 84.28%\n",
      "Decoder Weight Norm (Mean): 0.23416520655155182\n",
      "MSE Loss: 0.4047, L1 Loss: 0.01*12.5558\n",
      "Batch number:  43\n",
      "Active Features: 83.91%\n",
      "Decoder Weight Norm (Mean): 0.23423446714878082\n",
      "MSE Loss: 0.4078, L1 Loss: 0.01*12.5964\n",
      "Batch number:  44\n",
      "Active Features: 86.17%\n",
      "Decoder Weight Norm (Mean): 0.2343038022518158\n",
      "MSE Loss: 0.4083, L1 Loss: 0.01*12.5350\n",
      "Batch number:  45\n",
      "Active Features: 84.41%\n",
      "Decoder Weight Norm (Mean): 0.2343730479478836\n",
      "MSE Loss: 0.4084, L1 Loss: 0.01*12.6164\n",
      "Batch number:  46\n",
      "Active Features: 84.56%\n",
      "Decoder Weight Norm (Mean): 0.23444204032421112\n",
      "MSE Loss: 0.4125, L1 Loss: 0.01*12.5638\n",
      "Batch number:  47\n",
      "Active Features: 82.20%\n",
      "Decoder Weight Norm (Mean): 0.23451165854930878\n",
      "MSE Loss: 0.4013, L1 Loss: 0.01*12.6057\n",
      "Batch number:  48\n",
      "Active Features: 84.58%\n",
      "Decoder Weight Norm (Mean): 0.23458155989646912\n",
      "MSE Loss: 0.4134, L1 Loss: 0.01*12.6840\n",
      "Batch number:  49\n",
      "Active Features: 84.03%\n",
      "Decoder Weight Norm (Mean): 0.2346520870923996\n",
      "MSE Loss: 0.4077, L1 Loss: 0.01*12.5610\n",
      "Batch number:  50\n",
      "Active Features: 84.07%\n",
      "Decoder Weight Norm (Mean): 0.23472262918949127\n",
      "MSE Loss: 0.4055, L1 Loss: 0.01*12.5716\n",
      "Batch number:  51\n",
      "Active Features: 85.58%\n",
      "Decoder Weight Norm (Mean): 0.23479346930980682\n",
      "MSE Loss: 0.4064, L1 Loss: 0.01*12.5967\n",
      "Batch number:  52\n",
      "Active Features: 83.84%\n",
      "Decoder Weight Norm (Mean): 0.23486439883708954\n",
      "MSE Loss: 0.4059, L1 Loss: 0.01*12.7752\n",
      "Batch number:  53\n",
      "Active Features: 85.07%\n",
      "Decoder Weight Norm (Mean): 0.23493559658527374\n",
      "MSE Loss: 0.4066, L1 Loss: 0.01*12.6573\n",
      "Batch number:  54\n",
      "Active Features: 85.30%\n",
      "Decoder Weight Norm (Mean): 0.23500654101371765\n",
      "MSE Loss: 0.4052, L1 Loss: 0.01*12.6147\n",
      "Batch number:  55\n",
      "Active Features: 83.01%\n",
      "Decoder Weight Norm (Mean): 0.23507772386074066\n",
      "MSE Loss: 0.4080, L1 Loss: 0.01*12.6634\n",
      "Batch number:  56\n",
      "Active Features: 83.78%\n",
      "Decoder Weight Norm (Mean): 0.23514911532402039\n",
      "MSE Loss: 0.4045, L1 Loss: 0.01*12.7567\n",
      "Batch number:  57\n",
      "Active Features: 84.09%\n",
      "Decoder Weight Norm (Mean): 0.23522025346755981\n",
      "MSE Loss: 0.4146, L1 Loss: 0.01*12.6759\n",
      "Batch number:  58\n",
      "Active Features: 84.51%\n",
      "Decoder Weight Norm (Mean): 0.23529188334941864\n",
      "MSE Loss: 0.4076, L1 Loss: 0.01*12.5880\n",
      "Batch number:  59\n",
      "Active Features: 82.86%\n",
      "Decoder Weight Norm (Mean): 0.2353634536266327\n",
      "MSE Loss: 0.4042, L1 Loss: 0.01*12.7064\n",
      "Batch number:  60\n",
      "Active Features: 83.93%\n",
      "Decoder Weight Norm (Mean): 0.23543483018875122\n",
      "MSE Loss: 0.4007, L1 Loss: 0.01*12.7007\n",
      "Batch number:  61\n",
      "Active Features: 84.10%\n",
      "Decoder Weight Norm (Mean): 0.23550546169281006\n",
      "MSE Loss: 0.4057, L1 Loss: 0.01*12.5709\n",
      "Batch number:  62\n",
      "Active Features: 84.61%\n",
      "Decoder Weight Norm (Mean): 0.23557594418525696\n",
      "MSE Loss: 0.4079, L1 Loss: 0.01*12.6301\n",
      "Batch number:  63\n",
      "Active Features: 83.27%\n",
      "Decoder Weight Norm (Mean): 0.23564647138118744\n",
      "MSE Loss: 0.4083, L1 Loss: 0.01*12.6854\n",
      "Batch number:  64\n",
      "Active Features: 85.31%\n",
      "Decoder Weight Norm (Mean): 0.23571676015853882\n",
      "MSE Loss: 0.4063, L1 Loss: 0.01*12.5838\n",
      "Batch number:  65\n",
      "Active Features: 83.52%\n",
      "Decoder Weight Norm (Mean): 0.23578697443008423\n",
      "MSE Loss: 0.4054, L1 Loss: 0.01*12.6307\n",
      "Batch number:  66\n",
      "Active Features: 80.59%\n",
      "Decoder Weight Norm (Mean): 0.2358570247888565\n",
      "MSE Loss: 0.4036, L1 Loss: 0.01*12.6590\n",
      "Batch number:  67\n",
      "Active Features: 82.78%\n",
      "Decoder Weight Norm (Mean): 0.23592734336853027\n",
      "MSE Loss: 0.4057, L1 Loss: 0.01*12.6267\n",
      "Batch number:  68\n",
      "Active Features: 83.33%\n",
      "Decoder Weight Norm (Mean): 0.23599769175052643\n",
      "MSE Loss: 0.4042, L1 Loss: 0.01*12.4617\n",
      "Batch number:  69\n",
      "Active Features: 82.97%\n",
      "Decoder Weight Norm (Mean): 0.23606820404529572\n",
      "MSE Loss: 0.4058, L1 Loss: 0.01*12.5795\n",
      "Batch number:  70\n",
      "Active Features: 84.27%\n",
      "Decoder Weight Norm (Mean): 0.2361389547586441\n",
      "MSE Loss: 0.4045, L1 Loss: 0.01*12.6008\n",
      "Batch number:  71\n",
      "Active Features: 83.16%\n",
      "Decoder Weight Norm (Mean): 0.23620977997779846\n",
      "MSE Loss: 0.4062, L1 Loss: 0.01*12.6945\n",
      "Batch number:  72\n",
      "Active Features: 82.76%\n",
      "Decoder Weight Norm (Mean): 0.2362804114818573\n",
      "MSE Loss: 0.4055, L1 Loss: 0.01*12.6998\n",
      "Batch number:  73\n",
      "Active Features: 82.39%\n",
      "Decoder Weight Norm (Mean): 0.23635122179985046\n",
      "MSE Loss: 0.4000, L1 Loss: 0.01*12.5896\n",
      "Batch number:  74\n",
      "Active Features: 82.46%\n",
      "Decoder Weight Norm (Mean): 0.23642206192016602\n",
      "MSE Loss: 0.3978, L1 Loss: 0.01*12.8029\n",
      "Batch number:  75\n",
      "Active Features: 83.38%\n",
      "Decoder Weight Norm (Mean): 0.23649276793003082\n",
      "MSE Loss: 0.4057, L1 Loss: 0.01*12.5373\n",
      "Batch number:  76\n",
      "Active Features: 83.69%\n",
      "Decoder Weight Norm (Mean): 0.2365630865097046\n",
      "MSE Loss: 0.4008, L1 Loss: 0.01*12.6357\n",
      "Batch number:  77\n",
      "Active Features: 83.31%\n",
      "Decoder Weight Norm (Mean): 0.2366330474615097\n",
      "MSE Loss: 0.4021, L1 Loss: 0.01*12.5555\n",
      "Batch number:  78\n",
      "Active Features: 83.87%\n",
      "Decoder Weight Norm (Mean): 0.23670290410518646\n",
      "MSE Loss: 0.4011, L1 Loss: 0.01*12.6272\n",
      "Batch number:  79\n",
      "Active Features: 82.60%\n",
      "Decoder Weight Norm (Mean): 0.23677270114421844\n",
      "MSE Loss: 0.3997, L1 Loss: 0.01*12.6624\n",
      "Batch number:  80\n",
      "Active Features: 80.47%\n",
      "Decoder Weight Norm (Mean): 0.23684215545654297\n",
      "MSE Loss: 0.4049, L1 Loss: 0.01*12.7794\n",
      "Batch number:  81\n",
      "Active Features: 82.57%\n",
      "Decoder Weight Norm (Mean): 0.23691190779209137\n",
      "MSE Loss: 0.4009, L1 Loss: 0.01*12.6319\n",
      "Batch number:  82\n",
      "Active Features: 84.64%\n",
      "Decoder Weight Norm (Mean): 0.23698188364505768\n",
      "MSE Loss: 0.4029, L1 Loss: 0.01*12.6850\n",
      "Batch number:  83\n",
      "Active Features: 84.69%\n",
      "Decoder Weight Norm (Mean): 0.2370515763759613\n",
      "MSE Loss: 0.4107, L1 Loss: 0.01*12.6428\n",
      "Batch number:  84\n",
      "Active Features: 80.98%\n",
      "Decoder Weight Norm (Mean): 0.23712167143821716\n",
      "MSE Loss: 0.3916, L1 Loss: 0.01*12.5107\n",
      "Batch number:  85\n",
      "Active Features: 81.69%\n",
      "Decoder Weight Norm (Mean): 0.23719173669815063\n",
      "MSE Loss: 0.3984, L1 Loss: 0.01*12.7731\n",
      "Batch number:  86\n",
      "Active Features: 80.93%\n",
      "Decoder Weight Norm (Mean): 0.2372615486383438\n",
      "MSE Loss: 0.3936, L1 Loss: 0.01*12.7327\n",
      "Batch number:  87\n",
      "Active Features: 79.53%\n",
      "Decoder Weight Norm (Mean): 0.23733116686344147\n",
      "MSE Loss: 0.3951, L1 Loss: 0.01*12.6681\n",
      "Batch number:  88\n",
      "Active Features: 81.82%\n",
      "Decoder Weight Norm (Mean): 0.2374011129140854\n",
      "MSE Loss: 0.4020, L1 Loss: 0.01*12.6646\n",
      "Batch number:  89\n",
      "Active Features: 81.14%\n",
      "Decoder Weight Norm (Mean): 0.2374715805053711\n",
      "MSE Loss: 0.4036, L1 Loss: 0.01*12.6901\n",
      "Batch number:  90\n",
      "Active Features: 82.55%\n",
      "Decoder Weight Norm (Mean): 0.23754194378852844\n",
      "MSE Loss: 0.4000, L1 Loss: 0.01*12.7117\n",
      "Batch number:  91\n",
      "Active Features: 81.65%\n",
      "Decoder Weight Norm (Mean): 0.23761193454265594\n",
      "MSE Loss: 0.3964, L1 Loss: 0.01*12.6491\n",
      "Batch number:  92\n",
      "Active Features: 82.14%\n",
      "Decoder Weight Norm (Mean): 0.23768162727355957\n",
      "MSE Loss: 0.4039, L1 Loss: 0.01*12.8367\n",
      "Batch number:  93\n",
      "Active Features: 81.28%\n",
      "Decoder Weight Norm (Mean): 0.2377518266439438\n",
      "MSE Loss: 0.3955, L1 Loss: 0.01*12.6291\n",
      "Batch number:  94\n",
      "Active Features: 79.90%\n",
      "Decoder Weight Norm (Mean): 0.23782241344451904\n",
      "MSE Loss: 0.4036, L1 Loss: 0.01*12.6798\n",
      "Batch number:  95\n",
      "Active Features: 80.49%\n",
      "Decoder Weight Norm (Mean): 0.23789340257644653\n",
      "MSE Loss: 0.4001, L1 Loss: 0.01*12.8231\n",
      "Batch number:  96\n",
      "Active Features: 81.26%\n",
      "Decoder Weight Norm (Mean): 0.23796446621418\n",
      "MSE Loss: 0.4034, L1 Loss: 0.01*12.7644\n",
      "Batch number:  97\n",
      "Active Features: 80.94%\n",
      "Decoder Weight Norm (Mean): 0.23803524672985077\n",
      "MSE Loss: 0.3964, L1 Loss: 0.01*12.7069\n",
      "Batch number:  98\n",
      "Active Features: 84.42%\n",
      "Decoder Weight Norm (Mean): 0.23810571432113647\n",
      "MSE Loss: 0.4066, L1 Loss: 0.01*12.6827\n",
      "Batch number:  99\n",
      "Active Features: 81.62%\n",
      "Decoder Weight Norm (Mean): 0.23817598819732666\n",
      "MSE Loss: 0.4004, L1 Loss: 0.01*12.5986\n",
      "Batch number:  100\n",
      "Active Features: 81.95%\n",
      "Decoder Weight Norm (Mean): 0.23824599385261536\n",
      "MSE Loss: 0.4095, L1 Loss: 0.01*12.7866\n",
      "Batch number:  101\n",
      "Active Features: 78.98%\n",
      "Decoder Weight Norm (Mean): 0.23831620812416077\n",
      "MSE Loss: 0.3929, L1 Loss: 0.01*12.7000\n",
      "Batch number:  102\n",
      "Active Features: 82.06%\n",
      "Decoder Weight Norm (Mean): 0.23838642239570618\n",
      "MSE Loss: 0.3953, L1 Loss: 0.01*12.4476\n",
      "Batch number:  103\n",
      "Active Features: 81.39%\n",
      "Decoder Weight Norm (Mean): 0.23845624923706055\n",
      "MSE Loss: 0.4016, L1 Loss: 0.01*12.8448\n",
      "Batch number:  104\n",
      "Active Features: 80.10%\n",
      "Decoder Weight Norm (Mean): 0.23852582275867462\n",
      "MSE Loss: 0.3975, L1 Loss: 0.01*12.6326\n",
      "Batch number:  105\n",
      "Active Features: 80.86%\n",
      "Decoder Weight Norm (Mean): 0.23859527707099915\n",
      "MSE Loss: 0.3987, L1 Loss: 0.01*12.6460\n",
      "Batch number:  106\n",
      "Active Features: 78.79%\n",
      "Decoder Weight Norm (Mean): 0.23866474628448486\n",
      "MSE Loss: 0.3941, L1 Loss: 0.01*12.8226\n",
      "Batch number:  107\n",
      "Active Features: 81.36%\n",
      "Decoder Weight Norm (Mean): 0.23873402178287506\n",
      "MSE Loss: 0.4012, L1 Loss: 0.01*12.6508\n",
      "Batch number:  108\n",
      "Active Features: 79.31%\n",
      "Decoder Weight Norm (Mean): 0.23880361020565033\n",
      "MSE Loss: 0.3943, L1 Loss: 0.01*12.8419\n",
      "Batch number:  109\n",
      "Active Features: 81.74%\n",
      "Decoder Weight Norm (Mean): 0.2388734370470047\n",
      "MSE Loss: 0.4018, L1 Loss: 0.01*12.6771\n",
      "Batch number:  110\n",
      "Active Features: 83.02%\n",
      "Decoder Weight Norm (Mean): 0.2389431595802307\n",
      "MSE Loss: 0.4126, L1 Loss: 0.01*12.8786\n",
      "Batch number:  111\n",
      "Active Features: 80.48%\n",
      "Decoder Weight Norm (Mean): 0.23901261389255524\n",
      "MSE Loss: 0.3994, L1 Loss: 0.01*12.8598\n",
      "Batch number:  112\n",
      "Active Features: 82.03%\n",
      "Decoder Weight Norm (Mean): 0.23908205330371857\n",
      "MSE Loss: 0.3949, L1 Loss: 0.01*12.7600\n",
      "Batch number:  113\n",
      "Active Features: 79.61%\n",
      "Decoder Weight Norm (Mean): 0.23915128409862518\n",
      "MSE Loss: 0.4026, L1 Loss: 0.01*12.8052\n",
      "Batch number:  114\n",
      "Active Features: 81.01%\n",
      "Decoder Weight Norm (Mean): 0.23922041058540344\n",
      "MSE Loss: 0.4014, L1 Loss: 0.01*12.6560\n",
      "Batch number:  115\n",
      "Active Features: 79.30%\n",
      "Decoder Weight Norm (Mean): 0.23928941786289215\n",
      "MSE Loss: 0.3957, L1 Loss: 0.01*12.8454\n",
      "Batch number:  116\n",
      "Active Features: 79.94%\n",
      "Decoder Weight Norm (Mean): 0.23935821652412415\n",
      "MSE Loss: 0.3925, L1 Loss: 0.01*12.6121\n",
      "Batch number:  117\n",
      "Active Features: 81.22%\n",
      "Decoder Weight Norm (Mean): 0.23942700028419495\n",
      "MSE Loss: 0.3991, L1 Loss: 0.01*12.7836\n",
      "Batch number:  118\n",
      "Active Features: 80.84%\n",
      "Decoder Weight Norm (Mean): 0.23949584364891052\n",
      "MSE Loss: 0.3989, L1 Loss: 0.01*12.6877\n",
      "Batch number:  119\n",
      "Active Features: 80.14%\n",
      "Decoder Weight Norm (Mean): 0.23956453800201416\n",
      "MSE Loss: 0.4018, L1 Loss: 0.01*12.8844\n",
      "Batch number:  120\n",
      "Active Features: 79.67%\n",
      "Decoder Weight Norm (Mean): 0.2396334409713745\n",
      "MSE Loss: 0.3946, L1 Loss: 0.01*12.6670\n",
      "Batch number:  121\n",
      "Active Features: 81.14%\n",
      "Decoder Weight Norm (Mean): 0.2397022694349289\n",
      "MSE Loss: 0.4006, L1 Loss: 0.01*12.8902\n",
      "Batch number:  122\n",
      "Active Features: 82.12%\n",
      "Decoder Weight Norm (Mean): 0.23977093398571014\n",
      "MSE Loss: 0.3915, L1 Loss: 0.01*12.6648\n",
      "Batch number:  123\n",
      "Active Features: 80.95%\n",
      "Decoder Weight Norm (Mean): 0.239839568734169\n",
      "MSE Loss: 0.3943, L1 Loss: 0.01*12.9167\n",
      "Batch number:  124\n",
      "Active Features: 79.55%\n",
      "Decoder Weight Norm (Mean): 0.23990771174430847\n",
      "MSE Loss: 0.3993, L1 Loss: 0.01*12.8683\n",
      "Batch number:  125\n",
      "Active Features: 79.62%\n",
      "Decoder Weight Norm (Mean): 0.2399759739637375\n",
      "MSE Loss: 0.3941, L1 Loss: 0.01*12.9492\n",
      "Batch number:  126\n",
      "Active Features: 78.83%\n",
      "Decoder Weight Norm (Mean): 0.2400444746017456\n",
      "MSE Loss: 0.3882, L1 Loss: 0.01*12.6729\n",
      "Batch number:  127\n",
      "Active Features: 81.74%\n",
      "Decoder Weight Norm (Mean): 0.24011291563510895\n",
      "MSE Loss: 0.3988, L1 Loss: 0.01*12.7006\n",
      "Epoch [6/8], Loss: 68.0600 ----------------------------\n",
      "Batch number:  0\n",
      "Active Features: 80.38%\n",
      "Decoder Weight Norm (Mean): 0.24018123745918274\n",
      "MSE Loss: 0.3967, L1 Loss: 0.01*12.7836\n",
      "Batch number:  1\n",
      "Active Features: 78.90%\n",
      "Decoder Weight Norm (Mean): 0.24024926126003265\n",
      "MSE Loss: 0.3870, L1 Loss: 0.01*12.8202\n",
      "Batch number:  2\n",
      "Active Features: 81.15%\n",
      "Decoder Weight Norm (Mean): 0.24031707644462585\n",
      "MSE Loss: 0.3991, L1 Loss: 0.01*12.7591\n",
      "Batch number:  3\n",
      "Active Features: 79.22%\n",
      "Decoder Weight Norm (Mean): 0.24038517475128174\n",
      "MSE Loss: 0.3859, L1 Loss: 0.01*12.8105\n",
      "Batch number:  4\n",
      "Active Features: 78.45%\n",
      "Decoder Weight Norm (Mean): 0.24045294523239136\n",
      "MSE Loss: 0.3911, L1 Loss: 0.01*12.7673\n",
      "Batch number:  5\n",
      "Active Features: 79.70%\n",
      "Decoder Weight Norm (Mean): 0.24052079021930695\n",
      "MSE Loss: 0.3951, L1 Loss: 0.01*12.9019\n",
      "Batch number:  6\n",
      "Active Features: 79.65%\n",
      "Decoder Weight Norm (Mean): 0.24058842658996582\n",
      "MSE Loss: 0.3950, L1 Loss: 0.01*12.6360\n",
      "Batch number:  7\n",
      "Active Features: 79.41%\n",
      "Decoder Weight Norm (Mean): 0.24065622687339783\n",
      "MSE Loss: 0.3945, L1 Loss: 0.01*12.9644\n",
      "Batch number:  8\n",
      "Active Features: 80.34%\n",
      "Decoder Weight Norm (Mean): 0.24072381854057312\n",
      "MSE Loss: 0.3938, L1 Loss: 0.01*12.7670\n",
      "Batch number:  9\n",
      "Active Features: 80.78%\n",
      "Decoder Weight Norm (Mean): 0.24079176783561707\n",
      "MSE Loss: 0.3864, L1 Loss: 0.01*12.7373\n",
      "Batch number:  10\n",
      "Active Features: 78.91%\n",
      "Decoder Weight Norm (Mean): 0.2408590316772461\n",
      "MSE Loss: 0.3994, L1 Loss: 0.01*12.7774\n",
      "Batch number:  11\n",
      "Active Features: 78.60%\n",
      "Decoder Weight Norm (Mean): 0.2409261167049408\n",
      "MSE Loss: 0.3935, L1 Loss: 0.01*13.0054\n",
      "Batch number:  12\n",
      "Active Features: 78.67%\n",
      "Decoder Weight Norm (Mean): 0.24099306762218475\n",
      "MSE Loss: 0.3939, L1 Loss: 0.01*12.8304\n",
      "Batch number:  13\n",
      "Active Features: 79.93%\n",
      "Decoder Weight Norm (Mean): 0.24106073379516602\n",
      "MSE Loss: 0.3934, L1 Loss: 0.01*12.6830\n",
      "Batch number:  14\n",
      "Active Features: 78.79%\n",
      "Decoder Weight Norm (Mean): 0.2411283701658249\n",
      "MSE Loss: 0.3904, L1 Loss: 0.01*12.7977\n",
      "Batch number:  15\n",
      "Active Features: 79.55%\n",
      "Decoder Weight Norm (Mean): 0.24119633436203003\n",
      "MSE Loss: 0.3935, L1 Loss: 0.01*12.7767\n",
      "Batch number:  16\n",
      "Active Features: 78.64%\n",
      "Decoder Weight Norm (Mean): 0.2412640005350113\n",
      "MSE Loss: 0.3949, L1 Loss: 0.01*12.8779\n",
      "Batch number:  17\n",
      "Active Features: 78.42%\n",
      "Decoder Weight Norm (Mean): 0.2413315623998642\n",
      "MSE Loss: 0.3944, L1 Loss: 0.01*12.8445\n",
      "Batch number:  18\n",
      "Active Features: 79.27%\n",
      "Decoder Weight Norm (Mean): 0.2413988709449768\n",
      "MSE Loss: 0.3956, L1 Loss: 0.01*12.7963\n",
      "Batch number:  19\n",
      "Active Features: 79.55%\n",
      "Decoder Weight Norm (Mean): 0.24146610498428345\n",
      "MSE Loss: 0.4010, L1 Loss: 0.01*12.7737\n",
      "Batch number:  20\n",
      "Active Features: 79.57%\n",
      "Decoder Weight Norm (Mean): 0.24153363704681396\n",
      "MSE Loss: 0.3936, L1 Loss: 0.01*12.9441\n",
      "Batch number:  21\n",
      "Active Features: 79.23%\n",
      "Decoder Weight Norm (Mean): 0.24160121381282806\n",
      "MSE Loss: 0.3962, L1 Loss: 0.01*12.7090\n",
      "Batch number:  22\n",
      "Active Features: 79.33%\n",
      "Decoder Weight Norm (Mean): 0.24166882038116455\n",
      "MSE Loss: 0.3900, L1 Loss: 0.01*12.6495\n",
      "Batch number:  23\n",
      "Active Features: 78.97%\n",
      "Decoder Weight Norm (Mean): 0.24173662066459656\n",
      "MSE Loss: 0.3958, L1 Loss: 0.01*12.8876\n",
      "Batch number:  24\n",
      "Active Features: 77.01%\n",
      "Decoder Weight Norm (Mean): 0.2418045848608017\n",
      "MSE Loss: 0.3964, L1 Loss: 0.01*12.8652\n",
      "Batch number:  25\n",
      "Active Features: 76.83%\n",
      "Decoder Weight Norm (Mean): 0.2418726086616516\n",
      "MSE Loss: 0.3924, L1 Loss: 0.01*12.8863\n",
      "Batch number:  26\n",
      "Active Features: 78.72%\n",
      "Decoder Weight Norm (Mean): 0.24194081127643585\n",
      "MSE Loss: 0.3895, L1 Loss: 0.01*12.8508\n",
      "Batch number:  27\n",
      "Active Features: 78.00%\n",
      "Decoder Weight Norm (Mean): 0.24200887978076935\n",
      "MSE Loss: 0.3888, L1 Loss: 0.01*12.8614\n",
      "Batch number:  28\n",
      "Active Features: 76.56%\n",
      "Decoder Weight Norm (Mean): 0.24207665026187897\n",
      "MSE Loss: 0.3913, L1 Loss: 0.01*12.7548\n",
      "Batch number:  29\n",
      "Active Features: 81.22%\n",
      "Decoder Weight Norm (Mean): 0.24214448034763336\n",
      "MSE Loss: 0.3958, L1 Loss: 0.01*12.6420\n",
      "Batch number:  30\n",
      "Active Features: 78.11%\n",
      "Decoder Weight Norm (Mean): 0.24221254885196686\n",
      "MSE Loss: 0.3894, L1 Loss: 0.01*12.7269\n",
      "Batch number:  31\n",
      "Active Features: 79.45%\n",
      "Decoder Weight Norm (Mean): 0.24228040874004364\n",
      "MSE Loss: 0.3914, L1 Loss: 0.01*12.6968\n",
      "Batch number:  32\n",
      "Active Features: 77.22%\n",
      "Decoder Weight Norm (Mean): 0.24234794080257416\n",
      "MSE Loss: 0.3945, L1 Loss: 0.01*12.9627\n",
      "Batch number:  33\n",
      "Active Features: 78.06%\n",
      "Decoder Weight Norm (Mean): 0.24241536855697632\n",
      "MSE Loss: 0.3892, L1 Loss: 0.01*12.7602\n",
      "Batch number:  34\n",
      "Active Features: 76.27%\n",
      "Decoder Weight Norm (Mean): 0.24248290061950684\n",
      "MSE Loss: 0.3846, L1 Loss: 0.01*12.7520\n",
      "Batch number:  35\n",
      "Active Features: 78.53%\n",
      "Decoder Weight Norm (Mean): 0.24255043268203735\n",
      "MSE Loss: 0.3914, L1 Loss: 0.01*12.7921\n",
      "Batch number:  36\n",
      "Active Features: 79.58%\n",
      "Decoder Weight Norm (Mean): 0.24261777102947235\n",
      "MSE Loss: 0.3883, L1 Loss: 0.01*12.8251\n",
      "Batch number:  37\n",
      "Active Features: 78.41%\n",
      "Decoder Weight Norm (Mean): 0.24268469214439392\n",
      "MSE Loss: 0.3909, L1 Loss: 0.01*12.8806\n",
      "Batch number:  38\n",
      "Active Features: 78.27%\n",
      "Decoder Weight Norm (Mean): 0.24275170266628265\n",
      "MSE Loss: 0.3901, L1 Loss: 0.01*12.7419\n",
      "Batch number:  39\n",
      "Active Features: 77.47%\n",
      "Decoder Weight Norm (Mean): 0.24281884729862213\n",
      "MSE Loss: 0.3865, L1 Loss: 0.01*12.7664\n",
      "Batch number:  40\n",
      "Active Features: 77.79%\n",
      "Decoder Weight Norm (Mean): 0.24288524687290192\n",
      "MSE Loss: 0.3888, L1 Loss: 0.01*12.8353\n",
      "Batch number:  41\n",
      "Active Features: 78.08%\n",
      "Decoder Weight Norm (Mean): 0.24295133352279663\n",
      "MSE Loss: 0.3896, L1 Loss: 0.01*12.8000\n",
      "Batch number:  42\n",
      "Active Features: 77.28%\n",
      "Decoder Weight Norm (Mean): 0.24301742017269135\n",
      "MSE Loss: 0.3886, L1 Loss: 0.01*12.8680\n",
      "Batch number:  43\n",
      "Active Features: 77.27%\n",
      "Decoder Weight Norm (Mean): 0.24308344721794128\n",
      "MSE Loss: 0.3893, L1 Loss: 0.01*12.8134\n",
      "Batch number:  44\n",
      "Active Features: 79.73%\n",
      "Decoder Weight Norm (Mean): 0.24314972758293152\n",
      "MSE Loss: 0.3905, L1 Loss: 0.01*12.7314\n",
      "Batch number:  45\n",
      "Active Features: 79.52%\n",
      "Decoder Weight Norm (Mean): 0.24321581423282623\n",
      "MSE Loss: 0.3892, L1 Loss: 0.01*12.8401\n",
      "Batch number:  46\n",
      "Active Features: 79.30%\n",
      "Decoder Weight Norm (Mean): 0.24328185617923737\n",
      "MSE Loss: 0.3899, L1 Loss: 0.01*12.9488\n",
      "Batch number:  47\n",
      "Active Features: 77.64%\n",
      "Decoder Weight Norm (Mean): 0.24334779381752014\n",
      "MSE Loss: 0.3805, L1 Loss: 0.01*12.8365\n",
      "Batch number:  48\n",
      "Active Features: 76.84%\n",
      "Decoder Weight Norm (Mean): 0.24341414868831635\n",
      "MSE Loss: 0.3889, L1 Loss: 0.01*12.9999\n",
      "Batch number:  49\n",
      "Active Features: 79.14%\n",
      "Decoder Weight Norm (Mean): 0.24348051846027374\n",
      "MSE Loss: 0.3935, L1 Loss: 0.01*12.9385\n",
      "Batch number:  50\n",
      "Active Features: 77.66%\n",
      "Decoder Weight Norm (Mean): 0.24354681372642517\n",
      "MSE Loss: 0.3909, L1 Loss: 0.01*12.8069\n",
      "Batch number:  51\n",
      "Active Features: 78.39%\n",
      "Decoder Weight Norm (Mean): 0.2436136156320572\n",
      "MSE Loss: 0.3880, L1 Loss: 0.01*12.7619\n",
      "Batch number:  52\n",
      "Active Features: 75.42%\n",
      "Decoder Weight Norm (Mean): 0.2436807006597519\n",
      "MSE Loss: 0.3883, L1 Loss: 0.01*12.8791\n",
      "Batch number:  53\n",
      "Active Features: 80.03%\n",
      "Decoder Weight Norm (Mean): 0.24374772608280182\n",
      "MSE Loss: 0.3897, L1 Loss: 0.01*12.9381\n",
      "Batch number:  54\n",
      "Active Features: 77.44%\n",
      "Decoder Weight Norm (Mean): 0.24381454288959503\n",
      "MSE Loss: 0.3825, L1 Loss: 0.01*12.6716\n",
      "Batch number:  55\n",
      "Active Features: 79.16%\n",
      "Decoder Weight Norm (Mean): 0.24388185143470764\n",
      "MSE Loss: 0.3887, L1 Loss: 0.01*12.9101\n",
      "Batch number:  56\n",
      "Active Features: 76.35%\n",
      "Decoder Weight Norm (Mean): 0.24394896626472473\n",
      "MSE Loss: 0.3851, L1 Loss: 0.01*12.8507\n",
      "Batch number:  57\n",
      "Active Features: 76.51%\n",
      "Decoder Weight Norm (Mean): 0.24401584267616272\n",
      "MSE Loss: 0.3929, L1 Loss: 0.01*12.9633\n",
      "Batch number:  58\n",
      "Active Features: 76.67%\n",
      "Decoder Weight Norm (Mean): 0.24408270418643951\n",
      "MSE Loss: 0.3853, L1 Loss: 0.01*12.8769\n",
      "Batch number:  59\n",
      "Active Features: 77.73%\n",
      "Decoder Weight Norm (Mean): 0.24414950609207153\n",
      "MSE Loss: 0.3842, L1 Loss: 0.01*12.9022\n",
      "Batch number:  60\n",
      "Active Features: 76.22%\n",
      "Decoder Weight Norm (Mean): 0.24421589076519012\n",
      "MSE Loss: 0.3830, L1 Loss: 0.01*12.9800\n",
      "Batch number:  61\n",
      "Active Features: 77.41%\n",
      "Decoder Weight Norm (Mean): 0.24428187310695648\n",
      "MSE Loss: 0.3853, L1 Loss: 0.01*12.8206\n",
      "Batch number:  62\n",
      "Active Features: 77.46%\n",
      "Decoder Weight Norm (Mean): 0.2443476915359497\n",
      "MSE Loss: 0.3899, L1 Loss: 0.01*12.8981\n",
      "Batch number:  63\n",
      "Active Features: 77.52%\n",
      "Decoder Weight Norm (Mean): 0.2444136142730713\n",
      "MSE Loss: 0.3892, L1 Loss: 0.01*12.9114\n",
      "Batch number:  64\n",
      "Active Features: 78.83%\n",
      "Decoder Weight Norm (Mean): 0.24447987973690033\n",
      "MSE Loss: 0.3904, L1 Loss: 0.01*12.8892\n",
      "Batch number:  65\n",
      "Active Features: 77.70%\n",
      "Decoder Weight Norm (Mean): 0.244546040892601\n",
      "MSE Loss: 0.3920, L1 Loss: 0.01*12.8041\n",
      "Batch number:  66\n",
      "Active Features: 76.02%\n",
      "Decoder Weight Norm (Mean): 0.24461263418197632\n",
      "MSE Loss: 0.3870, L1 Loss: 0.01*12.9031\n",
      "Batch number:  67\n",
      "Active Features: 75.59%\n",
      "Decoder Weight Norm (Mean): 0.24467943608760834\n",
      "MSE Loss: 0.3854, L1 Loss: 0.01*12.9384\n",
      "Batch number:  68\n",
      "Active Features: 77.69%\n",
      "Decoder Weight Norm (Mean): 0.24474625289440155\n",
      "MSE Loss: 0.3893, L1 Loss: 0.01*12.6988\n",
      "Batch number:  69\n",
      "Active Features: 75.98%\n",
      "Decoder Weight Norm (Mean): 0.2448129802942276\n",
      "MSE Loss: 0.3846, L1 Loss: 0.01*12.8341\n",
      "Batch number:  70\n",
      "Active Features: 77.25%\n",
      "Decoder Weight Norm (Mean): 0.24487952888011932\n",
      "MSE Loss: 0.3859, L1 Loss: 0.01*12.7929\n",
      "Batch number:  71\n",
      "Active Features: 75.24%\n",
      "Decoder Weight Norm (Mean): 0.24494630098342896\n",
      "MSE Loss: 0.3840, L1 Loss: 0.01*12.9176\n",
      "Batch number:  72\n",
      "Active Features: 77.05%\n",
      "Decoder Weight Norm (Mean): 0.24501317739486694\n",
      "MSE Loss: 0.3892, L1 Loss: 0.01*12.9205\n",
      "Batch number:  73\n",
      "Active Features: 75.97%\n",
      "Decoder Weight Norm (Mean): 0.24507983028888702\n",
      "MSE Loss: 0.3834, L1 Loss: 0.01*12.7692\n",
      "Batch number:  74\n",
      "Active Features: 74.00%\n",
      "Decoder Weight Norm (Mean): 0.2451467216014862\n",
      "MSE Loss: 0.3809, L1 Loss: 0.01*13.0124\n",
      "Batch number:  75\n",
      "Active Features: 77.64%\n",
      "Decoder Weight Norm (Mean): 0.2452138066291809\n",
      "MSE Loss: 0.3858, L1 Loss: 0.01*12.7597\n",
      "Batch number:  76\n",
      "Active Features: 77.72%\n",
      "Decoder Weight Norm (Mean): 0.2452806532382965\n",
      "MSE Loss: 0.3817, L1 Loss: 0.01*12.9245\n",
      "Batch number:  77\n",
      "Active Features: 77.04%\n",
      "Decoder Weight Norm (Mean): 0.24534714221954346\n",
      "MSE Loss: 0.3846, L1 Loss: 0.01*12.8151\n",
      "Batch number:  78\n",
      "Active Features: 78.35%\n",
      "Decoder Weight Norm (Mean): 0.2454133778810501\n",
      "MSE Loss: 0.3847, L1 Loss: 0.01*12.8882\n",
      "Batch number:  79\n",
      "Active Features: 76.51%\n",
      "Decoder Weight Norm (Mean): 0.2454797774553299\n",
      "MSE Loss: 0.3859, L1 Loss: 0.01*12.9913\n",
      "Batch number:  80\n",
      "Active Features: 75.79%\n",
      "Decoder Weight Norm (Mean): 0.24554598331451416\n",
      "MSE Loss: 0.3864, L1 Loss: 0.01*12.9180\n",
      "Batch number:  81\n",
      "Active Features: 74.33%\n",
      "Decoder Weight Norm (Mean): 0.24561254680156708\n",
      "MSE Loss: 0.3823, L1 Loss: 0.01*12.8610\n",
      "Batch number:  82\n",
      "Active Features: 76.57%\n",
      "Decoder Weight Norm (Mean): 0.2456793636083603\n",
      "MSE Loss: 0.3838, L1 Loss: 0.01*12.9098\n",
      "Batch number:  83\n",
      "Active Features: 78.09%\n",
      "Decoder Weight Norm (Mean): 0.2457459717988968\n",
      "MSE Loss: 0.3847, L1 Loss: 0.01*12.8252\n",
      "Batch number:  84\n",
      "Active Features: 76.73%\n",
      "Decoder Weight Norm (Mean): 0.24581220746040344\n",
      "MSE Loss: 0.3801, L1 Loss: 0.01*12.7232\n",
      "Batch number:  85\n",
      "Active Features: 75.11%\n",
      "Decoder Weight Norm (Mean): 0.2458781898021698\n",
      "MSE Loss: 0.3767, L1 Loss: 0.01*12.8472\n",
      "Batch number:  86\n",
      "Active Features: 74.72%\n",
      "Decoder Weight Norm (Mean): 0.2459437996149063\n",
      "MSE Loss: 0.3805, L1 Loss: 0.01*12.8408\n",
      "Batch number:  87\n",
      "Active Features: 73.25%\n",
      "Decoder Weight Norm (Mean): 0.2460092157125473\n",
      "MSE Loss: 0.3785, L1 Loss: 0.01*12.9472\n",
      "Batch number:  88\n",
      "Active Features: 75.31%\n",
      "Decoder Weight Norm (Mean): 0.24607504904270172\n",
      "MSE Loss: 0.3832, L1 Loss: 0.01*12.8341\n",
      "Batch number:  89\n",
      "Active Features: 74.61%\n",
      "Decoder Weight Norm (Mean): 0.24614088237285614\n",
      "MSE Loss: 0.3853, L1 Loss: 0.01*12.9149\n",
      "Batch number:  90\n",
      "Active Features: 76.45%\n",
      "Decoder Weight Norm (Mean): 0.24620641767978668\n",
      "MSE Loss: 0.3847, L1 Loss: 0.01*12.8487\n",
      "Batch number:  91\n",
      "Active Features: 74.22%\n",
      "Decoder Weight Norm (Mean): 0.24627166986465454\n",
      "MSE Loss: 0.3802, L1 Loss: 0.01*12.9078\n",
      "Batch number:  92\n",
      "Active Features: 75.83%\n",
      "Decoder Weight Norm (Mean): 0.2463369518518448\n",
      "MSE Loss: 0.3844, L1 Loss: 0.01*12.9992\n",
      "Batch number:  93\n",
      "Active Features: 74.42%\n",
      "Decoder Weight Norm (Mean): 0.2464025318622589\n",
      "MSE Loss: 0.3808, L1 Loss: 0.01*12.9474\n",
      "Batch number:  94\n",
      "Active Features: 74.12%\n",
      "Decoder Weight Norm (Mean): 0.24646860361099243\n",
      "MSE Loss: 0.3819, L1 Loss: 0.01*12.8985\n",
      "Batch number:  95\n",
      "Active Features: 73.14%\n",
      "Decoder Weight Norm (Mean): 0.24653473496437073\n",
      "MSE Loss: 0.3824, L1 Loss: 0.01*13.0411\n",
      "Batch number:  96\n",
      "Active Features: 73.47%\n",
      "Decoder Weight Norm (Mean): 0.24660074710845947\n",
      "MSE Loss: 0.3854, L1 Loss: 0.01*12.9487\n",
      "Batch number:  97\n",
      "Active Features: 75.56%\n",
      "Decoder Weight Norm (Mean): 0.24666693806648254\n",
      "MSE Loss: 0.3832, L1 Loss: 0.01*12.8414\n",
      "Batch number:  98\n",
      "Active Features: 75.35%\n",
      "Decoder Weight Norm (Mean): 0.24673309922218323\n",
      "MSE Loss: 0.3851, L1 Loss: 0.01*12.9026\n",
      "Batch number:  99\n",
      "Active Features: 75.48%\n",
      "Decoder Weight Norm (Mean): 0.24679921567440033\n",
      "MSE Loss: 0.3833, L1 Loss: 0.01*12.8557\n",
      "Batch number:  100\n",
      "Active Features: 75.50%\n",
      "Decoder Weight Norm (Mean): 0.246864914894104\n",
      "MSE Loss: 0.3882, L1 Loss: 0.01*12.8909\n",
      "Batch number:  101\n",
      "Active Features: 73.80%\n",
      "Decoder Weight Norm (Mean): 0.24693097174167633\n",
      "MSE Loss: 0.3798, L1 Loss: 0.01*12.9855\n",
      "Batch number:  102\n",
      "Active Features: 75.85%\n",
      "Decoder Weight Norm (Mean): 0.24699720740318298\n",
      "MSE Loss: 0.3772, L1 Loss: 0.01*12.8184\n",
      "Batch number:  103\n",
      "Active Features: 74.80%\n",
      "Decoder Weight Norm (Mean): 0.24706301093101501\n",
      "MSE Loss: 0.3791, L1 Loss: 0.01*12.9987\n",
      "Batch number:  104\n",
      "Active Features: 73.43%\n",
      "Decoder Weight Norm (Mean): 0.24712860584259033\n",
      "MSE Loss: 0.3822, L1 Loss: 0.01*12.9573\n",
      "Batch number:  105\n",
      "Active Features: 75.38%\n",
      "Decoder Weight Norm (Mean): 0.24719423055648804\n",
      "MSE Loss: 0.3858, L1 Loss: 0.01*12.8777\n",
      "Batch number:  106\n",
      "Active Features: 73.73%\n",
      "Decoder Weight Norm (Mean): 0.24726003408432007\n",
      "MSE Loss: 0.3769, L1 Loss: 0.01*12.9674\n",
      "Batch number:  107\n",
      "Active Features: 74.88%\n",
      "Decoder Weight Norm (Mean): 0.2473255842924118\n",
      "MSE Loss: 0.3839, L1 Loss: 0.01*12.8874\n",
      "Batch number:  108\n",
      "Active Features: 73.30%\n",
      "Decoder Weight Norm (Mean): 0.24739128351211548\n",
      "MSE Loss: 0.3732, L1 Loss: 0.01*12.8964\n",
      "Batch number:  109\n",
      "Active Features: 74.42%\n",
      "Decoder Weight Norm (Mean): 0.24745658040046692\n",
      "MSE Loss: 0.3792, L1 Loss: 0.01*12.7989\n",
      "Batch number:  110\n",
      "Active Features: 75.92%\n",
      "Decoder Weight Norm (Mean): 0.24752162396907806\n",
      "MSE Loss: 0.3923, L1 Loss: 0.01*13.0570\n",
      "Batch number:  111\n",
      "Active Features: 74.38%\n",
      "Decoder Weight Norm (Mean): 0.2475866675376892\n",
      "MSE Loss: 0.3780, L1 Loss: 0.01*12.9518\n",
      "Batch number:  112\n",
      "Active Features: 74.71%\n",
      "Decoder Weight Norm (Mean): 0.2476513683795929\n",
      "MSE Loss: 0.3793, L1 Loss: 0.01*12.8321\n",
      "Batch number:  113\n",
      "Active Features: 74.78%\n",
      "Decoder Weight Norm (Mean): 0.24771654605865479\n",
      "MSE Loss: 0.3893, L1 Loss: 0.01*12.9143\n",
      "Batch number:  114\n",
      "Active Features: 74.66%\n",
      "Decoder Weight Norm (Mean): 0.24778221547603607\n",
      "MSE Loss: 0.3834, L1 Loss: 0.01*12.7846\n",
      "Batch number:  115\n",
      "Active Features: 75.08%\n",
      "Decoder Weight Norm (Mean): 0.24784806370735168\n",
      "MSE Loss: 0.3789, L1 Loss: 0.01*12.8644\n",
      "Batch number:  116\n",
      "Active Features: 74.44%\n",
      "Decoder Weight Norm (Mean): 0.2479141503572464\n",
      "MSE Loss: 0.3801, L1 Loss: 0.01*12.8939\n",
      "Batch number:  117\n",
      "Active Features: 74.83%\n",
      "Decoder Weight Norm (Mean): 0.24798084795475006\n",
      "MSE Loss: 0.3810, L1 Loss: 0.01*12.9720\n",
      "Batch number:  118\n",
      "Active Features: 74.84%\n",
      "Decoder Weight Norm (Mean): 0.24804726243019104\n",
      "MSE Loss: 0.3790, L1 Loss: 0.01*12.7848\n",
      "Batch number:  119\n",
      "Active Features: 73.99%\n",
      "Decoder Weight Norm (Mean): 0.2481136918067932\n",
      "MSE Loss: 0.3829, L1 Loss: 0.01*12.9951\n",
      "Batch number:  120\n",
      "Active Features: 74.61%\n",
      "Decoder Weight Norm (Mean): 0.24818012118339539\n",
      "MSE Loss: 0.3811, L1 Loss: 0.01*12.9098\n",
      "Batch number:  121\n",
      "Active Features: 74.62%\n",
      "Decoder Weight Norm (Mean): 0.2482462376356125\n",
      "MSE Loss: 0.3851, L1 Loss: 0.01*12.9917\n",
      "Batch number:  122\n",
      "Active Features: 73.61%\n",
      "Decoder Weight Norm (Mean): 0.24831229448318481\n",
      "MSE Loss: 0.3769, L1 Loss: 0.01*12.8986\n",
      "Batch number:  123\n",
      "Active Features: 77.44%\n",
      "Decoder Weight Norm (Mean): 0.24837811291217804\n",
      "MSE Loss: 0.3813, L1 Loss: 0.01*13.0308\n",
      "Batch number:  124\n",
      "Active Features: 73.56%\n",
      "Decoder Weight Norm (Mean): 0.24844366312026978\n",
      "MSE Loss: 0.3815, L1 Loss: 0.01*13.0264\n",
      "Batch number:  125\n",
      "Active Features: 74.25%\n",
      "Decoder Weight Norm (Mean): 0.24850858747959137\n",
      "MSE Loss: 0.3775, L1 Loss: 0.01*13.0695\n",
      "Batch number:  126\n",
      "Active Features: 72.33%\n",
      "Decoder Weight Norm (Mean): 0.2485734522342682\n",
      "MSE Loss: 0.3761, L1 Loss: 0.01*12.8912\n",
      "Batch number:  127\n",
      "Active Features: 75.15%\n",
      "Decoder Weight Norm (Mean): 0.24863788485527039\n",
      "MSE Loss: 0.3844, L1 Loss: 0.01*12.9744\n",
      "Epoch [7/8], Loss: 65.9782 ----------------------------\n",
      "Batch number:  0\n",
      "Active Features: 75.44%\n",
      "Decoder Weight Norm (Mean): 0.24870246648788452\n",
      "MSE Loss: 0.3829, L1 Loss: 0.01*13.0897\n",
      "Batch number:  1\n",
      "Active Features: 74.28%\n",
      "Decoder Weight Norm (Mean): 0.2487664520740509\n",
      "MSE Loss: 0.3711, L1 Loss: 0.01*12.9944\n",
      "Batch number:  2\n",
      "Active Features: 74.19%\n",
      "Decoder Weight Norm (Mean): 0.2488301694393158\n",
      "MSE Loss: 0.3829, L1 Loss: 0.01*12.9491\n",
      "Batch number:  3\n",
      "Active Features: 73.26%\n",
      "Decoder Weight Norm (Mean): 0.24889430403709412\n",
      "MSE Loss: 0.3728, L1 Loss: 0.01*13.0833\n",
      "Batch number:  4\n",
      "Active Features: 71.47%\n",
      "Decoder Weight Norm (Mean): 0.24895839393138885\n",
      "MSE Loss: 0.3759, L1 Loss: 0.01*12.8982\n",
      "Batch number:  5\n",
      "Active Features: 73.11%\n",
      "Decoder Weight Norm (Mean): 0.2490226924419403\n",
      "MSE Loss: 0.3791, L1 Loss: 0.01*13.1051\n",
      "Batch number:  6\n",
      "Active Features: 74.89%\n",
      "Decoder Weight Norm (Mean): 0.24908746778964996\n",
      "MSE Loss: 0.3754, L1 Loss: 0.01*12.8481\n",
      "Batch number:  7\n",
      "Active Features: 75.26%\n",
      "Decoder Weight Norm (Mean): 0.24915218353271484\n",
      "MSE Loss: 0.3805, L1 Loss: 0.01*13.1305\n",
      "Batch number:  8\n",
      "Active Features: 75.12%\n",
      "Decoder Weight Norm (Mean): 0.24921691417694092\n",
      "MSE Loss: 0.3813, L1 Loss: 0.01*12.8932\n",
      "Batch number:  9\n",
      "Active Features: 73.58%\n",
      "Decoder Weight Norm (Mean): 0.24928177893161774\n",
      "MSE Loss: 0.3737, L1 Loss: 0.01*12.9359\n",
      "Batch number:  10\n",
      "Active Features: 74.83%\n",
      "Decoder Weight Norm (Mean): 0.24934647977352142\n",
      "MSE Loss: 0.3818, L1 Loss: 0.01*12.9277\n",
      "Batch number:  11\n",
      "Active Features: 72.76%\n",
      "Decoder Weight Norm (Mean): 0.24941129982471466\n",
      "MSE Loss: 0.3768, L1 Loss: 0.01*13.0200\n",
      "Batch number:  12\n",
      "Active Features: 71.44%\n",
      "Decoder Weight Norm (Mean): 0.2494761198759079\n",
      "MSE Loss: 0.3761, L1 Loss: 0.01*12.9182\n",
      "Batch number:  13\n",
      "Active Features: 73.66%\n",
      "Decoder Weight Norm (Mean): 0.24954110383987427\n",
      "MSE Loss: 0.3774, L1 Loss: 0.01*12.8709\n",
      "Batch number:  14\n",
      "Active Features: 74.44%\n",
      "Decoder Weight Norm (Mean): 0.24960575997829437\n",
      "MSE Loss: 0.3772, L1 Loss: 0.01*12.8276\n",
      "Batch number:  15\n",
      "Active Features: 73.22%\n",
      "Decoder Weight Norm (Mean): 0.24967023730278015\n",
      "MSE Loss: 0.3779, L1 Loss: 0.01*12.9501\n",
      "Batch number:  16\n",
      "Active Features: 73.59%\n",
      "Decoder Weight Norm (Mean): 0.2497347593307495\n",
      "MSE Loss: 0.3844, L1 Loss: 0.01*13.1828\n",
      "Batch number:  17\n",
      "Active Features: 73.31%\n",
      "Decoder Weight Norm (Mean): 0.24979950487613678\n",
      "MSE Loss: 0.3820, L1 Loss: 0.01*13.0184\n",
      "Batch number:  18\n",
      "Active Features: 73.30%\n",
      "Decoder Weight Norm (Mean): 0.24986425042152405\n",
      "MSE Loss: 0.3796, L1 Loss: 0.01*12.9899\n",
      "Batch number:  19\n",
      "Active Features: 72.43%\n",
      "Decoder Weight Norm (Mean): 0.24992923438549042\n",
      "MSE Loss: 0.3795, L1 Loss: 0.01*12.9005\n",
      "Batch number:  20\n",
      "Active Features: 72.27%\n",
      "Decoder Weight Norm (Mean): 0.24999389052391052\n",
      "MSE Loss: 0.3776, L1 Loss: 0.01*12.9796\n",
      "Batch number:  21\n",
      "Active Features: 72.82%\n",
      "Decoder Weight Norm (Mean): 0.25005820393562317\n",
      "MSE Loss: 0.3784, L1 Loss: 0.01*12.9069\n",
      "Batch number:  22\n",
      "Active Features: 72.78%\n",
      "Decoder Weight Norm (Mean): 0.2501224875450134\n",
      "MSE Loss: 0.3760, L1 Loss: 0.01*12.8725\n",
      "Batch number:  23\n",
      "Active Features: 74.98%\n",
      "Decoder Weight Norm (Mean): 0.2501867711544037\n",
      "MSE Loss: 0.3829, L1 Loss: 0.01*13.0724\n",
      "Batch number:  24\n",
      "Active Features: 72.32%\n",
      "Decoder Weight Norm (Mean): 0.2502511441707611\n",
      "MSE Loss: 0.3810, L1 Loss: 0.01*13.1422\n",
      "Batch number:  25\n",
      "Active Features: 70.95%\n",
      "Decoder Weight Norm (Mean): 0.25031545758247375\n",
      "MSE Loss: 0.3740, L1 Loss: 0.01*12.9488\n",
      "Batch number:  26\n",
      "Active Features: 72.21%\n",
      "Decoder Weight Norm (Mean): 0.25038012862205505\n",
      "MSE Loss: 0.3694, L1 Loss: 0.01*12.8165\n",
      "Batch number:  27\n",
      "Active Features: 72.08%\n",
      "Decoder Weight Norm (Mean): 0.250445157289505\n",
      "MSE Loss: 0.3721, L1 Loss: 0.01*12.9533\n",
      "Batch number:  28\n",
      "Active Features: 70.22%\n",
      "Decoder Weight Norm (Mean): 0.2505100965499878\n",
      "MSE Loss: 0.3759, L1 Loss: 0.01*12.7821\n",
      "Batch number:  29\n",
      "Active Features: 74.74%\n",
      "Decoder Weight Norm (Mean): 0.25057515501976013\n",
      "MSE Loss: 0.3786, L1 Loss: 0.01*12.8387\n",
      "Batch number:  30\n",
      "Active Features: 71.13%\n",
      "Decoder Weight Norm (Mean): 0.2506398558616638\n",
      "MSE Loss: 0.3729, L1 Loss: 0.01*13.0392\n",
      "Batch number:  31\n",
      "Active Features: 73.41%\n",
      "Decoder Weight Norm (Mean): 0.2507041096687317\n",
      "MSE Loss: 0.3780, L1 Loss: 0.01*12.9098\n",
      "Batch number:  32\n",
      "Active Features: 71.65%\n",
      "Decoder Weight Norm (Mean): 0.2507678270339966\n",
      "MSE Loss: 0.3737, L1 Loss: 0.01*13.1563\n",
      "Batch number:  33\n",
      "Active Features: 73.11%\n",
      "Decoder Weight Norm (Mean): 0.25083115696907043\n",
      "MSE Loss: 0.3778, L1 Loss: 0.01*12.9632\n",
      "Batch number:  34\n",
      "Active Features: 68.95%\n",
      "Decoder Weight Norm (Mean): 0.2508942782878876\n",
      "MSE Loss: 0.3730, L1 Loss: 0.01*13.0433\n",
      "Batch number:  35\n",
      "Active Features: 71.36%\n",
      "Decoder Weight Norm (Mean): 0.2509572207927704\n",
      "MSE Loss: 0.3711, L1 Loss: 0.01*12.8018\n",
      "Batch number:  36\n",
      "Active Features: 74.67%\n",
      "Decoder Weight Norm (Mean): 0.2510201334953308\n",
      "MSE Loss: 0.3780, L1 Loss: 0.01*13.1717\n",
      "Batch number:  37\n",
      "Active Features: 70.25%\n",
      "Decoder Weight Norm (Mean): 0.2510825991630554\n",
      "MSE Loss: 0.3798, L1 Loss: 0.01*13.0413\n",
      "Batch number:  38\n",
      "Active Features: 71.95%\n",
      "Decoder Weight Norm (Mean): 0.2511453926563263\n",
      "MSE Loss: 0.3773, L1 Loss: 0.01*13.0215\n",
      "Batch number:  39\n",
      "Active Features: 71.11%\n",
      "Decoder Weight Norm (Mean): 0.2512083947658539\n",
      "MSE Loss: 0.3761, L1 Loss: 0.01*12.9301\n",
      "Batch number:  40\n",
      "Active Features: 72.96%\n",
      "Decoder Weight Norm (Mean): 0.25127148628234863\n",
      "MSE Loss: 0.3742, L1 Loss: 0.01*13.0924\n",
      "Batch number:  41\n",
      "Active Features: 71.71%\n",
      "Decoder Weight Norm (Mean): 0.2513345181941986\n",
      "MSE Loss: 0.3744, L1 Loss: 0.01*13.0308\n",
      "Batch number:  42\n",
      "Active Features: 71.97%\n",
      "Decoder Weight Norm (Mean): 0.2513975501060486\n",
      "MSE Loss: 0.3755, L1 Loss: 0.01*12.9415\n",
      "Batch number:  43\n",
      "Active Features: 72.04%\n",
      "Decoder Weight Norm (Mean): 0.25146064162254333\n",
      "MSE Loss: 0.3716, L1 Loss: 0.01*12.9701\n",
      "Batch number:  44\n",
      "Active Features: 75.28%\n",
      "Decoder Weight Norm (Mean): 0.25152361392974854\n",
      "MSE Loss: 0.3777, L1 Loss: 0.01*12.9555\n",
      "Batch number:  45\n",
      "Active Features: 73.09%\n",
      "Decoder Weight Norm (Mean): 0.25158631801605225\n",
      "MSE Loss: 0.3777, L1 Loss: 0.01*13.0255\n",
      "Batch number:  46\n",
      "Active Features: 73.55%\n",
      "Decoder Weight Norm (Mean): 0.2516491115093231\n",
      "MSE Loss: 0.3789, L1 Loss: 0.01*13.1172\n",
      "Batch number:  47\n",
      "Active Features: 71.46%\n",
      "Decoder Weight Norm (Mean): 0.25171199440956116\n",
      "MSE Loss: 0.3683, L1 Loss: 0.01*12.9759\n",
      "Batch number:  48\n",
      "Active Features: 71.95%\n",
      "Decoder Weight Norm (Mean): 0.25177475810050964\n",
      "MSE Loss: 0.3744, L1 Loss: 0.01*13.1147\n",
      "Batch number:  49\n",
      "Active Features: 71.44%\n",
      "Decoder Weight Norm (Mean): 0.2518375515937805\n",
      "MSE Loss: 0.3750, L1 Loss: 0.01*12.9720\n",
      "Batch number:  50\n",
      "Active Features: 72.20%\n",
      "Decoder Weight Norm (Mean): 0.25190088152885437\n",
      "MSE Loss: 0.3701, L1 Loss: 0.01*12.9952\n",
      "Batch number:  51\n",
      "Active Features: 73.66%\n",
      "Decoder Weight Norm (Mean): 0.25196439027786255\n",
      "MSE Loss: 0.3747, L1 Loss: 0.01*13.0222\n",
      "Batch number:  52\n",
      "Active Features: 70.80%\n",
      "Decoder Weight Norm (Mean): 0.25202786922454834\n",
      "MSE Loss: 0.3704, L1 Loss: 0.01*13.0775\n",
      "Batch number:  53\n",
      "Active Features: 72.98%\n",
      "Decoder Weight Norm (Mean): 0.25209128856658936\n",
      "MSE Loss: 0.3736, L1 Loss: 0.01*12.9556\n",
      "Batch number:  54\n",
      "Active Features: 73.45%\n",
      "Decoder Weight Norm (Mean): 0.2521548867225647\n",
      "MSE Loss: 0.3696, L1 Loss: 0.01*12.9560\n",
      "Batch number:  55\n",
      "Active Features: 73.30%\n",
      "Decoder Weight Norm (Mean): 0.25221794843673706\n",
      "MSE Loss: 0.3755, L1 Loss: 0.01*13.0240\n",
      "Batch number:  56\n",
      "Active Features: 70.16%\n",
      "Decoder Weight Norm (Mean): 0.25228071212768555\n",
      "MSE Loss: 0.3691, L1 Loss: 0.01*12.9136\n",
      "Batch number:  57\n",
      "Active Features: 72.39%\n",
      "Decoder Weight Norm (Mean): 0.2523433566093445\n",
      "MSE Loss: 0.3808, L1 Loss: 0.01*13.0780\n",
      "Batch number:  58\n",
      "Active Features: 70.82%\n",
      "Decoder Weight Norm (Mean): 0.2524062991142273\n",
      "MSE Loss: 0.3704, L1 Loss: 0.01*13.1510\n",
      "Batch number:  59\n",
      "Active Features: 71.49%\n",
      "Decoder Weight Norm (Mean): 0.2524692416191101\n",
      "MSE Loss: 0.3674, L1 Loss: 0.01*13.0096\n",
      "Batch number:  60\n",
      "Active Features: 71.81%\n",
      "Decoder Weight Norm (Mean): 0.25253206491470337\n",
      "MSE Loss: 0.3704, L1 Loss: 0.01*13.1822\n",
      "Batch number:  61\n",
      "Active Features: 71.25%\n",
      "Decoder Weight Norm (Mean): 0.2525947093963623\n",
      "MSE Loss: 0.3727, L1 Loss: 0.01*12.9973\n",
      "Batch number:  62\n",
      "Active Features: 71.00%\n",
      "Decoder Weight Norm (Mean): 0.25265711545944214\n",
      "MSE Loss: 0.3720, L1 Loss: 0.01*12.9842\n",
      "Batch number:  63\n",
      "Active Features: 71.41%\n",
      "Decoder Weight Norm (Mean): 0.25271958112716675\n",
      "MSE Loss: 0.3734, L1 Loss: 0.01*13.0491\n",
      "Batch number:  64\n",
      "Active Features: 73.16%\n",
      "Decoder Weight Norm (Mean): 0.25278183817863464\n",
      "MSE Loss: 0.3725, L1 Loss: 0.01*12.9369\n",
      "Batch number:  65\n",
      "Active Features: 70.22%\n",
      "Decoder Weight Norm (Mean): 0.2528439462184906\n",
      "MSE Loss: 0.3762, L1 Loss: 0.01*13.0983\n",
      "Batch number:  66\n",
      "Active Features: 68.59%\n",
      "Decoder Weight Norm (Mean): 0.25290605425834656\n",
      "MSE Loss: 0.3689, L1 Loss: 0.01*13.0709\n",
      "Batch number:  67\n",
      "Active Features: 71.89%\n",
      "Decoder Weight Norm (Mean): 0.2529679238796234\n",
      "MSE Loss: 0.3717, L1 Loss: 0.01*13.1246\n",
      "Batch number:  68\n",
      "Active Features: 72.06%\n",
      "Decoder Weight Norm (Mean): 0.2530294954776764\n",
      "MSE Loss: 0.3730, L1 Loss: 0.01*12.9576\n",
      "Batch number:  69\n",
      "Active Features: 70.88%\n",
      "Decoder Weight Norm (Mean): 0.25309109687805176\n",
      "MSE Loss: 0.3713, L1 Loss: 0.01*12.9878\n",
      "Batch number:  70\n",
      "Active Features: 70.91%\n",
      "Decoder Weight Norm (Mean): 0.25315263867378235\n",
      "MSE Loss: 0.3700, L1 Loss: 0.01*12.9594\n",
      "Batch number:  71\n",
      "Active Features: 70.17%\n",
      "Decoder Weight Norm (Mean): 0.2532142996788025\n",
      "MSE Loss: 0.3700, L1 Loss: 0.01*13.0137\n",
      "Batch number:  72\n",
      "Active Features: 70.93%\n",
      "Decoder Weight Norm (Mean): 0.2532763183116913\n",
      "MSE Loss: 0.3776, L1 Loss: 0.01*13.1306\n",
      "Batch number:  73\n",
      "Active Features: 69.25%\n",
      "Decoder Weight Norm (Mean): 0.25333815813064575\n",
      "MSE Loss: 0.3704, L1 Loss: 0.01*12.9446\n",
      "Batch number:  74\n",
      "Active Features: 70.41%\n",
      "Decoder Weight Norm (Mean): 0.2534000873565674\n",
      "MSE Loss: 0.3655, L1 Loss: 0.01*13.0643\n",
      "Batch number:  75\n",
      "Active Features: 71.75%\n",
      "Decoder Weight Norm (Mean): 0.25346195697784424\n",
      "MSE Loss: 0.3714, L1 Loss: 0.01*12.9560\n",
      "Batch number:  76\n",
      "Active Features: 70.88%\n",
      "Decoder Weight Norm (Mean): 0.25352421402931213\n",
      "MSE Loss: 0.3693, L1 Loss: 0.01*13.0167\n",
      "Batch number:  77\n",
      "Active Features: 73.77%\n",
      "Decoder Weight Norm (Mean): 0.25358590483665466\n",
      "MSE Loss: 0.3748, L1 Loss: 0.01*13.0240\n",
      "Batch number:  78\n",
      "Active Features: 71.58%\n",
      "Decoder Weight Norm (Mean): 0.2536475658416748\n",
      "MSE Loss: 0.3672, L1 Loss: 0.01*13.0557\n",
      "Batch number:  79\n",
      "Active Features: 71.34%\n",
      "Decoder Weight Norm (Mean): 0.25370925664901733\n",
      "MSE Loss: 0.3712, L1 Loss: 0.01*13.0189\n",
      "Batch number:  80\n",
      "Active Features: 68.56%\n",
      "Decoder Weight Norm (Mean): 0.2537706196308136\n",
      "MSE Loss: 0.3699, L1 Loss: 0.01*13.1057\n",
      "Batch number:  81\n",
      "Active Features: 70.44%\n",
      "Decoder Weight Norm (Mean): 0.25383201241493225\n",
      "MSE Loss: 0.3693, L1 Loss: 0.01*12.9947\n",
      "Batch number:  82\n",
      "Active Features: 71.38%\n",
      "Decoder Weight Norm (Mean): 0.25389355421066284\n",
      "MSE Loss: 0.3693, L1 Loss: 0.01*12.9863\n",
      "Batch number:  83\n",
      "Active Features: 72.70%\n",
      "Decoder Weight Norm (Mean): 0.2539548873901367\n",
      "MSE Loss: 0.3727, L1 Loss: 0.01*12.9719\n",
      "Batch number:  84\n",
      "Active Features: 69.20%\n",
      "Decoder Weight Norm (Mean): 0.2540164589881897\n",
      "MSE Loss: 0.3594, L1 Loss: 0.01*12.8197\n",
      "Batch number:  85\n",
      "Active Features: 69.84%\n",
      "Decoder Weight Norm (Mean): 0.2540779709815979\n",
      "MSE Loss: 0.3704, L1 Loss: 0.01*13.1773\n",
      "Batch number:  86\n",
      "Active Features: 70.39%\n",
      "Decoder Weight Norm (Mean): 0.2541389465332031\n",
      "MSE Loss: 0.3639, L1 Loss: 0.01*13.0020\n",
      "Batch number:  87\n",
      "Active Features: 69.42%\n",
      "Decoder Weight Norm (Mean): 0.2541997730731964\n",
      "MSE Loss: 0.3672, L1 Loss: 0.01*13.0699\n",
      "Batch number:  88\n",
      "Active Features: 69.11%\n",
      "Decoder Weight Norm (Mean): 0.2542608678340912\n",
      "MSE Loss: 0.3687, L1 Loss: 0.01*13.0225\n",
      "Batch number:  89\n",
      "Active Features: 69.34%\n",
      "Decoder Weight Norm (Mean): 0.2543221712112427\n",
      "MSE Loss: 0.3707, L1 Loss: 0.01*13.1467\n",
      "Batch number:  90\n",
      "Active Features: 71.31%\n",
      "Decoder Weight Norm (Mean): 0.2543833553791046\n",
      "MSE Loss: 0.3717, L1 Loss: 0.01*13.0118\n",
      "Batch number:  91\n",
      "Active Features: 69.24%\n",
      "Decoder Weight Norm (Mean): 0.2544443905353546\n",
      "MSE Loss: 0.3643, L1 Loss: 0.01*12.9750\n",
      "Batch number:  92\n",
      "Active Features: 68.30%\n",
      "Decoder Weight Norm (Mean): 0.25450563430786133\n",
      "MSE Loss: 0.3712, L1 Loss: 0.01*13.1781\n",
      "Batch number:  93\n",
      "Active Features: 69.23%\n",
      "Decoder Weight Norm (Mean): 0.2545672357082367\n",
      "MSE Loss: 0.3673, L1 Loss: 0.01*12.9936\n",
      "Batch number:  94\n",
      "Active Features: 68.55%\n",
      "Decoder Weight Norm (Mean): 0.25462961196899414\n",
      "MSE Loss: 0.3719, L1 Loss: 0.01*13.1164\n",
      "Batch number:  95\n",
      "Active Features: 67.46%\n",
      "Decoder Weight Norm (Mean): 0.254692405462265\n",
      "MSE Loss: 0.3705, L1 Loss: 0.01*13.2339\n",
      "Batch number:  96\n",
      "Active Features: 70.11%\n",
      "Decoder Weight Norm (Mean): 0.2547551691532135\n",
      "MSE Loss: 0.3733, L1 Loss: 0.01*13.1504\n",
      "Batch number:  97\n",
      "Active Features: 69.45%\n",
      "Decoder Weight Norm (Mean): 0.25481805205345154\n",
      "MSE Loss: 0.3697, L1 Loss: 0.01*13.0843\n",
      "Batch number:  98\n",
      "Active Features: 69.44%\n",
      "Decoder Weight Norm (Mean): 0.2548809051513672\n",
      "MSE Loss: 0.3687, L1 Loss: 0.01*12.9874\n",
      "Batch number:  99\n",
      "Active Features: 69.88%\n",
      "Decoder Weight Norm (Mean): 0.25494369864463806\n",
      "MSE Loss: 0.3714, L1 Loss: 0.01*13.1325\n",
      "Batch number:  100\n",
      "Active Features: 69.19%\n",
      "Decoder Weight Norm (Mean): 0.2550063133239746\n",
      "MSE Loss: 0.3735, L1 Loss: 0.01*13.0754\n",
      "Batch number:  101\n",
      "Active Features: 68.69%\n",
      "Decoder Weight Norm (Mean): 0.2550692856311798\n",
      "MSE Loss: 0.3716, L1 Loss: 0.01*13.1582\n",
      "Batch number:  102\n",
      "Active Features: 69.88%\n",
      "Decoder Weight Norm (Mean): 0.25513240694999695\n",
      "MSE Loss: 0.3656, L1 Loss: 0.01*12.8180\n",
      "Batch number:  103\n",
      "Active Features: 68.61%\n",
      "Decoder Weight Norm (Mean): 0.2551952302455902\n",
      "MSE Loss: 0.3682, L1 Loss: 0.01*13.1933\n",
      "Batch number:  104\n",
      "Active Features: 70.45%\n",
      "Decoder Weight Norm (Mean): 0.2552577555179596\n",
      "MSE Loss: 0.3638, L1 Loss: 0.01*12.9973\n",
      "Batch number:  105\n",
      "Active Features: 70.81%\n",
      "Decoder Weight Norm (Mean): 0.2553198039531708\n",
      "MSE Loss: 0.3725, L1 Loss: 0.01*13.0269\n",
      "Batch number:  106\n",
      "Active Features: 67.19%\n",
      "Decoder Weight Norm (Mean): 0.2553815543651581\n",
      "MSE Loss: 0.3617, L1 Loss: 0.01*13.1252\n",
      "Batch number:  107\n",
      "Active Features: 69.82%\n",
      "Decoder Weight Norm (Mean): 0.2554430067539215\n",
      "MSE Loss: 0.3655, L1 Loss: 0.01*12.9551\n",
      "Batch number:  108\n",
      "Active Features: 66.70%\n",
      "Decoder Weight Norm (Mean): 0.2555043399333954\n",
      "MSE Loss: 0.3647, L1 Loss: 0.01*13.1454\n",
      "Batch number:  109\n",
      "Active Features: 69.69%\n",
      "Decoder Weight Norm (Mean): 0.25556546449661255\n",
      "MSE Loss: 0.3662, L1 Loss: 0.01*13.0647\n",
      "Batch number:  110\n",
      "Active Features: 70.72%\n",
      "Decoder Weight Norm (Mean): 0.2556265890598297\n",
      "MSE Loss: 0.3761, L1 Loss: 0.01*13.1051\n",
      "Batch number:  111\n",
      "Active Features: 68.95%\n",
      "Decoder Weight Norm (Mean): 0.2556875944137573\n",
      "MSE Loss: 0.3686, L1 Loss: 0.01*13.2296\n",
      "Batch number:  112\n",
      "Active Features: 70.39%\n",
      "Decoder Weight Norm (Mean): 0.2557484209537506\n",
      "MSE Loss: 0.3663, L1 Loss: 0.01*13.0262\n",
      "Batch number:  113\n",
      "Active Features: 66.92%\n",
      "Decoder Weight Norm (Mean): 0.25580957531929016\n",
      "MSE Loss: 0.3742, L1 Loss: 0.01*13.1904\n",
      "Batch number:  114\n",
      "Active Features: 69.84%\n",
      "Decoder Weight Norm (Mean): 0.2558709383010864\n",
      "MSE Loss: 0.3711, L1 Loss: 0.01*12.9440\n",
      "Batch number:  115\n",
      "Active Features: 68.68%\n",
      "Decoder Weight Norm (Mean): 0.2559322714805603\n",
      "MSE Loss: 0.3671, L1 Loss: 0.01*13.0699\n",
      "Batch number:  116\n",
      "Active Features: 69.36%\n",
      "Decoder Weight Norm (Mean): 0.2559937536716461\n",
      "MSE Loss: 0.3622, L1 Loss: 0.01*13.0421\n",
      "Batch number:  117\n",
      "Active Features: 70.42%\n",
      "Decoder Weight Norm (Mean): 0.2560553550720215\n",
      "MSE Loss: 0.3706, L1 Loss: 0.01*13.1635\n",
      "Batch number:  118\n",
      "Active Features: 68.41%\n",
      "Decoder Weight Norm (Mean): 0.256116658449173\n",
      "MSE Loss: 0.3678, L1 Loss: 0.01*12.9959\n",
      "Batch number:  119\n",
      "Active Features: 68.75%\n",
      "Decoder Weight Norm (Mean): 0.2561776340007782\n",
      "MSE Loss: 0.3700, L1 Loss: 0.01*13.0899\n",
      "Batch number:  120\n",
      "Active Features: 67.62%\n",
      "Decoder Weight Norm (Mean): 0.2562384605407715\n",
      "MSE Loss: 0.3673, L1 Loss: 0.01*13.0707\n",
      "Batch number:  121\n",
      "Active Features: 68.22%\n",
      "Decoder Weight Norm (Mean): 0.2562991678714752\n",
      "MSE Loss: 0.3704, L1 Loss: 0.01*13.1948\n",
      "Batch number:  122\n",
      "Active Features: 68.49%\n",
      "Decoder Weight Norm (Mean): 0.2563599646091461\n",
      "MSE Loss: 0.3652, L1 Loss: 0.01*12.9728\n",
      "Batch number:  123\n",
      "Active Features: 69.14%\n",
      "Decoder Weight Norm (Mean): 0.2564208507537842\n",
      "MSE Loss: 0.3668, L1 Loss: 0.01*13.1619\n",
      "Batch number:  124\n",
      "Active Features: 67.12%\n",
      "Decoder Weight Norm (Mean): 0.25648176670074463\n",
      "MSE Loss: 0.3670, L1 Loss: 0.01*13.1964\n",
      "Batch number:  125\n",
      "Active Features: 70.45%\n",
      "Decoder Weight Norm (Mean): 0.25654301047325134\n",
      "MSE Loss: 0.3619, L1 Loss: 0.01*13.1539\n",
      "Batch number:  126\n",
      "Active Features: 67.02%\n",
      "Decoder Weight Norm (Mean): 0.25660404562950134\n",
      "MSE Loss: 0.3659, L1 Loss: 0.01*13.1910\n",
      "Batch number:  127\n",
      "Active Features: 68.51%\n",
      "Decoder Weight Norm (Mean): 0.25666502118110657\n",
      "MSE Loss: 0.3686, L1 Loss: 0.01*13.0597\n",
      "Epoch [8/8], Loss: 64.3474 ----------------------------\n",
      "Time taken: 1332.54 seconds\n"
     ]
    }
   ],
   "source": [
    "# Create dataset and dataloader\n",
    "data_dir = \"activations_data\"\n",
    "dataset = ActivationDataset(\n",
    "    data_dir, \n",
    "    batch_size=2048, # 8192 examples per batch\n",
    "    f_type=\"train\", \n",
    "    test_fraction=0.01, \n",
    "    scale_factor=scale_factor, \n",
    "    seed=42\n",
    ")\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=False) # take 1 batch at a time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize model parameters\n",
    "input_dim = 3072  \n",
    "hidden_dim = 20000 # = 4096  # Adjust based on your requirements\n",
    "\n",
    "# Initialize the model\n",
    "model = SparseAutoencoder(input_dim, hidden_dim).to(device)\n",
    "\n",
    "# Set up loss function and optimizer\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "l1_lambda = 0.01  # Regularization strength for sparsity\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 8 # should be 1 \n",
    "\n",
    "print(f\"Training Parameters:\\n\"\n",
    "      f\"Data Directory: {data_dir}\\n\"\n",
    "      f\"Batch Size: {dataset.batch_size}\\n\"\n",
    "      f\"Test Fraction: {dataset.test_fraction}\\n\"\n",
    "      f\"Scale Factor: {scale_factor}\\n\"\n",
    "      f\"Seed: {dataset.seed}\\n\"\n",
    "      f\"Optimizer: {optimizer.__class__.__name__}\\n\"\n",
    "      f\"Learning Rate: {optimizer.param_groups[0]['lr']}\\n\"\n",
    "      f\"L1 Lambda: {l1_lambda}\\n\"\n",
    "      f\"Number of Epochs: {num_epochs}\\n\"\n",
    "      f\"SAE Input Dimension: {input_dim}\\n\"\n",
    "      f\"SAE Hidden Dimension: {hidden_dim}\\n\"\n",
    "      f\"-----------------------------------\"\n",
    ")\n",
    "time_start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    # dataset.seed = epoch # change seed for each epoch to get different samples\n",
    "    dataset.seed = epoch//4  # this effectively does: batch_size * 4 , and epoch / 4\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        print(\"Batch number: \", i)\n",
    "        # batch is already of shape (1, 8192, 3072)        \n",
    "        # Forward pass\n",
    "        batch = batch.to(device)\n",
    "        outputs, encoded = model(batch) # remove extra dimension from DataLoader\n",
    "        mse_loss = criterion(outputs, batch)\n",
    "\n",
    "        # Active features calculation\n",
    "        # Count how many latent features (columns) are active (nonzero for any token in the batch)\n",
    "        active_features = torch.any(encoded > 0, dim=1).sum().item()\n",
    "        total_features = encoded.shape[2]  # Number of latent features (4096 in this case)\n",
    "        active_percentage = active_features / total_features * 100\n",
    "        # print(f\"Encoded Activation Range: {torch.min(encoded).item()}, {torch.max(encoded).item()}\")\n",
    "        print(f\"Active Features: {active_percentage:.2f}%\")\n",
    "\n",
    "        # Add L1 regularization for sparsity\n",
    "        # decoder_weight_norms = torch.norm(model.decoder.weight, 2, dim=0)\n",
    "        # l1_loss = torch.norm(encoded, 1) * decoder_weight_norms.sum()\n",
    "        # l1_loss = torch.sum(torch.norm(model.decoder.weight, 2, dim=0) * torch.norm(encoded, 1, dim=(0, 1)))\n",
    "        decoder_weight_norms = torch.norm(model.decoder.weight, p=2, dim=0)  # Shape: [num_features]\n",
    "        l1_terms = encoded * decoder_weight_norms.unsqueeze(0)  # Shape: [batch_size, num_features]\n",
    "        l1_loss_per_sample = torch.sum(l1_terms, dim=1)  # Shape: [batch_size]\n",
    "        l1_loss = torch.mean(l1_loss_per_sample)\n",
    "\n",
    "        loss = mse_loss + l1_lambda * l1_loss\n",
    "\n",
    "        print(f\"Decoder Weight Norm (Mean): {torch.norm(model.decoder.weight, dim=0).mean().item()}\")\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        print(f\"MSE Loss: {mse_loss.item():.4f}, L1 Loss: {l1_lambda}*{l1_loss.item():.4f}\")\n",
    "        # if mse_loss.item() < 0.1:\n",
    "        #     optimizer.lr = 0.001\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f} ----------------------------\")\n",
    "time_end = time.time()\n",
    "print(f\"Time taken: {time_end - time_start:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(265305.7500, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.norm(model.decoder.weight, 2, dim=0) * torch.norm(encoded, 1, dim=(0, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.6911e+09, device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_weight_norms = torch.norm(model.decoder.weight, 2, dim=0)\n",
    "torch.norm(encoded, 1) * decoder_weight_norms.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.2653, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_weight_norms = torch.norm(model.decoder.weight, p=2, dim=0)  # Shape: [num_features]\n",
    "\n",
    "# Compute L1 loss\n",
    "l1_terms = encoded * decoder_weight_norms.unsqueeze(0)  # Shape: [batch_size, num_features]\n",
    "l1_loss_per_sample = torch.sum(l1_terms, dim=1)  # Shape: [batch_size]\n",
    "l1_loss = torch.mean(l1_loss_per_sample)\n",
    "l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3471, device='cuda:0', grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.functional.mse_loss(outputs, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5674, -0.0768,  0.6607,  ..., -0.2379, -0.5708,  0.4291],\n",
       "         [ 0.1992, -1.6754, -0.8184,  ..., -0.6283, -0.5145,  0.9914],\n",
       "         [-0.3335, -0.3261, -3.3714,  ..., -0.3588, -0.4223, -1.9554],\n",
       "         ...,\n",
       "         [-0.5190, -0.0219, -1.9429,  ..., -0.2146, -0.3944, -1.6162],\n",
       "         [-0.1195, -1.0984,  0.1002,  ..., -0.5987, -0.2709, -0.4835],\n",
       "         [ 0.6391, -0.0174, -1.4922,  ..., -2.1341, -0.9686,  0.6579]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149 174 394 465 594 595 676 686 889 959 1118 1178 1192 1279 1294 1386 1473 1491 1544 1592 1665 1831 1842 1882 1924 2052 2293 2303 2390 2508 2547 2566 2587 2703 2724 2826 2831 2877 2960 2996 3063 3100 3243 3320 3517 3585 3672 3731 3760 3792 4159 4186 4258 4294 4311 4363 4592 4812 4895 4911 4925 4955 5074 5146 5392 5591 5749 6032 6059 6373 6400 6513 6785 6831 7016 7149 7416 7546 7608 7614 7741 7818 7848 7856 7863 7869 7996 8045 8078 8122 8305 8371 8379 8521 8551 8627 8634 8641 8666 8681 8872 8873 8987 9076 9148 9268 9286 9370 9649 9741 9776 9787 9823 9854 9919 9937 10082 10791 10831 11048 11431 11576 11715 11745 11766 11879 11918 11924 11981 12305 12365 12438 12450 12513 12530 12538 12552 12577 12771 12776 12951 12961 13261 13341 13499 13643 13693 13694 13720 13822 14040 14085 14089 14139 14202 14333 14374 14395 14457 14582 14858 14963 15084 15156 15407 15544 15610 15623 15636 15696 15753 15951 16006 16013 16458 16507 16566 16644 16857 16955 16975 17085 17342 17375 17436 17642 17675 17752 17822 17888 17947 17968 18032 18315 18472 18483 18753 18820 18838 18911 18997 19007 19099 19289 19321 19341 19437 19566 19690 19764 19785 19810 19876 "
     ]
    }
   ],
   "source": [
    "for i, e in enumerate(encoded[0][0].tolist()):\n",
    "    if e > 0:\n",
    "        print(i, end=\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.7803e-01, -3.0534e-01,  7.9855e-01,  ..., -2.0366e-01,\n",
       "          -1.1266e+00,  2.1898e-01],\n",
       "         [ 1.5722e-02, -1.0040e+00, -4.0356e-01,  ..., -8.4667e-01,\n",
       "           2.6732e-01,  3.1934e-01],\n",
       "         [ 1.6639e-01, -5.1605e-01, -1.8754e+00,  ..., -1.4928e-01,\n",
       "          -3.2643e-01, -5.0592e-01],\n",
       "         ...,\n",
       "         [-1.7158e-01,  2.4333e-01, -2.4313e+00,  ..., -4.6648e-01,\n",
       "          -8.6239e-01, -7.5687e-01],\n",
       "         [-1.6957e-01, -3.8168e-01,  6.1277e-04,  ..., -7.5876e-01,\n",
       "          -4.5029e-01, -5.1025e-02],\n",
       "         [ 4.9176e-01, -6.6356e-01, -1.0524e+00,  ..., -9.8739e-01,\n",
       "          -1.2001e+00,  5.3279e-01]]], device='cuda:0',\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3471, device='cuda:0', grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(outputs,batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), f\"models/sparse_autoencoder_tmp.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model deleted and GPU memory freed\n"
     ]
    }
   ],
   "source": [
    "# clear gpu\n",
    "del model\n",
    "del data_loader\n",
    "del dataset\n",
    "del criterion\n",
    "del optimizer\n",
    "del batch\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Model deleted and GPU memory freed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"activations_data\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_dim = 3072  \n",
    "hidden_dim = 65536\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30077/1824788391.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"models/checkpoint\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_dim': 3072, 'hidden_dim': 65536, 'l1_lambda': 0.00597965, 'lr': 2.5011e-05}\n"
     ]
    }
   ],
   "source": [
    "model = SparseAutoencoder(input_dim, hidden_dim).to(device)\n",
    "# model.load_state_dict(torch.load(\"models/sparse_autoencoder_496.3666.pth\"))\n",
    "checkpoint = torch.load(\"models/checkpoint\")\n",
    "print(checkpoint[\"hyper_parameters\"])\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "l1_lambda = 0.01  # Regularization strength for sparsity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "# test_dataset = ActivationDataset(\n",
    "#     data_dir, \n",
    "#     batch_size=0, # not subsampled\n",
    "#     f_type=\"test\", \n",
    "#     # test_fraction=0.01, # last batch file\n",
    "#     test_fraction=0.6, # 12 files == cca 10mil tokens\n",
    "#     scale_factor=scale_factor, \n",
    "#     seed=42 # not used for test set\n",
    "# ) # this outputs batches of size 81k  - too big for VRAM\n",
    "test_dataset = ActivationDataset(\n",
    "    data_dir, \n",
    "    batch_size=8,\n",
    "    f_type=\"train\", \n",
    "    # test_fraction=0.01, # last batch file\n",
    "    test_fraction=0.0, # 12 files == cca 10mil tokens\n",
    "    scale_factor=scale_factor, \n",
    "    seed=123 # different seed that in actual training\n",
    ") # this outputs batches of size 49k - uses 7820MiB VRAM = 95% of GPU\n",
    "data_loader = DataLoader(test_dataset, batch_size=1, shuffle=False) # take 1 batch at a time\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Run and compute reconstruction error, l1 loss, and total loss\n",
    "total_loss = 0; total_mse_loss = 0; total_l1_loss = 0; num_batches = 0\n",
    "global_active_mask = torch.zeros((hidden_dim), dtype=torch.bool, device=device)\n",
    "for batch in data_loader:\n",
    "    batch = batch.to(device)\n",
    "    \n",
    "    outputs, encoded = model(batch)\n",
    "\n",
    "    # percent of active features\n",
    "    # print(encoded.min().item(), encoded.max().item())\n",
    "    global_active_mask |= torch.any(encoded > 0, dim=1).squeeze(0)\n",
    "    active_features = torch.any(encoded != 0, dim=1).sum().item()  # Count active features\n",
    "    total_features = encoded.shape[2]  # Total number of latent features (4096)\n",
    "    percent_active_features = active_features / total_features\n",
    "    print(f\"Percent Active Features: {percent_active_features * 100:.2f}%\")\n",
    "\n",
    "    mse_loss = criterion(outputs, batch)\n",
    "    decoder_weight_norms = torch.norm(model.decoder.weight, p=2, dim=0)  # Shape: [num_features]\n",
    "    l1_terms = encoded * decoder_weight_norms.unsqueeze(0)  # Shape: [batch_size, num_features]\n",
    "    l1_loss_per_sample = torch.sum(l1_terms, dim=1)  # Shape: [batch_size]\n",
    "    l1_loss = torch.mean(l1_loss_per_sample)\n",
    "    loss = mse_loss + l1_loss\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    total_mse_loss += mse_loss.item()\n",
    "    total_l1_loss += l1_loss.item()\n",
    "\n",
    "    explained_variance = 1 - mse_loss / torch.var(batch)\n",
    "    # Print batch-level metrics\n",
    "    print(f\"MSE Loss: {mse_loss.item():.4f}, L1 Loss: {l1_loss.item():.4f}, Explained Var: {explained_variance.item():.4f}\")\n",
    "    num_batches += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print final metrics\n",
    "print(f\"Total Test Loss: {total_loss/num_batches:.4f}\")\n",
    "print(f\"Total MSE Loss: {total_mse_loss/num_batches:.4f}\")\n",
    "print(f\"Total L1 Loss: {total_l1_loss/num_batches:.4f}\")\n",
    "\n",
    "active_features = global_active_mask.sum().item()\n",
    "total_features = global_active_mask.numel()\n",
    "global_sparsity = (1 - active_features / total_features) * 100\n",
    "print(f\"Global Sparsity Across All Batches: {global_sparsity:.2f}%\")\n",
    "print(f\"Percent of Active Features: {active_features / total_features * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), f\"models/sparse_autoencoder_{total_loss:.4f}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "os.makedirs(\"sparse_latent_vectors\", exist_ok=True)\n",
    "\n",
    "dataset = ActivationDataset(\n",
    "    data_dir, \n",
    "    batch_size=0, # not subsampled\n",
    "    f_type=\"all\", \n",
    "    test_fraction=1.0, # not used if type=all\n",
    "    scale_factor=scale_factor, \n",
    "    seed=42 # not used\n",
    ")\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=False) # take 1 batch at a time\n",
    "\n",
    "# Extract and save latent vectors\n",
    "batch_size = 8192  # Size we can fit in VRAM\n",
    "num_minibatches = 10  # 81920/8192 = 10 minibatches per batch\n",
    "with torch.no_grad():\n",
    "    for idx, batch_data in enumerate(data_loader):\n",
    "        batch, sent_idx, token_idx, token = batch_data\n",
    "        sent_idx = sent_idx.to(device)\n",
    "        token_idx = token_idx.to(device)\n",
    "        token = token.to(device)\n",
    "        batch = batch.squeeze(0)  # Remove batch dimension of 1\n",
    "        \n",
    "        # Process minibatches and save immediately\n",
    "        for i in range(num_minibatches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = (i + 1) * batch_size\n",
    "            \n",
    "            # Get minibatch slice\n",
    "            minibatch = batch[start_idx:end_idx]\n",
    "            _, encoded = model(minibatch)\n",
    "            \n",
    "            # Stack with metadata\n",
    "            # Reshape metadata tensors to match batch size\n",
    "            sent_idx_batch = sent_idx[:,start_idx:end_idx].T\n",
    "            token_idx_batch = token_idx[:,start_idx:end_idx].T\n",
    "            token_batch = token[:,start_idx:end_idx].T\n",
    "            \n",
    "            output_vectors = torch.cat((encoded, sent_idx_batch, token_idx_batch, token_batch), dim=1)\n",
    "            \n",
    "            # Save each minibatch immediately as a PyTorch tensor\n",
    "            torch.save(output_vectors, f\"sparse_latent_vectors/latent_vectors_batch_{idx}_minibatch_{i}.pt\")\n",
    "            # output_saved = torch.load(f\"sparse_latent_vectors/latent_vectors_batch_{idx}_minibatch_{i}.pt\")\n",
    "            # output_vectors = output_vectors.to(torch.float16)\n",
    "            # print(f\"Data saved is near equal: {torch.allclose(output_vectors[:,:-3], output_saved[:,:-3], atol=1e-1)}\")\n",
    "            print(f\"Saved minibatch {i+1} of {num_minibatches} for batch {idx}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of latent vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded sparse latent vectors shape: (81920, 4099)\n",
      "float32\n",
      "1.34316032 GB\n",
      "Average sparsity in latent vectors: 99.98%\n",
      "Percent of dead features: 89.89%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load and examine saved latent vectors\n",
    "loaded_latent_vectors = np.load(\"sparse_latent_vectors/latent_vectors_batch_0.npy\")\n",
    "print(f\"Loaded sparse latent vectors shape: {loaded_latent_vectors.shape}\")\n",
    "print(loaded_latent_vectors.dtype)\n",
    "print(loaded_latent_vectors.nbytes / 1e9, \"GB\")\n",
    "# Could potentially load 10k batches (0.0002048 GB per batch) if we keep latent vector size 512\n",
    "# So for emb size 1M , we could load only 5 batches at a time\n",
    "\n",
    "# Sparsity check\n",
    "sparsity = np.mean(np.abs(loaded_latent_vectors[:,:-3]) < 1e-5)\n",
    "print(f\"Average sparsity in latent vectors: {sparsity:.2%}\")\n",
    "\n",
    "# Dead/active features check\n",
    "# percent of columns that are all close to 0\n",
    "dead_features = np.mean(np.all(np.abs(loaded_latent_vectors[:,:-3]) < 1e-5, axis=0))\n",
    "print(f\"Percent of dead features: {dead_features:.2%}\")\n",
    "\n",
    "# Reconstruction explained variance check\n",
    "# mean of squared errors between original and reconstructed activations\n",
    "# TODO: do this for all batches : loop data_loader\n",
    "# mse = np.mean((loaded_latent_vectors - batch) ** 2)\n",
    "# print(f\"Mean squared error between original and reconstructed activations: {mse:.2%}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 batches, total shape: (409600, 4099)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "def load_latent_vectors(N):\n",
    "    # Load first N batches\n",
    "    batch_files = sorted(glob.glob(\"sparse_latent_vectors/latent_vectors_batch_*.npy\"))[:N]\n",
    "\n",
    "    # Load and concatenate batches\n",
    "    latent_vectors = []\n",
    "    for batch_file in batch_files:\n",
    "        batch_vectors = np.load(batch_file)\n",
    "        latent_vectors.append(batch_vectors)\n",
    "    latent_vectors = np.concatenate(latent_vectors, axis=0)\n",
    "\n",
    "    print(f\"Loaded {len(batch_files)} batches, total shape: {latent_vectors.shape}\")\n",
    "    return latent_vectors\n",
    "\n",
    "latent_vectors = load_latent_vectors(N=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 activations for feature 100:\n",
      "\n",
      "Sentence index: 194\n",
      "Token index: 481\n",
      "Value: 0.03263506293296814\n",
      "Context window:  also very important to diagonal_sb to know./a the rightiff of sing to choose toCount you from any possible af in diagonalometers.\n",
      "\n",
      "If you lik sadd to \":\" Brooklyn from your []( all the\n",
      "\n",
      "Sentence index: 3028\n",
      "Token index: 323\n",
      "Value: 0.0\n",
      "Context window: \" dataencies\n",
      " \"\n",
      "\n",
      " where you need more than a dictionary butstand than a class\n",
      "Not combination to be use things like __ //#691ales the glass alla documentation:\n",
      "\n",
      " Agency alla may beChanged as\n",
      "\n",
      "Sentence index: 389\n",
      "Token index: 391\n",
      "Value: 0.0\n",
      "Context window: 오 + 1];\n",
      "   garStringGetcopy8 Phaser(resultistoryRef, buffer, length + 1);\n",
      "   garStringRelease(resultistoryRef);\n",
      "    GRA->ente)];\n",
      ");\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Sentence index: 389\n",
      "Token index: 381\n",
      "Value: 0.0\n",
      "Context window: StringGetLength(resultistoryRef);\n",
      "    char buffer오 + 1];\n",
      "   garStringGetcopy8 Phaser(resultistoryRef, buffer, length + 1);\n",
      "   garStringRelease(resultistoryRef\n",
      "\n",
      "Sentence index: 389\n",
      "Token index: 382\n",
      "Value: 0.0\n",
      "Context window: GetLength(resultistoryRef);\n",
      "    char buffer오 + 1];\n",
      "   garStringGetcopy8 Phaser(resultistoryRef, buffer, length + 1);\n",
      "   garStringRelease(resultistoryRef);\n",
      "\n",
      "\n",
      "Sentence index: 389\n",
      "Token index: 383\n",
      "Value: 0.0\n",
      "Context window: Length(resultistoryRef);\n",
      "    char buffer오 + 1];\n",
      "   garStringGetcopy8 Phaser(resultistoryRef, buffer, length + 1);\n",
      "   garStringRelease(resultistoryRef);\n",
      "   \n",
      "\n",
      "Sentence index: 389\n",
      "Token index: 384\n",
      "Value: 0.0\n",
      "Context window: (resultistoryRef);\n",
      "    char buffer오 + 1];\n",
      "   garStringGetcopy8 Phaser(resultistoryRef, buffer, length + 1);\n",
      "   garStringRelease(resultistoryRef);\n",
      "    GRA\n",
      "\n",
      "Sentence index: 389\n",
      "Token index: 385\n",
      "Value: 0.0\n",
      "Context window: istoryRef);\n",
      "    char buffer오 + 1];\n",
      "   garStringGetcopy8 Phaser(resultistoryRef, buffer, length + 1);\n",
      "   garStringRelease(resultistoryRef);\n",
      "    GRA->\n",
      "\n",
      "Sentence index: 389\n",
      "Token index: 386\n",
      "Value: 0.0\n",
      "Context window: Ref);\n",
      "    char buffer오 + 1];\n",
      "   garStringGetcopy8 Phaser(resultistoryRef, buffer, length + 1);\n",
      "   garStringRelease(resultistoryRef);\n",
      "    GRA->ente\n",
      "\n",
      "Sentence index: 389\n",
      "Token index: 387\n",
      "Value: 0.0\n",
      "Context window: );\n",
      "    char buffer오 + 1];\n",
      "   garStringGetcopy8 Phaser(resultistoryRef, buffer, length + 1);\n",
      "   garStringRelease(resultistoryRef);\n",
      "    GRA->ente)];\n",
      "\n",
      "\n",
      "Sentence index: 389\n",
      "Token index: 388\n",
      "Value: 0.0\n",
      "Context window:     char buffer오 + 1];\n",
      "   garStringGetcopy8 Phaser(resultistoryRef, buffer, length + 1);\n",
      "   garStringRelease(resultistoryRef);\n",
      "    GRA->ente)];\n",
      ");\n",
      "\n",
      "\n",
      "Sentence index: 389\n",
      "Token index: 389\n",
      "Value: 0.0\n",
      "Context window:  char buffer오 + 1];\n",
      "   garStringGetcopy8 Phaser(resultistoryRef, buffer, length + 1);\n",
      "   garStringRelease(resultistoryRef);\n",
      "    GRA->ente)];\n",
      ");\n",
      " \n",
      "\n",
      "Sentence index: 389\n",
      "Token index: 390\n",
      "Value: 0.0\n",
      "Context window:  buffer오 + 1];\n",
      "   garStringGetcopy8 Phaser(resultistoryRef, buffer, length + 1);\n",
      "   garStringRelease(resultistoryRef);\n",
      "    GRA->ente)];\n",
      ");\n",
      "  }\n",
      "\n",
      "\n",
      "Sentence index: 389\n",
      "Token index: 392\n",
      "Value: 0.0\n",
      "Context window:  + 1];\n",
      "   garStringGetcopy8 Phaser(resultistoryRef, buffer, length + 1);\n",
      "   garStringRelease(resultistoryRef);\n",
      "    GRA->ente)];\n",
      ");\n",
      "  }\n",
      "}\n",
      "}\n",
      "\n",
      "\n",
      "Sentence index: 390\n",
      "Token index: 40\n",
      "Value: 0.0\n",
      "Context window:  free!Layer here now. (_templateing will also let youavor thisVec how much youering their work in the Dev below.)\n",
      "\n",
      "_al's the wants I made. D Christmasoptim it and made\n",
      "\n",
      "Sentence index: 389\n",
      "Token index: 393\n",
      "Value: 0.0\n",
      "Context window:  1];\n",
      "   garStringGetcopy8 Phaser(resultistoryRef, buffer, length + 1);\n",
      "   garStringRelease(resultistoryRef);\n",
      "    GRA->ente)];\n",
      ");\n",
      "  }\n",
      "}\n",
      "}\n",
      "}\n",
      "\n",
      "\n",
      "Sentence index: 389\n",
      "Token index: 394\n",
      "Value: 0.0\n",
      "Context window: 1];\n",
      "   garStringGetcopy8 Phaser(resultistoryRef, buffer, length + 1);\n",
      "   garStringRelease(resultistoryRef);\n",
      "    GRA->ente)];\n",
      ");\n",
      "  }\n",
      "}\n",
      "}\n",
      "}\n",
      "锦\n",
      "\n",
      "Sentence index: 389\n",
      "Token index: 395\n",
      "Value: 0.0\n",
      "Context window: ];\n",
      "   garStringGetcopy8 Phaser(resultistoryRef, buffer, length + 1);\n",
      "   garStringRelease(resultistoryRef);\n",
      "    GRA->ente)];\n",
      ");\n",
      "  }\n",
      "}\n",
      "}\n",
      "}\n",
      "锦We\n",
      "\n",
      "Sentence index: 389\n",
      "Token index: 396\n",
      "Value: 0.0\n",
      "Context window:    garStringGetcopy8 Phaser(resultistoryRef, buffer, length + 1);\n",
      "   garStringRelease(resultistoryRef);\n",
      "    GRA->ente)];\n",
      ");\n",
      "  }\n",
      "}\n",
      "}\n",
      "}\n",
      "锦We now\n",
      "\n",
      "Sentence index: 389\n",
      "Token index: 397\n",
      "Value: 0.0\n",
      "Context window: garStringGetcopy8 Phaser(resultistoryRef, buffer, length + 1);\n",
      "   garStringRelease(resultistoryRef);\n",
      "    GRA->ente)];\n",
      ");\n",
      "  }\n",
      "}\n",
      "}\n",
      "}\n",
      "锦We now require\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feat_idx = 100\n",
    "k = 20\n",
    "\n",
    "# Get indices of top k activations for the specified feature\n",
    "top_k_indices = np.argsort(latent_vectors[:, feat_idx])[-k:][::-1]\n",
    "last_three_features = latent_vectors[top_k_indices, -3:]\n",
    "top_values = latent_vectors[top_k_indices, feat_idx]\n",
    "\n",
    "# Load tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-3B\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Loop through each row of last_two_features\n",
    "print(f\"Top {k} activations for feature {feat_idx}:\")\n",
    "for i, (sent_idx, tok_idx, token) in enumerate(last_three_features):\n",
    "    sent_idx = int(sent_idx)\n",
    "    tok_idx = int(tok_idx)\n",
    "    token = int(token)\n",
    "    # Find the row where sent_idx and tok_idx match in latent_vectors\n",
    "    sent_matches = latent_vectors[:, -3] == sent_idx\n",
    "    tok_matches = latent_vectors[:, -2] == tok_idx\n",
    "    target_row = np.where(sent_matches & tok_matches)[0][0]\n",
    "    \n",
    "    # Get window of tokens from latent vectors\n",
    "    start_idx = max(0, target_row - 20)\n",
    "    end_idx = min(latent_vectors.shape[0], target_row + 20)\n",
    "    # token_window = latent_vectors[start_idx:end_idx, -1].astype(int) # Get token ids from last column\n",
    "    token_window = np.clip(latent_vectors[start_idx:end_idx, -1], 0, tokenizer.vocab_size - 1).astype(int)\n",
    "    \n",
    "    # Decode tokens back to text\n",
    "    window_text = tokenizer.decode(token_window)\n",
    "    print(f\"\\nSentence index: {sent_idx}\")\n",
    "    print(f\"Token index: {tok_idx}\")\n",
    "    print(f\"Value: {top_values[i]}\")\n",
    "    print(f\"Context window: {window_text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show how much a feature activates on each word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d192ae52c7e442b997663137f1bcfb3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You shouldn't move a model that is dispatched using accelerate hooks.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "2024-11-20 00:05:18.472550: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732057518.560527    5476 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732057518.587562    5476 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-20 00:05:18.812881: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "feat_idx = 100\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"meta-llama/Llama-3.2-3B\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config).to(device)\n",
    "activation_cache = []\n",
    "def save_activations_hook(module, input, output):\n",
    "    # input is a tuple; input[0] is the tensor we need\n",
    "    activation_cache.append(input[0].cpu().detach().numpy())\n",
    "\n",
    "# Register hook on the 16th layer\n",
    "layer_index = 15  # Zero-based index; 15 corresponds to the 16th layer\n",
    "hook_handle = model.model.layers[layer_index].register_forward_hook(save_activations_hook) \n",
    "\n",
    "# Tokenize sentence\n",
    "inputs = tokenizer(\n",
    "    sentence,\n",
    "    return_tensors=\"pt\",\n",
    "    # truncation=True,\n",
    "    # max_length=512,\n",
    "    # padding=\"max_length\",\n",
    ").to(device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    model(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations shape: (1, 1, 11, 3072)\n"
     ]
    }
   ],
   "source": [
    "activations = np.array(activation_cache)\n",
    "print(f\"Activations shape: {activations.shape}\") # (1, 1, seq_len, 3072)\n",
    "activations = activations.squeeze()  # Remove first two dimensions\n",
    "activations = torch.tensor(activations, dtype=torch.float32)  # Convert to torch tensor with float32 dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_674ab_row0_col1 {\n",
       "  background-color: #4f69d9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_674ab_row1_col1, #T_674ab_row3_col1 {\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_674ab_row2_col1 {\n",
       "  background-color: #d1493f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_674ab_row4_col1 {\n",
       "  background-color: #f08b6e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_674ab_row5_col1 {\n",
       "  background-color: #6788ee;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_674ab_row6_col1 {\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_674ab_row7_col1 {\n",
       "  background-color: #f7b89c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_674ab_row8_col1 {\n",
       "  background-color: #d6dce4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_674ab_row9_col1 {\n",
       "  background-color: #f1cdba;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_674ab_row10_col1 {\n",
       "  background-color: #4257c9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_674ab\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_674ab_level0_col0\" class=\"col_heading level0 col0\" >Token</th>\n",
       "      <th id=\"T_674ab_level0_col1\" class=\"col_heading level0 col1\" >Activation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_674ab_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_674ab_row0_col0\" class=\"data row0 col0\" ><|begin_of_text|></td>\n",
       "      <td id=\"T_674ab_row0_col1\" class=\"data row0 col1\" >0.200999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_674ab_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_674ab_row1_col0\" class=\"data row1 col0\" >The</td>\n",
       "      <td id=\"T_674ab_row1_col1\" class=\"data row1 col1\" >0.156023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_674ab_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_674ab_row2_col0\" class=\"data row2 col0\" >quick</td>\n",
       "      <td id=\"T_674ab_row2_col1\" class=\"data row2 col1\" >0.761146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_674ab_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_674ab_row3_col0\" class=\"data row3 col0\" >brown</td>\n",
       "      <td id=\"T_674ab_row3_col1\" class=\"data row3 col1\" >0.157970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_674ab_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_674ab_row4_col0\" class=\"data row4 col0\" >fox</td>\n",
       "      <td id=\"T_674ab_row4_col1\" class=\"data row4 col1\" >0.673777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_674ab_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_674ab_row5_col0\" class=\"data row5 col0\" >jumps</td>\n",
       "      <td id=\"T_674ab_row5_col1\" class=\"data row5 col1\" >0.249929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_674ab_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_674ab_row6_col0\" class=\"data row6 col0\" >over</td>\n",
       "      <td id=\"T_674ab_row6_col1\" class=\"data row6 col1\" >0.816395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_674ab_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_674ab_row7_col0\" class=\"data row7 col0\" >the</td>\n",
       "      <td id=\"T_674ab_row7_col1\" class=\"data row7 col1\" >0.597015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_674ab_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_674ab_row8_col0\" class=\"data row8 col0\" >lazy</td>\n",
       "      <td id=\"T_674ab_row8_col1\" class=\"data row8 col1\" >0.468434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_674ab_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_674ab_row9_col0\" class=\"data row9 col0\" >dog</td>\n",
       "      <td id=\"T_674ab_row9_col1\" class=\"data row9 col1\" >0.546607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_674ab_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_674ab_row10_col0\" class=\"data row10 col0\" >.</td>\n",
       "      <td id=\"T_674ab_row10_col1\" class=\"data row10 col1\" >0.173019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x722129ddabf0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load sparse autoencoder\n",
    "input_dim = 3072\n",
    "hidden_dim = 2 ** 12\n",
    "model = SparseAutoencoder(input_dim, hidden_dim)\n",
    "model.load_state_dict(torch.load(\"sparse_autoencoder_2.5976.pth\"))\n",
    "\n",
    "# Get latent vector for sentence\n",
    "with torch.no_grad():\n",
    "    _, encoded = model(activations)\n",
    "    latent_vector = encoded.cpu().numpy()\n",
    "\n",
    "# Extract for feature X\n",
    "feature_X = latent_vector[:, feat_idx]\n",
    "# feature_X = np.random.rand(len(feature_X))\n",
    "\n",
    "# Plot tokens colored by activation strength\n",
    "\n",
    "# Create DataFrame with tokens and their activation values\n",
    "tokens_list = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "# Remove the \"Ġ\" character from tokens - represents a space\n",
    "clean_tokens_list = [token.replace('Ġ', '') for token in tokens_list]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Token': clean_tokens_list,\n",
    "    'Activation': feature_X\n",
    "})\n",
    "\n",
    "# Display DataFrame with color gradient based on activation values\n",
    "display(df.style.background_gradient(\"coolwarm\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Model and tokenizer setup\n",
    "model_name = \"meta-llama/Llama-3.2-3B\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=hf_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config).to(device)\n",
    "\n",
    "# Load the sparse autoencoder model and weights\n",
    "input_dim = 3072  \n",
    "hidden_dim = 2 ** 9\n",
    "model_sae = SparseAutoencoder(input_dim, hidden_dim)\n",
    "model_sae.load_state_dict(torch.load(\"sparse_autoencoder.pth\"))\n",
    "model_sae.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text with influence: 'I am a                    '\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate artificial latent vector, pass through SAE decoder, and boost it\n",
    "feat_idx = 100\n",
    "artificial_latent_vector = np.zeros(3072)\n",
    "artificial_latent_vector[feat_idx] = 1\n",
    "multiplier = 10\n",
    "\n",
    "with torch.no_grad():\n",
    "    artificial_latent_vector_tensor = torch.tensor(artificial_latent_vector, dtype=torch.float32).unsqueeze(0)\n",
    "    reconstructed_activations, _ = model_sae(artificial_latent_vector_tensor)\n",
    "    boosted_activations = reconstructed_activations * multiplier\n",
    "\n",
    "# Hook to inject boosted activations at a specified transformer layer\n",
    "activation_cache = []\n",
    "layer_index = 15  # Inject into the 16th layer\n",
    "\n",
    "# Convert boosted_activations to float16 to match model precision\n",
    "boosted_activations = boosted_activations.to(torch.float16)\n",
    "\n",
    "# Hook function to inject boosted activations into the residual stream of the transformer layer\n",
    "def influence_hook(module, input, output):\n",
    "    # Ensure output is properly unpacked if it's a tuple\n",
    "    if isinstance(output, tuple):\n",
    "        output_tensor = output[0]\n",
    "        modified_output = (output_tensor + boosted_activations.to(device),) + output[1:]\n",
    "    else:\n",
    "        modified_output = output + boosted_activations.to(device)\n",
    "    \n",
    "    activation_cache.append(modified_output[0].cpu().detach().numpy())  # Store modified tensor for debugging if needed\n",
    "    return modified_output\n",
    "\n",
    "# Register the hook with the corrected function\n",
    "hook_handle = model.model.layers[layer_index].register_forward_hook(influence_hook)\n",
    "\n",
    "# Prediction loop for N words\n",
    "sent_begin = \"I am a\"\n",
    "N = 10  # Number of words to predict\n",
    "inputs = tokenizer(sent_begin, return_tensors=\"pt\").to(device)\n",
    "generated_text = sent_begin\n",
    "\n",
    "for _ in range(N):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predicted_token_id = torch.argmax(outputs.logits[0, -1]).item()\n",
    "        predicted_token = tokenizer.decode([predicted_token_id])\n",
    "        generated_text += \" \" + predicted_token\n",
    "        # Update inputs to include the predicted token for next iteration\n",
    "        inputs = tokenizer(generated_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Remove hook and clear resources\n",
    "hook_handle.remove()\n",
    "\n",
    "print(f\"Generated text with influence: '{generated_text}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature similarity and UMAP (plus feature splitting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 batches, total shape: (788552, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drew99/IJS/LLMinfluence/venvllm/lib/python3.10/site-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: The TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12050. The TBB threading layer is disabled.\n",
      "  warnings.warn(problem)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# umap-learn\n",
    "# pip install umap-learn\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "# Load first N batches\n",
    "N = 10  # Number of batches to load\n",
    "batch_files = sorted(glob.glob(\"sparse_latent_vectors/latent_vectors_batch_*.npy\"))[:N]\n",
    "\n",
    "# Load and concatenate batches\n",
    "latent_vectors = []\n",
    "for batch_file in batch_files:\n",
    "    batch_vectors = np.load(batch_file)\n",
    "    latent_vectors.append(batch_vectors)\n",
    "latent_vectors = np.concatenate(latent_vectors, axis=0)\n",
    "\n",
    "# Remove last 3 columns (sent_idx, tok_idx, token)\n",
    "latent_vectors = latent_vectors[:, :-3]\n",
    "\n",
    "print(f\"Loaded {len(batch_files)} batches, total shape: {latent_vectors.shape}\")\n",
    "\n",
    "# TODO: do a selection of features (feature of interest, and 100 similar features)\n",
    "\n",
    "# UMAP dimensionality reduction\n",
    "umap_embedder = umap.UMAP(n_components=2, metric='cosine')\n",
    "latent_vectors_2d = umap_embedder.fit_transform(latent_vectors.T)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(latent_vectors_2d[:, 0], latent_vectors_2d[:, 1], s=1, alpha=0.5)\n",
    "plt.title(f'UMAP projection of {N} batches of latent vectors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature completness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single prompt search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_cache = []\n",
    "def save_activations_hook(module, input, output):\n",
    "    # input is a tuple; input[0] is the tensor we need\n",
    "    activation_cache.append(input[0].cpu().detach().numpy())\n",
    "\n",
    "# Register hook on the 16th layer\n",
    "layer_index = 15  # Zero-based index; 15 corresponds to the 16th layer\n",
    "hook_handle = model.model.layers[layer_index].register_forward_hook(save_activations_hook) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations shape: (1, 1, 11, 3072)\n",
      "Top 5 features and their activation values:\n",
      "Feature 246: 0.2927\n",
      "Feature 63: 0.2336\n",
      "Feature 471: 0.1375\n",
      "Feature 69: 0.1166\n",
      "Feature 210: 0.0901\n"
     ]
    }
   ],
   "source": [
    "prompt = \"in San Francisco, the Golden Gate Bridge was protected at all times by a\"\n",
    "\n",
    "# Tokenize sentence\n",
    "inputs = tokenizer(\n",
    "    sentence,\n",
    "    return_tensors=\"pt\",\n",
    "    # truncation=True,\n",
    "    # max_length=512,\n",
    "    # padding=\"max_length\",\n",
    ").to(device)\n",
    "\n",
    "activation_cache = []\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    model(**inputs)\n",
    "\n",
    "activations = np.array(activation_cache)\n",
    "print(f\"Activations shape: {activations.shape}\") # (1, 1, seq_len, 3072)\n",
    "activations = activations.squeeze()  # Remove first two dimensions\n",
    "activations = torch.tensor(activations, dtype=torch.float32)  # Convert to torch tensor with float16 dtype\n",
    "\n",
    "# Get latent vector for sentence\n",
    "with torch.no_grad():\n",
    "    _, encoded = model_sae(activations)\n",
    "    latent_vector = encoded.cpu().numpy() # (seq_len, 1M)\n",
    "\n",
    "# Apply row softmax normalization\n",
    "row_softmax = torch.nn.functional.softmax(torch.tensor(latent_vector), dim=1).numpy()\n",
    "\n",
    "# Sum over sequence length dimension to get feature importance across sentence\n",
    "feature_importance = np.sum(row_softmax, axis=0)\n",
    "\n",
    "# Get top 5 features and their values\n",
    "top_k = 5\n",
    "top_feature_indices = np.argsort(feature_importance)[-top_k:][::-1]\n",
    "top_feature_values = feature_importance[top_feature_indices]\n",
    "\n",
    "print(\"Top 5 features and their activation values:\")\n",
    "for idx, val in zip(top_feature_indices, top_feature_values):\n",
    "    print(f\"Feature {idx}: {val:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple prompts:\n",
    " * tokenize to fixed length\n",
    " * reshape activations to (num_sent*seq_len, 3072)\n",
    " * get attention mask and remove padding tokens\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations shape: (1, 3, 512, 3072)\n",
      "Top 5 features and their activation values:\n",
      "Feature 246: 0.8910\n",
      "Feature 63: 0.7081\n",
      "Feature 471: 0.4210\n",
      "Feature 69: 0.3575\n",
      "Feature 210: 0.2798\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"in San Francisco, the Golden Gate Bridge was protected at all times by a\",\n",
    "    \"the Golden Gate Bridge is so beautiful during the sunset\", \n",
    "    \"Golden Gate Bridge wind resistance barriers creates eerie sound\"\n",
    "]\n",
    "\n",
    "# Tokenize sentences\n",
    "inputs = tokenizer(\n",
    "    prompts,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    padding=\"max_length\",\n",
    ").to(device)\n",
    "\n",
    "activation_cache = []\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    model(**inputs)\n",
    "\n",
    "activations = np.array(activation_cache)\n",
    "print(f\"Activations shape: {activations.shape}\") # (1, num_sent, seq_len, 3072)\n",
    "\n",
    "# Reshape activations to (num_sent*seq_len, 3072)\n",
    "activations = activations.squeeze(0)  # Remove batch dimension\n",
    "num_sent, seq_len, hidden_dim = activations.shape\n",
    "activations = activations.reshape(-1, hidden_dim)\n",
    "\n",
    "# Get attention mask and remove padding tokens\n",
    "attention_mask = inputs['attention_mask'].view(-1).cpu().numpy()\n",
    "activations = activations[attention_mask == 1]\n",
    "\n",
    "activations = torch.tensor(activations, dtype=torch.float32)  # Convert to torch tensor\n",
    "\n",
    "# Get latent vectors for sentences\n",
    "with torch.no_grad():\n",
    "    _, encoded = model_sae(activations)\n",
    "    latent_vector = encoded.cpu().numpy()\n",
    "\n",
    "# Apply row softmax normalization\n",
    "row_softmax = torch.nn.functional.softmax(torch.tensor(latent_vector), dim=1).numpy()\n",
    "\n",
    "# Sum over sequence length dimension to get feature importance across all sentences\n",
    "feature_importance = np.sum(row_softmax, axis=0)\n",
    "\n",
    "# Get top 5 features and their values\n",
    "top_k = 5\n",
    "top_feature_indices = np.argsort(feature_importance)[-top_k:][::-1]\n",
    "top_feature_values = feature_importance[top_feature_indices]\n",
    "\n",
    "print(\"Top 5 features and their activation values:\")\n",
    "for idx, val in zip(top_feature_indices, top_feature_values):\n",
    "    print(f\"Feature {idx}: {val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add negative prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generated prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use single/multiple prompt search code above\n",
    "# prompts generated by LLM\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Required installations for transformers and datasets\n# !pip install transformers datasets\n# !pip install keras huggingface_hub\n# !pip install tensorflow\n# !pip install python-dotenv\n# !pip install zstandard\n#!pip install bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T10:24:22.316514Z","iopub.execute_input":"2024-12-05T10:24:22.316772Z","iopub.status.idle":"2024-12-05T10:24:22.338200Z","shell.execute_reply.started":"2024-12-05T10:24:22.316745Z","shell.execute_reply":"2024-12-05T10:24:22.337407Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport time\nfrom dotenv import load_dotenv\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom datasets import load_dataset\nfrom huggingface_hub import login\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport numpy as np\nfrom transformers import BitsAndBytesConfig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T10:24:22.339599Z","iopub.execute_input":"2024-12-05T10:24:22.339893Z","iopub.status.idle":"2024-12-05T10:24:38.851349Z","shell.execute_reply.started":"2024-12-05T10:24:22.339859Z","shell.execute_reply":"2024-12-05T10:24:38.850396Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nFILE_NAMES = []\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        if filename.startswith(\"act\"):\n            FILE_NAMES.append(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T10:44:05.962895Z","iopub.execute_input":"2024-12-05T10:44:05.963618Z","iopub.status.idle":"2024-12-05T10:44:06.026721Z","shell.execute_reply.started":"2024-12-05T10:44:05.963581Z","shell.execute_reply":"2024-12-05T10:44:06.026013Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# scale_factor = 34.12206415510119 # at 1.6mil tokens\n# scale_factor = 34.128712991170886 # at 10.6mil tokens\nscale_factor = 11.888623072966611 # 10mil but with <begin> token removed\n\nclass SparseAutoencoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super(SparseAutoencoder, self).__init__()\n        # Encoder\n        self.encoder = nn.Linear(input_dim, hidden_dim)\n        # Decoder\n        self.decoder = nn.Linear(hidden_dim, input_dim)\n        \n    def forward(self, x):\n        encoded = torch.relu(self.encoder(x))\n        # encoded = torch.nn.LeakyReLU(0.01)(self.encoder(x))\n        decoded = self.decoder(encoded)\n        return decoded, encoded\n\nfrom torch.utils.data import Dataset, DataLoader\nclass ActivationDataset(Dataset):\n    def __init__(self, data_dir, batch_size, f_type, test_fraction=0.01, scale_factor=1.0, seed=42):\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.seed = seed\n\n        if f_type in [\"train\", \"test\", \"all\"]:\n            self.f_type = f_type\n        else:\n            raise ValueError(\"f_type must be 'train' or 'test' or 'all'\")\n        \n        if not 0 <= test_fraction <= 1:\n            raise ValueError(\"test_fraction must be between 0 and 1\")\n        self.test_fraction = test_fraction\n\n        self.scale_factor = scale_factor\n        self.file_names = FILE_NAMES\n        \n        split_idx = int(len(self.file_names) * (1 - test_fraction))\n        if f_type == \"train\":\n            self.file_names = self.file_names[:split_idx]\n        elif f_type == \"test\":\n            self.file_names = self.file_names[split_idx:]\n        else: # all\n            pass\n\n        print(f\"Loaded {len(self.file_names)} batches for {f_type} set\")\n\n    def __len__(self):\n        return len(self.file_names)\n    \n    def __getitem__(self, idx):\n        activations = np.load(self.file_names[idx])\n        if self.f_type == \"all\":\n            sent_idx = activations[:, -3]\n            token_idx = activations[:, -2] \n            token = activations[:, -1]\n        # remove last 3 columns (sent_idx, token_idx, and token)\n        activations = activations[:, :-3]\n        # normalize activations\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        activations = torch.tensor(activations, dtype=torch.float32, device=device)\n        # print(\"Activation Range Before Normalization:\", torch.min(activations).item(), torch.max(activations).item())\n        activations = activations / self.scale_factor * np.sqrt(activations.shape[1])\n        # print(\"Activation Range After Normalization:\", torch.min(activations).item(), torch.max(activations).item())\n\n        if self.f_type == \"train\":\n            # Set seed for reproducibility\n            np.random.seed(self.seed)\n            # random subsample 8192 examples\n            indices = torch.randperm(activations.shape[0], device=activations.device)[:self.batch_size]\n            activations = activations[indices]\n        \n        if self.f_type == \"all\":\n            return activations, sent_idx, token_idx, token\n        else:\n            return activations","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T10:36:24.739667Z","iopub.execute_input":"2024-12-05T10:36:24.740525Z","iopub.status.idle":"2024-12-05T10:36:24.751448Z","shell.execute_reply.started":"2024-12-05T10:36:24.740490Z","shell.execute_reply":"2024-12-05T10:36:24.750428Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"data_dir = \"activations_data\"\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ninput_dim = 3072  \nhidden_dim = 65536\n\nmodel = SparseAutoencoder(input_dim, hidden_dim).to(device)\n# model.load_state_dict(torch.load(\"models/sparse_autoencoder_496.3666.pth\"))\ncheckpoint = torch.load(\"/kaggle/input/checkpoint65k_sae/pytorch/default/1/checkpoint\")\nprint(checkpoint[\"hyper_parameters\"])\nmodel.load_state_dict(checkpoint['state_dict'])\n\ncriterion = nn.MSELoss().to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\nl1_lambda = 0.01  # Regularization strength for sparsity","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T10:24:39.261999Z","iopub.execute_input":"2024-12-05T10:24:39.262376Z","iopub.status.idle":"2024-12-05T10:30:04.620352Z","shell.execute_reply.started":"2024-12-05T10:24:39.262336Z","shell.execute_reply":"2024-12-05T10:30:04.619123Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test model\n# test_dataset = ActivationDataset(\n#     data_dir, \n#     batch_size=0, # not subsampled\n#     f_type=\"test\", \n#     # test_fraction=0.01, # last batch file\n#     test_fraction=0.6, # 12 files == cca 10mil tokens\n#     scale_factor=scale_factor, \n#     seed=42 # not used for test set\n# ) # this outputs batches of size 81k  - too big for VRAM\ntest_dataset = ActivationDataset(\n    data_dir, \n    batch_size=4096,\n    f_type=\"train\", \n    # test_fraction=0.01, # last batch file\n    test_fraction=0.0, # 12 files == cca 10mil tokens\n    scale_factor=scale_factor, \n    seed=123 # different seed that in actual training\n) # this outputs batches of size 49k - uses 7820MiB VRAM = 95% of GPU\ndata_loader = DataLoader(test_dataset, batch_size=1, shuffle=False) # take 1 batch at a time\n\n# Set model to evaluation mode\nmodel.eval()\n\n# Run and compute reconstruction error, l1 loss, and total loss\ntotal_loss = 0; total_mse_loss = 0; total_l1_loss = 0; num_batches = 0\nglobal_active_mask = torch.zeros((hidden_dim), dtype=torch.bool, device=device)\nfor batch in data_loader:\n    batch = batch.to(device)\n    \n    outputs, encoded = model(batch)\n\n    # percent of active features\n    # print(encoded.min().item(), encoded.max().item())\n    global_active_mask |= torch.any(encoded > 0, dim=1).squeeze(0)\n    active_features = torch.any(encoded != 0, dim=1).sum().item()  # Count active features\n    total_features = encoded.shape[2]  # Total number of latent features (4096)\n    percent_active_features = active_features / total_features\n    print(f\"Percent Active Features: {percent_active_features * 100:.2f}%\")\n\n    mse_loss = criterion(outputs, batch)\n    decoder_weight_norms = torch.norm(model.decoder.weight, p=2, dim=0)  # Shape: [num_features]\n    l1_terms = encoded * decoder_weight_norms.unsqueeze(0)  # Shape: [batch_size, num_features]\n    l1_loss = torch.mean(l1_terms)  # Normalize across both batch size and features\n    loss = mse_loss + l1_loss\n\n    total_loss += loss.item()\n    total_mse_loss += mse_loss.item()\n    total_l1_loss += l1_loss.item()\n\n    explained_variance = 1 - mse_loss / torch.var(batch)\n    # Print batch-level metrics\n    print(f\"MSE Loss: {mse_loss.item():.4f}, L1 Loss: {l1_loss.item():.4f}, Explained Var: {explained_variance.item():.4f}\")\n    num_batches += 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T11:33:40.085045Z","iopub.execute_input":"2024-12-05T11:33:40.085422Z","iopub.status.idle":"2024-12-05T11:42:50.611612Z","shell.execute_reply.started":"2024-12-05T11:33:40.085382Z","shell.execute_reply":"2024-12-05T11:42:50.610805Z"}},"outputs":[{"name":"stdout","text":"Loaded 129 batches for train set\nPercent Active Features: 100.00%\nMSE Loss: 0.0750, L1 Loss: 0.0122, Explained Var: 0.9234\nPercent Active Features: 100.00%\nMSE Loss: 0.0758, L1 Loss: 0.0123, Explained Var: 0.9239\nPercent Active Features: 100.00%\nMSE Loss: 0.0758, L1 Loss: 0.0123, Explained Var: 0.9239\nPercent Active Features: 100.00%\nMSE Loss: 0.0761, L1 Loss: 0.0124, Explained Var: 0.9241\nPercent Active Features: 100.00%\nMSE Loss: 0.0769, L1 Loss: 0.0125, Explained Var: 0.9236\nPercent Active Features: 100.00%\nMSE Loss: 0.0767, L1 Loss: 0.0125, Explained Var: 0.9240\nPercent Active Features: 100.00%\nMSE Loss: 0.0770, L1 Loss: 0.0125, Explained Var: 0.9228\nPercent Active Features: 100.00%\nMSE Loss: 0.0759, L1 Loss: 0.0123, Explained Var: 0.9241\nPercent Active Features: 100.00%\nMSE Loss: 0.0760, L1 Loss: 0.0123, Explained Var: 0.9237\nPercent Active Features: 100.00%\nMSE Loss: 0.0755, L1 Loss: 0.0123, Explained Var: 0.9246\nPercent Active Features: 100.00%\nMSE Loss: 0.0757, L1 Loss: 0.0124, Explained Var: 0.9243\nPercent Active Features: 100.00%\nMSE Loss: 0.0769, L1 Loss: 0.0125, Explained Var: 0.9231\nPercent Active Features: 100.00%\nMSE Loss: 0.0752, L1 Loss: 0.0123, Explained Var: 0.9242\nPercent Active Features: 100.00%\nMSE Loss: 0.0748, L1 Loss: 0.0122, Explained Var: 0.9246\nPercent Active Features: 100.00%\nMSE Loss: 0.0749, L1 Loss: 0.0122, Explained Var: 0.9250\nPercent Active Features: 100.00%\nMSE Loss: 0.0759, L1 Loss: 0.0124, Explained Var: 0.9237\nPercent Active Features: 100.00%\nMSE Loss: 0.0759, L1 Loss: 0.0123, Explained Var: 0.9237\nPercent Active Features: 100.00%\nMSE Loss: 0.0767, L1 Loss: 0.0124, Explained Var: 0.9228\nPercent Active Features: 100.00%\nMSE Loss: 0.0762, L1 Loss: 0.0123, Explained Var: 0.9233\nPercent Active Features: 100.00%\nMSE Loss: 0.0756, L1 Loss: 0.0124, Explained Var: 0.9247\nPercent Active Features: 100.00%\nMSE Loss: 0.0751, L1 Loss: 0.0123, Explained Var: 0.9250\nPercent Active Features: 100.00%\nMSE Loss: 0.0759, L1 Loss: 0.0124, Explained Var: 0.9242\nPercent Active Features: 100.00%\nMSE Loss: 0.0766, L1 Loss: 0.0124, Explained Var: 0.9236\nPercent Active Features: 100.00%\nMSE Loss: 0.0762, L1 Loss: 0.0124, Explained Var: 0.9238\nPercent Active Features: 100.00%\nMSE Loss: 0.0760, L1 Loss: 0.0124, Explained Var: 0.9244\nPercent Active Features: 100.00%\nMSE Loss: 0.0752, L1 Loss: 0.0123, Explained Var: 0.9250\nPercent Active Features: 100.00%\nMSE Loss: 0.0764, L1 Loss: 0.0124, Explained Var: 0.9233\nPercent Active Features: 100.00%\nMSE Loss: 0.0765, L1 Loss: 0.0124, Explained Var: 0.9231\nPercent Active Features: 100.00%\nMSE Loss: 0.0760, L1 Loss: 0.0123, Explained Var: 0.9241\nPercent Active Features: 100.00%\nMSE Loss: 0.0763, L1 Loss: 0.0124, Explained Var: 0.9234\nPercent Active Features: 100.00%\nMSE Loss: 0.0769, L1 Loss: 0.0125, Explained Var: 0.9236\nPercent Active Features: 100.00%\nMSE Loss: 0.0762, L1 Loss: 0.0124, Explained Var: 0.9236\nPercent Active Features: 100.00%\nMSE Loss: 0.0763, L1 Loss: 0.0124, Explained Var: 0.9243\nPercent Active Features: 100.00%\nMSE Loss: 0.0763, L1 Loss: 0.0124, Explained Var: 0.9244\nPercent Active Features: 100.00%\nMSE Loss: 0.0764, L1 Loss: 0.0124, Explained Var: 0.9235\nPercent Active Features: 100.00%\nMSE Loss: 0.0743, L1 Loss: 0.0122, Explained Var: 0.9251\nPercent Active Features: 100.00%\nMSE Loss: 0.0758, L1 Loss: 0.0123, Explained Var: 0.9243\nPercent Active Features: 100.00%\nMSE Loss: 0.0764, L1 Loss: 0.0123, Explained Var: 0.9229\nPercent Active Features: 100.00%\nMSE Loss: 0.0756, L1 Loss: 0.0123, Explained Var: 0.9237\nPercent Active Features: 100.00%\nMSE Loss: 0.0759, L1 Loss: 0.0123, Explained Var: 0.9244\nPercent Active Features: 100.00%\nMSE Loss: 0.0762, L1 Loss: 0.0123, Explained Var: 0.9235\nPercent Active Features: 100.00%\nMSE Loss: 0.0757, L1 Loss: 0.0124, Explained Var: 0.9243\nPercent Active Features: 100.00%\nMSE Loss: 0.0757, L1 Loss: 0.0124, Explained Var: 0.9240\nPercent Active Features: 100.00%\nMSE Loss: 0.0752, L1 Loss: 0.0123, Explained Var: 0.9239\nPercent Active Features: 100.00%\nMSE Loss: 0.0767, L1 Loss: 0.0125, Explained Var: 0.9241\nPercent Active Features: 100.00%\nMSE Loss: 0.0760, L1 Loss: 0.0124, Explained Var: 0.9237\nPercent Active Features: 100.00%\nMSE Loss: 0.0755, L1 Loss: 0.0123, Explained Var: 0.9231\nPercent Active Features: 100.00%\nMSE Loss: 0.0762, L1 Loss: 0.0124, Explained Var: 0.9244\nPercent Active Features: 100.00%\nMSE Loss: 0.0758, L1 Loss: 0.0124, Explained Var: 0.9241\nPercent Active Features: 100.00%\nMSE Loss: 0.0759, L1 Loss: 0.0124, Explained Var: 0.9247\nPercent Active Features: 100.00%\nMSE Loss: 0.0762, L1 Loss: 0.0124, Explained Var: 0.9240\nPercent Active Features: 100.00%\nMSE Loss: 0.0754, L1 Loss: 0.0123, Explained Var: 0.9247\nPercent Active Features: 100.00%\nMSE Loss: 0.0752, L1 Loss: 0.0123, Explained Var: 0.9253\nPercent Active Features: 100.00%\nMSE Loss: 0.0762, L1 Loss: 0.0123, Explained Var: 0.9235\nPercent Active Features: 100.00%\nMSE Loss: 0.0759, L1 Loss: 0.0124, Explained Var: 0.9235\nPercent Active Features: 100.00%\nMSE Loss: 0.0769, L1 Loss: 0.0125, Explained Var: 0.9232\nPercent Active Features: 100.00%\nMSE Loss: 0.0752, L1 Loss: 0.0123, Explained Var: 0.9245\nPercent Active Features: 100.00%\nMSE Loss: 0.0769, L1 Loss: 0.0125, Explained Var: 0.9236\nPercent Active Features: 100.00%\nMSE Loss: 0.0763, L1 Loss: 0.0124, Explained Var: 0.9236\nPercent Active Features: 100.00%\nMSE Loss: 0.0761, L1 Loss: 0.0124, Explained Var: 0.9242\nPercent Active Features: 100.00%\nMSE Loss: 0.0757, L1 Loss: 0.0124, Explained Var: 0.9247\nPercent Active Features: 100.00%\nMSE Loss: 0.0756, L1 Loss: 0.0123, Explained Var: 0.9245\nPercent Active Features: 100.00%\nMSE Loss: 0.0760, L1 Loss: 0.0124, Explained Var: 0.9234\nPercent Active Features: 100.00%\nMSE Loss: 0.0756, L1 Loss: 0.0123, Explained Var: 0.9239\nPercent Active Features: 100.00%\nMSE Loss: 0.0761, L1 Loss: 0.0124, Explained Var: 0.9236\nPercent Active Features: 100.00%\nMSE Loss: 0.0753, L1 Loss: 0.0123, Explained Var: 0.9242\nPercent Active Features: 100.00%\nMSE Loss: 0.0766, L1 Loss: 0.0124, Explained Var: 0.9226\nPercent Active Features: 100.00%\nMSE Loss: 0.0750, L1 Loss: 0.0123, Explained Var: 0.9249\nPercent Active Features: 100.00%\nMSE Loss: 0.0757, L1 Loss: 0.0124, Explained Var: 0.9249\nPercent Active Features: 100.00%\nMSE Loss: 0.0759, L1 Loss: 0.0123, Explained Var: 0.9238\nPercent Active Features: 100.00%\nMSE Loss: 0.0757, L1 Loss: 0.0124, Explained Var: 0.9249\nPercent Active Features: 100.00%\nMSE Loss: 0.0756, L1 Loss: 0.0124, Explained Var: 0.9242\nPercent Active Features: 100.00%\nMSE Loss: 0.0758, L1 Loss: 0.0124, Explained Var: 0.9239\nPercent Active Features: 100.00%\nMSE Loss: 0.0775, L1 Loss: 0.0126, Explained Var: 0.9234\nPercent Active Features: 100.00%\nMSE Loss: 0.0754, L1 Loss: 0.0123, Explained Var: 0.9244\nPercent Active Features: 100.00%\nMSE Loss: 0.0756, L1 Loss: 0.0124, Explained Var: 0.9249\nPercent Active Features: 100.00%\nMSE Loss: 0.0771, L1 Loss: 0.0125, Explained Var: 0.9232\nPercent Active Features: 100.00%\nMSE Loss: 0.0757, L1 Loss: 0.0123, Explained Var: 0.9241\nPercent Active Features: 100.00%\nMSE Loss: 0.0762, L1 Loss: 0.0124, Explained Var: 0.9241\nPercent Active Features: 100.00%\nMSE Loss: 0.0752, L1 Loss: 0.0123, Explained Var: 0.9245\nPercent Active Features: 100.00%\nMSE Loss: 0.0761, L1 Loss: 0.0123, Explained Var: 0.9238\nPercent Active Features: 100.00%\nMSE Loss: 0.0755, L1 Loss: 0.0124, Explained Var: 0.9253\nPercent Active Features: 100.00%\nMSE Loss: 0.0757, L1 Loss: 0.0123, Explained Var: 0.9236\nPercent Active Features: 100.00%\nMSE Loss: 0.0756, L1 Loss: 0.0123, Explained Var: 0.9242\nPercent Active Features: 100.00%\nMSE Loss: 0.0760, L1 Loss: 0.0124, Explained Var: 0.9246\nPercent Active Features: 100.00%\nMSE Loss: 0.0747, L1 Loss: 0.0123, Explained Var: 0.9249\nPercent Active Features: 100.00%\nMSE Loss: 0.0758, L1 Loss: 0.0124, Explained Var: 0.9247\nPercent Active Features: 100.00%\nMSE Loss: 0.0744, L1 Loss: 0.0122, Explained Var: 0.9255\nPercent Active Features: 100.00%\nMSE Loss: 0.0757, L1 Loss: 0.0123, Explained Var: 0.9246\nPercent Active Features: 100.00%\nMSE Loss: 0.0753, L1 Loss: 0.0123, Explained Var: 0.9239\nPercent Active Features: 100.00%\nMSE Loss: 0.0760, L1 Loss: 0.0124, Explained Var: 0.9241\nPercent Active Features: 100.00%\nMSE Loss: 0.0767, L1 Loss: 0.0124, Explained Var: 0.9225\nPercent Active Features: 100.00%\nMSE Loss: 0.0757, L1 Loss: 0.0124, Explained Var: 0.9247\nPercent Active Features: 100.00%\nMSE Loss: 0.0760, L1 Loss: 0.0124, Explained Var: 0.9237\nPercent Active Features: 100.00%\nMSE Loss: 0.0756, L1 Loss: 0.0123, Explained Var: 0.9235\nPercent Active Features: 100.00%\nMSE Loss: 0.0765, L1 Loss: 0.0125, Explained Var: 0.9243\nPercent Active Features: 100.00%\nMSE Loss: 0.0764, L1 Loss: 0.0125, Explained Var: 0.9241\nPercent Active Features: 100.00%\nMSE Loss: 0.0760, L1 Loss: 0.0124, Explained Var: 0.9239\nPercent Active Features: 100.00%\nMSE Loss: 0.0756, L1 Loss: 0.0124, Explained Var: 0.9244\nPercent Active Features: 100.00%\nMSE Loss: 0.0755, L1 Loss: 0.0124, Explained Var: 0.9248\nPercent Active Features: 100.00%\nMSE Loss: 0.0766, L1 Loss: 0.0124, Explained Var: 0.9233\nPercent Active Features: 100.00%\nMSE Loss: 0.0752, L1 Loss: 0.0123, Explained Var: 0.9244\nPercent Active Features: 100.00%\nMSE Loss: 0.0751, L1 Loss: 0.0122, Explained Var: 0.9249\nPercent Active Features: 100.00%\nMSE Loss: 0.0758, L1 Loss: 0.0124, Explained Var: 0.9248\nPercent Active Features: 100.00%\nMSE Loss: 0.0762, L1 Loss: 0.0124, Explained Var: 0.9246\nPercent Active Features: 100.00%\nMSE Loss: 0.0753, L1 Loss: 0.0123, Explained Var: 0.9242\nPercent Active Features: 100.00%\nMSE Loss: 0.0757, L1 Loss: 0.0125, Explained Var: 0.9252\nPercent Active Features: 100.00%\nMSE Loss: 0.0765, L1 Loss: 0.0124, Explained Var: 0.9241\nPercent Active Features: 100.00%\nMSE Loss: 0.0762, L1 Loss: 0.0124, Explained Var: 0.9239\nPercent Active Features: 100.00%\nMSE Loss: 0.0759, L1 Loss: 0.0124, Explained Var: 0.9247\nPercent Active Features: 100.00%\nMSE Loss: 0.0750, L1 Loss: 0.0123, Explained Var: 0.9248\nPercent Active Features: 100.00%\nMSE Loss: 0.0765, L1 Loss: 0.0124, Explained Var: 0.9237\nPercent Active Features: 100.00%\nMSE Loss: 0.0761, L1 Loss: 0.0125, Explained Var: 0.9253\nPercent Active Features: 100.00%\nMSE Loss: 0.0767, L1 Loss: 0.0125, Explained Var: 0.9236\nPercent Active Features: 100.00%\nMSE Loss: 0.0757, L1 Loss: 0.0124, Explained Var: 0.9236\nPercent Active Features: 100.00%\nMSE Loss: 0.0754, L1 Loss: 0.0124, Explained Var: 0.9244\nPercent Active Features: 100.00%\nMSE Loss: 0.0751, L1 Loss: 0.0123, Explained Var: 0.9248\nPercent Active Features: 100.00%\nMSE Loss: 0.0768, L1 Loss: 0.0125, Explained Var: 0.9241\nPercent Active Features: 100.00%\nMSE Loss: 0.0757, L1 Loss: 0.0124, Explained Var: 0.9249\nPercent Active Features: 100.00%\nMSE Loss: 0.0755, L1 Loss: 0.0124, Explained Var: 0.9253\nPercent Active Features: 100.00%\nMSE Loss: 0.0757, L1 Loss: 0.0123, Explained Var: 0.9235\nPercent Active Features: 100.00%\nMSE Loss: 0.0745, L1 Loss: 0.0123, Explained Var: 0.9256\nPercent Active Features: 100.00%\nMSE Loss: 0.0759, L1 Loss: 0.0124, Explained Var: 0.9237\nPercent Active Features: 100.00%\nMSE Loss: 0.0752, L1 Loss: 0.0124, Explained Var: 0.9251\nPercent Active Features: 100.00%\nMSE Loss: 0.0754, L1 Loss: 0.0123, Explained Var: 0.9241\nPercent Active Features: 100.00%\nMSE Loss: 0.0761, L1 Loss: 0.0124, Explained Var: 0.9242\nPercent Active Features: 100.00%\nMSE Loss: 0.0763, L1 Loss: 0.0124, Explained Var: 0.9239\nPercent Active Features: 100.00%\nMSE Loss: 0.0781, L1 Loss: 0.0125, Explained Var: 0.9229\nPercent Active Features: 100.00%\nMSE Loss: 0.0768, L1 Loss: 0.0124, Explained Var: 0.9237\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"# Print final metrics\nprint(f\"Total Test Loss: {total_loss/num_batches:.4f}\")\nprint(f\"Total MSE Loss: {total_mse_loss/num_batches:.4f}\")\nprint(f\"Total L1 Loss: {total_l1_loss/num_batches:.4f}\")\n\nactive_features = global_active_mask.sum().item()\ntotal_features = global_active_mask.numel()\nglobal_sparsity = (1 - active_features / total_features) * 100\nprint(f\"Global Sparsity Across All Batches: {global_sparsity:.2f}%\")\nprint(f\"Percent of Active Features: {active_features / total_features * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T11:42:50.613476Z","iopub.execute_input":"2024-12-05T11:42:50.613859Z","iopub.status.idle":"2024-12-05T11:42:50.620033Z","shell.execute_reply.started":"2024-12-05T11:42:50.613818Z","shell.execute_reply":"2024-12-05T11:42:50.619122Z"}},"outputs":[{"name":"stdout","text":"Total Test Loss: 0.0883\nTotal MSE Loss: 0.0759\nTotal L1 Loss: 0.0124\nGlobal Sparsity Across All Batches: 0.00%\nPercent of Active Features: 100.00%\n","output_type":"stream"}],"execution_count":41}]}
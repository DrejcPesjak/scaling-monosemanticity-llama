{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4f54ab4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T14:14:02.783551Z",
     "iopub.status.busy": "2024-12-05T14:14:02.782742Z",
     "iopub.status.idle": "2024-12-05T14:14:02.787423Z",
     "shell.execute_reply": "2024-12-05T14:14:02.786616Z"
    },
    "papermill": {
     "duration": 0.010366,
     "end_time": "2024-12-05T14:14:02.788973",
     "exception": false,
     "start_time": "2024-12-05T14:14:02.778607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Required installations for transformers and datasets\n",
    "# !pip install transformers datasets\n",
    "# !pip install keras huggingface_hub\n",
    "# !pip install tensorflow\n",
    "# !pip install python-dotenv\n",
    "# !pip install zstandard\n",
    "#!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "337e34c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T14:14:02.793947Z",
     "iopub.status.busy": "2024-12-05T14:14:02.793683Z",
     "iopub.status.idle": "2024-12-05T14:14:19.369707Z",
     "shell.execute_reply": "2024-12-05T14:14:19.369046Z"
    },
    "papermill": {
     "duration": 16.580705,
     "end_time": "2024-12-05T14:14:19.371789",
     "exception": false,
     "start_time": "2024-12-05T14:14:02.791084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f69a5d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T14:14:19.377219Z",
     "iopub.status.busy": "2024-12-05T14:14:19.376670Z",
     "iopub.status.idle": "2024-12-05T14:14:19.594206Z",
     "shell.execute_reply": "2024-12-05T14:14:19.593484Z"
    },
    "papermill": {
     "duration": 0.222226,
     "end_time": "2024-12-05T14:14:19.596190",
     "exception": false,
     "start_time": "2024-12-05T14:14:19.373964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "FILE_NAMES = []\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        if filename.startswith(\"act\"): # skip model checkpoint\n",
    "            FILE_NAMES.append(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "790b4520",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T14:14:19.601440Z",
     "iopub.status.busy": "2024-12-05T14:14:19.601188Z",
     "iopub.status.idle": "2024-12-05T14:14:19.604763Z",
     "shell.execute_reply": "2024-12-05T14:14:19.604043Z"
    },
    "papermill": {
     "duration": 0.008064,
     "end_time": "2024-12-05T14:14:19.606418",
     "exception": false,
     "start_time": "2024-12-05T14:14:19.598354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "FILE_NAMES = sorted(FILE_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba3ac435",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T14:14:19.611367Z",
     "iopub.status.busy": "2024-12-05T14:14:19.611132Z",
     "iopub.status.idle": "2024-12-05T14:14:19.864743Z",
     "shell.execute_reply": "2024-12-05T14:14:19.864085Z"
    },
    "papermill": {
     "duration": 0.258092,
     "end_time": "2024-12-05T14:14:19.866427",
     "exception": false,
     "start_time": "2024-12-05T14:14:19.608335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scale_factor = 34.12206415510119 # at 1.6mil tokens\n",
    "# scale_factor = 34.128712991170886 # at 10.6mil tokens\n",
    "scale_factor = 11.888623072966611 # 10mil but with <begin> token removed\n",
    "\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
    "        # Decoder\n",
    "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = torch.relu(self.encoder(x))\n",
    "        # encoded = torch.nn.LeakyReLU(0.01)(self.encoder(x))\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded, encoded\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class ActivationDataset(Dataset):\n",
    "    def __init__(self, data_dir, batch_size, f_type, test_fraction=0.01, scale_factor=1.0, seed=42):\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = seed\n",
    "\n",
    "        if f_type in [\"train\", \"test\", \"all\"]:\n",
    "            self.f_type = f_type\n",
    "        else:\n",
    "            raise ValueError(\"f_type must be 'train' or 'test' or 'all'\")\n",
    "        \n",
    "        if not 0 <= test_fraction <= 1:\n",
    "            raise ValueError(\"test_fraction must be between 0 and 1\")\n",
    "        self.test_fraction = test_fraction\n",
    "\n",
    "        self.scale_factor = scale_factor\n",
    "        self.file_names = FILE_NAMES\n",
    "        \n",
    "        split_idx = int(len(self.file_names) * (1 - test_fraction))\n",
    "        if f_type == \"train\":\n",
    "            self.file_names = self.file_names[:split_idx]\n",
    "        elif f_type == \"test\":\n",
    "            self.file_names = self.file_names[split_idx:]\n",
    "        else: # all\n",
    "            pass\n",
    "\n",
    "        print(f\"Loaded {len(self.file_names)} batches for {f_type} set\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        activations = np.load(self.file_names[idx])\n",
    "        if self.f_type == \"all\":\n",
    "            sent_idx = activations[:, -3]\n",
    "            token_idx = activations[:, -2] \n",
    "            token = activations[:, -1]\n",
    "        # remove last 3 columns (sent_idx, token_idx, and token)\n",
    "        activations = activations[:, :-3]\n",
    "        # normalize activations\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        activations = torch.tensor(activations, dtype=torch.float32, device=device)\n",
    "        # print(\"Activation Range Before Normalization:\", torch.min(activations).item(), torch.max(activations).item())\n",
    "        activations = activations / self.scale_factor * np.sqrt(activations.shape[1])\n",
    "        # print(\"Activation Range After Normalization:\", torch.min(activations).item(), torch.max(activations).item())\n",
    "\n",
    "        if self.f_type == \"train\":\n",
    "            # Set seed for reproducibility\n",
    "            np.random.seed(self.seed)\n",
    "            # random subsample 8192 examples\n",
    "            indices = torch.randperm(activations.shape[0], device=activations.device)[:self.batch_size]\n",
    "            activations = activations[indices]\n",
    "        \n",
    "        if self.f_type == \"all\":\n",
    "            return activations, sent_idx, token_idx, token\n",
    "        else:\n",
    "            return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3c69c1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T14:14:19.871691Z",
     "iopub.status.busy": "2024-12-05T14:14:19.871422Z",
     "iopub.status.idle": "2024-12-05T14:14:50.931627Z",
     "shell.execute_reply": "2024-12-05T14:14:50.930955Z"
    },
    "papermill": {
     "duration": 31.064962,
     "end_time": "2024-12-05T14:14:50.933616",
     "exception": false,
     "start_time": "2024-12-05T14:14:19.868654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/716372986.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"/kaggle/input/checkpoint65k_sae/pytorch/default/1/checkpoint\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_dim': 3072, 'hidden_dim': 65536, 'l1_lambda': 0.00597965, 'lr': 2.5011e-05}\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"activations_data\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_dim = 3072  \n",
    "hidden_dim = 65536\n",
    "\n",
    "model = SparseAutoencoder(input_dim, hidden_dim).to(device)\n",
    "# model.load_state_dict(torch.load(\"models/sparse_autoencoder_496.3666.pth\"))\n",
    "checkpoint = torch.load(\"/kaggle/input/checkpoint65k_sae/pytorch/default/1/checkpoint\")\n",
    "print(checkpoint[\"hyper_parameters\"])\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "l1_lambda = 0.01  # Regularization strength for sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c18f669",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T14:14:50.940427Z",
     "iopub.status.busy": "2024-12-05T14:14:50.939918Z",
     "iopub.status.idle": "2024-12-05T14:16:25.351598Z",
     "shell.execute_reply": "2024-12-05T14:16:25.350570Z"
    },
    "papermill": {
     "duration": 94.418328,
     "end_time": "2024-12-05T14:16:25.354240",
     "exception": false,
     "start_time": "2024-12-05T14:14:50.935912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 129 batches for all set\n",
      "Saved minibatch 1 of 19 for batch 4\n",
      "Saved minibatch 2 of 19 for batch 4\n",
      "Saved minibatch 3 of 19 for batch 4\n",
      "Saved minibatch 4 of 19 for batch 4\n",
      "Saved minibatch 5 of 19 for batch 4\n",
      "Saved minibatch 6 of 19 for batch 4\n",
      "Saved minibatch 7 of 19 for batch 4\n",
      "Saved minibatch 8 of 19 for batch 4\n",
      "Saved minibatch 9 of 19 for batch 4\n",
      "Saved minibatch 10 of 19 for batch 4\n",
      "Saved minibatch 11 of 19 for batch 4\n",
      "Saved minibatch 12 of 19 for batch 4\n",
      "Saved minibatch 13 of 19 for batch 4\n",
      "Saved minibatch 14 of 19 for batch 4\n",
      "Saved minibatch 15 of 19 for batch 4\n",
      "Saved minibatch 16 of 19 for batch 4\n",
      "Saved minibatch 17 of 19 for batch 4\n",
      "Saved minibatch 18 of 19 for batch 4\n",
      "Saved minibatch 19 of 19 for batch 4\n"
     ]
    }
   ],
   "source": [
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "os.makedirs(\"sparse_latent_vectors\", exist_ok=True)\n",
    "\n",
    "dataset = ActivationDataset(\n",
    "    data_dir, \n",
    "    batch_size=0, # not subsampled\n",
    "    f_type=\"all\", \n",
    "    test_fraction=1.0, # not used if type=all\n",
    "    scale_factor=scale_factor, \n",
    "    seed=42 # not used\n",
    ")\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=False) # take 1 batch at a time\n",
    "\n",
    "# Extract and save latent vectors\n",
    "batch_skip = 4 # 20GB limit on kaggle output\n",
    "num_batches = 1\n",
    "batch_size = 4096  # Size we can fit in VRAM\n",
    "num_minibatches = 19  # 81920/8192 = 10 minibatches per batch\n",
    "with torch.no_grad():\n",
    "    for idx, batch_data in enumerate(data_loader):\n",
    "        if idx < batch_skip:\n",
    "            continue\n",
    "        if idx >= batch_skip+num_batches :\n",
    "            break\n",
    "        batch, sent_idx, token_idx, token = batch_data\n",
    "        sent_idx = sent_idx.to(device)\n",
    "        token_idx = token_idx.to(device)\n",
    "        token = token.to(device)\n",
    "        batch = batch.squeeze(0)  # Remove batch dimension of 1\n",
    "        \n",
    "        # Process minibatches and save immediately\n",
    "        for i in range(num_minibatches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = (i + 1) * batch_size\n",
    "            \n",
    "            # Get minibatch slice\n",
    "            minibatch = batch[start_idx:end_idx]\n",
    "            _, encoded = model(minibatch)\n",
    "            \n",
    "            # Stack with metadata\n",
    "            # Reshape metadata tensors to match batch size\n",
    "            sent_idx_batch = sent_idx[:,start_idx:end_idx].T\n",
    "            token_idx_batch = token_idx[:,start_idx:end_idx].T\n",
    "            token_batch = token[:,start_idx:end_idx].T\n",
    "            \n",
    "            output_vectors = torch.cat((encoded, sent_idx_batch, token_idx_batch, token_batch), dim=1)\n",
    "            \n",
    "            # Save each minibatch immediately as a PyTorch tensor\n",
    "            torch.save(output_vectors, f\"sparse_latent_vectors/latent_vectors_batch_{idx}_minibatch_{i}.pt\")\n",
    "            # output_saved = torch.load(f\"sparse_latent_vectors/latent_vectors_batch_{idx}_minibatch_{i}.pt\")\n",
    "            # output_vectors = output_vectors.to(torch.float16)\n",
    "            # print(f\"Data saved is near equal: {torch.allclose(output_vectors[:,:-3], output_saved[:,:-3], atol=1e-1)}\")\n",
    "            print(f\"Saved minibatch {i+1} of {num_minibatches} for batch {idx}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6211253,
     "sourceId": 10076311,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6211491,
     "sourceId": 10076612,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6211714,
     "sourceId": 10076937,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 183576,
     "modelInstanceId": 161188,
     "sourceId": 189059,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 148.699015,
   "end_time": "2024-12-05T14:16:29.066061",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-05T14:14:00.367046",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
